{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARoDias/DataMiningProject_CTCT_2024Analysis/blob/main/CTCT_Data_Mining_Prediction_and_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7VeyAqgiAuO"
      },
      "source": [
        "**Prediction and Classification notebook**\n",
        "\n",
        "This notebook answers the following research question:\n",
        "* **RQ5:** Can student performance in one theme of the CTCT course predict outcomes in subsequent themes?\n",
        "\n",
        "**Important:**\n",
        "Create a folder called **inputs** and store files there to load."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoD2cYqri9Yj"
      },
      "source": [
        "# 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install additional libraries that are not included by default in Google Colaboratory\n",
        "\n",
        "# Install CatBoost\n",
        "!pip install catboost\n",
        "\n",
        "# Install XGBoost\n",
        "!pip install xgboost\n",
        "\n",
        "# Install LightGBM\n",
        "!pip install lightgbm\n",
        "\n",
        "# Install Imbalanced-learn (for SMOTE and other imbalanced data techniques)\n",
        "#!pip install imbalanced-learn\n",
        "\n",
        "# Install Python-docx (for working with .docx files if needed)\n",
        "!pip install python-docx\n",
        "\n",
        "# If you need to use Google Colab-specific commands\n",
        "# !pip install google-colab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rokXvEIHRi6",
        "outputId": "09cfa57a-e2c2-4849-c474-1f7dbf149e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.22.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMiCK-WBh_7v"
      },
      "outputs": [],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd  # Main library for data manipulation and analysis\n",
        "import numpy as np  # Library for numerical operations\n",
        "import os  # Provides a way to use operating system dependent functionality\n",
        "import shutil  # Provides functions to operate on files and collections of files\n",
        "\n",
        "# Model selection and evaluation\n",
        "from sklearn.model_selection import train_test_split  # Function to split datasets into training and testing sets\n",
        "from sklearn.model_selection import GridSearchCV  # Performs exhaustive search over specified parameter values for an estimator\n",
        "from sklearn.model_selection import cross_val_score  # Evaluates a score by cross-validation\n",
        "\n",
        "# Metrics for model evaluation\n",
        "from sklearn.metrics import r2_score  # R^2 (coefficient of determination) regression score function\n",
        "from sklearn.metrics import confusion_matrix  # Confusion matrix to evaluate the accuracy of a classification\n",
        "from sklearn.metrics import classification_report  # Builds a text report showing the main classification metrics\n",
        "from sklearn.metrics import accuracy_score  # Classification accuracy metric\n",
        "from sklearn.metrics import mean_squared_error  # Mean squared error regression loss\n",
        "from sklearn.metrics import mean_absolute_error  # Mean absolute error regression loss\n",
        "from sklearn.metrics import f1_score  # F1 score for classification tasks\n",
        "from sklearn.metrics import precision_score  # Precision metric for classification tasks\n",
        "from sklearn.metrics import recall_score  # Recall metric for classification tasks\n",
        "from sklearn.metrics import roc_auc_score  # ROC AUC metric for evaluating classification models\n",
        "\n",
        "# Preprocessing tools\n",
        "from sklearn.preprocessing import StandardScaler  # Standardize features by removing the mean and scaling to unit variance\n",
        "from sklearn.preprocessing import MinMaxScaler  # Transforms features by scaling each feature to a given range\n",
        "from sklearn.pipeline import Pipeline  # Allows assembling several steps that can be cross-validated together\n",
        "from sklearn.decomposition import PCA  # Principal Component Analysis (PCA) for dimensionality reduction\n",
        "from sklearn.impute import SimpleImputer  # Impute missing values for both numerical and categorical data\n",
        "from sklearn.impute import KNNImputer  # K-Nearest Neighbors imputer for handling missing values\n",
        "from sklearn.feature_selection import VarianceThreshold  # Removes features with low variance\n",
        "\n",
        "# Machine learning models\n",
        "from sklearn.linear_model import LogisticRegression  # Logistic Regression classifier\n",
        "from sklearn.linear_model import LinearRegression  # Linear Regression model\n",
        "from sklearn.tree import DecisionTreeClassifier  # Decision Tree classifier\n",
        "from sklearn.tree import DecisionTreeRegressor  # Decision Tree regressor\n",
        "from sklearn.ensemble import RandomForestClassifier  # Random Forest classifier\n",
        "from sklearn.ensemble import RandomForestRegressor  # Random Forest regressor\n",
        "from sklearn.ensemble import StackingRegressor  # Stacking Regressor\n",
        "from sklearn.naive_bayes import GaussianNB  # Naive Bayes classifier for Gaussian-distributed data\n",
        "from sklearn.svm import SVC  # Support Vector Classifier\n",
        "from sklearn.svm import SVR  # Support Vector Regressor\n",
        "from sklearn.neighbors import KNeighborsClassifier  # k-Nearest Neighbors classifier\n",
        "from sklearn.neighbors import KNeighborsRegressor  # k-Nearest Neighbors regressor\n",
        "from sklearn.neural_network import MLPClassifier  # Multi-layer Perceptron classifier\n",
        "from sklearn.neural_network import MLPRegressor  # Multi-layer Perceptron regressor\n",
        "import xgboost as xgb  # Extreme Gradient Boosting for classification and regression tasks\n",
        "import lightgbm as lgb  # LightGBM for classification and regression tasks\n",
        "import catboost as cb  # CatBoost for classification and regression tasks\n",
        "\n",
        "# Error handling\n",
        "from sklearn.exceptions import ConvergenceWarning  # Handles convergence warnings in iterative algorithms\n",
        "import warnings  # Python's built-in warnings module\n",
        "warnings.filterwarnings('ignore', category=ConvergenceWarning)  # Ignore convergence warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Document generation and display (Optional)\n",
        "from docx import Document  # Library to create and update Microsoft Word (.docx) files\n",
        "from IPython.display import display as ipy_display  # Function to display objects in IPython notebooks\n",
        "\n",
        "# Date and time utilities\n",
        "from datetime import datetime  # Supplies classes for manipulating dates and times\n",
        "import time  # Provides various time-related functions\n",
        "\n",
        "# Data serialization\n",
        "import pickle  # Python object serialization module\n",
        "from io import StringIO  # In-memory file-like object\n",
        "\n",
        "# Optional: Integration with Google Colaboratory (uncomment if needed)\n",
        "from google.colab import files  # For file handling in Google Colaboratory\n",
        "\n",
        "# Ensure directories exist\n",
        "if not os.path.exists('inputs'):\n",
        "    os.makedirs('inputs')\n",
        "\n",
        "if not os.path.exists('best_models'):\n",
        "    os.makedirs('best_models')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W2lTZ9mCAbT"
      },
      "source": [
        "# 2. Load Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUFdC5clgdKE"
      },
      "source": [
        "## 2.1 Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkVHXzhBCVKo"
      },
      "source": [
        "###### load_feather_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcURi6xHB8Rj",
        "outputId": "bceb6646-0219-4c8a-9373-9a05c0846e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset from 'inputs/predict_finalgrade_Lasso_less_restricted.feather' with shape (7890, 90)\n",
            "Loaded dataset from 'inputs/predict_total_ap_Lasso_more_restricted_regression.feather' with shape (7890, 63)\n",
            "Loaded dataset from 'inputs/predict_total_ap_RF_more_restricted.feather' with shape (7890, 63)\n",
            "Loaded dataset from 'inputs/predict_passed_DT_less_restricted.feather' with shape (7890, 4)\n",
            "Loaded dataset from 'inputs/predict_total_pp_RF_less_restricted.feather' with shape (7890, 144)\n",
            "Loaded dataset from 'inputs/predict_total_spa_DT_less_restricted.feather' with shape (7890, 108)\n",
            "Loaded dataset from 'inputs/combined_df_Passed.feather' with shape (7890, 68)\n",
            "Loaded dataset from 'inputs/predict_passed_RF_more_restricted.feather' with shape (7890, 58)\n",
            "Loaded dataset from 'inputs/predict_total_spa_DT_restricted.feather' with shape (7890, 108)\n",
            "Loaded dataset from 'inputs/predict_total_pp_MI_more_restricted.feather' with shape (7890, 62)\n",
            "Loaded dataset from 'inputs/predict_total_spa_MI_restricted.feather' with shape (7890, 117)\n",
            "Loaded dataset from 'inputs/predict_total_pp_MI_less_restricted.feather' with shape (7890, 152)\n",
            "Loaded dataset from 'inputs/predict_finalgrade_Lasso_restricted.feather' with shape (7890, 127)\n",
            "Loaded dataset from 'inputs/predict_total_pp_DT_more_restricted.feather' with shape (7890, 60)\n",
            "Loaded dataset from 'inputs/predict_total_ap_RF_less_restricted.feather' with shape (7890, 146)\n",
            "Loaded dataset from 'inputs/predict_total_ap_MI_more_restricted.feather' with shape (7890, 62)\n",
            "Loaded dataset from 'inputs/predict_finalgrade_FR_more_restricted.feather' with shape (7890, 50)\n",
            "Loaded dataset from 'inputs/predict_total_spa_MI_more_restricted.feather' with shape (7890, 63)\n",
            "Loaded dataset from 'inputs/predict_total_pp_Lasso_more_restricted_regression.feather' with shape (7890, 63)\n",
            "Loaded dataset from 'inputs/predict_total_ap_DT_more_restricted.feather' with shape (7890, 55)\n",
            "Loaded dataset from 'inputs/predict_total_pp_RF_more_restricted_regression.feather' with shape (7890, 59)\n",
            "Loaded dataset from 'inputs/predict_passed_RF_restricted.feather' with shape (7890, 109)\n",
            "Loaded dataset from 'inputs/predict_total_spa_MI_less_restricted.feather' with shape (7890, 117)\n",
            "Loaded dataset from 'inputs/predict_total_ap_MI_less_restricted.feather' with shape (7890, 155)\n",
            "Loaded dataset from 'inputs/combined_df_Total_SPA.feather' with shape (7890, 68)\n",
            "Loaded dataset from 'inputs/predict_passed_MI_restricted.feather' with shape (7890, 114)\n",
            "Loaded dataset from 'inputs/predict_total_spa_DT_more_restricted.feather' with shape (7890, 60)\n",
            "Loaded dataset from 'inputs/predict_total_pp_MI_restricted.feather' with shape (7890, 108)\n",
            "Loaded dataset from 'inputs/predict_total_ap_DT_restricted.feather' with shape (7890, 110)\n",
            "Loaded dataset from 'inputs/predict_total_ap_RF_restricted.feather' with shape (7890, 110)\n",
            "Loaded dataset from 'inputs/predict_passed_MI_more_restricted.feather' with shape (7890, 61)\n",
            "Loaded dataset from 'inputs/predict_passed_RF_less_restricted.feather' with shape (7890, 13)\n",
            "Loaded dataset from 'inputs/predict_total_spa_FR_more_restricted_regression.feather' with shape (7890, 51)\n",
            "Loaded dataset from 'inputs/predict_total_ap_DT_less_restricted.feather' with shape (7890, 147)\n",
            "Loaded dataset from 'inputs/filtered_combined_df_FinalGrade.feather' with shape (7498, 68)\n",
            "Loaded dataset from 'inputs/combined_df_Total_AP.feather' with shape (7890, 68)\n",
            "Loaded dataset from 'inputs/predict_total_ap_FR_more_restricted_regression.feather' with shape (7890, 51)\n",
            "Loaded dataset from 'inputs/predict_total_spa_RF_more_restricted_regression.feather' with shape (7890, 59)\n",
            "Loaded dataset from 'inputs/predict_finalgrade_Lasso_more_restricted.feather' with shape (7890, 62)\n",
            "Loaded dataset from 'inputs/predict_total_pp_RF_more_restricted.feather' with shape (7890, 64)\n",
            "Loaded dataset from 'inputs/predict_finalgrade_RF_restricted.feather' with shape (7890, 113)\n",
            "Loaded dataset from 'inputs/predict_finalgrade_FR_restricted.feather' with shape (7890, 105)\n",
            "Loaded dataset from 'inputs/predict_total_pp_RF_restricted.feather' with shape (7890, 118)\n",
            "Loaded dataset from 'inputs/predict_passed_DT_more_restricted.feather' with shape (7890, 43)\n",
            "Loaded dataset from 'inputs/predict_total_spa_RF_more_restricted.feather' with shape (7890, 62)\n",
            "Loaded dataset from 'inputs/predict_passed_DT_restricted.feather' with shape (7890, 37)\n",
            "Loaded dataset from 'inputs/predict_total_pp_FR_more_restricted_regression.feather' with shape (7890, 51)\n",
            "Loaded dataset from 'inputs/predict_finalgrade_FR_less_restricted.feather' with shape (7890, 151)\n",
            "Loaded dataset from 'inputs/combined_df_Total_PP.feather' with shape (7890, 68)\n",
            "Loaded dataset from 'inputs/predict_total_spa_RF_less_restricted.feather' with shape (7890, 113)\n",
            "Loaded dataset from 'inputs/predict_finalgrade_RF_less_restricted.feather' with shape (7890, 152)\n",
            "Loaded dataset from 'inputs/predict_total_ap_RF_more_restricted_regression.feather' with shape (7890, 57)\n",
            "Loaded dataset from 'inputs/predict_finalgrade_RF_more_restricted.feather' with shape (7890, 58)\n",
            "Loaded dataset from 'inputs/predict_passed_MI_less_restricted.feather' with shape (7890, 156)\n",
            "Loaded dataset from 'inputs/predict_total_pp_DT_restricted.feather' with shape (7890, 109)\n",
            "Loaded dataset from 'inputs/predict_total_ap_MI_restricted.feather' with shape (7890, 108)\n",
            "Loaded dataset from 'inputs/predict_total_pp_DT_less_restricted.feather' with shape (7890, 146)\n",
            "Loaded dataset from 'inputs/predict_total_spa_Lasso_more_restricted_regression.feather' with shape (7890, 61)\n",
            "Loaded dataset from 'inputs/predict_total_spa_RF_restricted.feather' with shape (7890, 113)\n"
          ]
        }
      ],
      "source": [
        "# Define a function to load Feather files and organize datasets by specific features or target columns\n",
        "\n",
        "def load_feather_files(output_dir):\n",
        "    \"\"\"\n",
        "    Load all Feather files from the specified output directory.\n",
        "\n",
        "    Args:\n",
        "    - output_dir (str): Directory where the Feather files are stored.\n",
        "\n",
        "    Returns:\n",
        "    - datasets (dict): Dictionary with filenames (without .feather extension) as keys and loaded DataFrames as values.\n",
        "    \"\"\"\n",
        "    datasets = {}\n",
        "    for file_name in os.listdir(output_dir):\n",
        "        if file_name.endswith('.feather'):\n",
        "            file_path = os.path.join(output_dir, file_name)\n",
        "            dataset_name = os.path.splitext(file_name)[0]  # Remove the .feather extension\n",
        "            datasets[dataset_name] = pd.read_feather(file_path)\n",
        "            print(f\"Loaded dataset from '{file_path}' with shape {datasets[dataset_name].shape}\")\n",
        "    return datasets\n",
        "\n",
        "# Load all Feather files from the \"inputs\" folder\n",
        "input_dir = 'inputs'\n",
        "loaded_datasets = load_feather_files(input_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ew9d8ERsDLMz"
      },
      "outputs": [],
      "source": [
        "#for file_name, df in loaded_datasets.items():\n",
        "  #print(\"-\"*50)\n",
        "  #print(f\"Dataset '{file_name}':\")\n",
        "  #print('Information:')\n",
        "  #print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObtkM1t5jIEH"
      },
      "source": [
        "## 2.2 Check Missing Values in Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6obNIQcnbL4q"
      },
      "source": [
        "###### check_missing_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9lQZ7E7bKTn"
      },
      "outputs": [],
      "source": [
        "def check_missing_values_in_single_df(dataset):\n",
        "    \"\"\"\n",
        "    Check and print the missing values for a single dataset (DataFrame).\n",
        "\n",
        "    Args:\n",
        "    - dataset (DataFrame): A DataFrame representing the dataset.\n",
        "\n",
        "    Functionality:\n",
        "    - The function calculates the sum of missing values for each column.\n",
        "    - If there are columns with missing values, it prints the count of missing values per column.\n",
        "    \"\"\"\n",
        "    print(\"Missing values in the dataset:\")\n",
        "    missing_values = dataset.isnull().sum()\n",
        "    # Filter and print only columns with missing values\n",
        "    print(missing_values[missing_values > 0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad1nE5E1KnoE"
      },
      "outputs": [],
      "source": [
        "#for file_name, df in loaded_datasets.items():\n",
        "  #print(\"-\"*50)\n",
        "  #print(f\"Dataset '{file_name}':\")\n",
        "  #check_missing_values_in_single_df(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB35WJaijCF-"
      },
      "source": [
        "## 2.3  Organizing Loaded Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### create_filtered_dataframe"
      ],
      "metadata": {
        "id": "4KO0Rm21FF0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create filtered DataFrame with relevant columns and Year, keeping target column last\n",
        "def create_filtered_dataframe(base_df, selected_columns, target_column):\n",
        "    # Filter only the columns that exist in the base DataFrame\n",
        "    filtered_columns = [col for col in selected_columns if col in base_df.columns]\n",
        "\n",
        "    # Ensure 'Year' column is included if it exists\n",
        "    if 'Year' in base_df.columns and 'Year' not in filtered_columns:\n",
        "        filtered_columns.insert(0, 'Year')  # Ensure Year is the first column\n",
        "\n",
        "    # Add target column as the last one\n",
        "    if target_column in base_df.columns and target_column not in filtered_columns:\n",
        "        filtered_columns.append(target_column)\n",
        "\n",
        "    # Create and return the filtered DataFrame\n",
        "    return base_df[filtered_columns]\n",
        "\n",
        "# Define the target column mapping based on the filename suffix to organize datasets by target variables\n",
        "target_column_mapping = {\n",
        "    'predict_passed': 'Passed',\n",
        "    'predict_finalgrade': 'FinalGrade',\n",
        "    'predict_total_ap': 'Total_AP',\n",
        "    'predict_total_pp': 'Total_PP',\n",
        "    'predict_total_spa': 'Total_SPA'\n",
        "}\n",
        "\n",
        "# Filter datasets based on feature selection method\n",
        "mi_datasets = {name: df for name, df in loaded_datasets.items() if '_MI_' in name}\n",
        "rf_datasets = {name: df for name, df in loaded_datasets.items() if '_RF_' in name}\n",
        "dt_datasets = {name: df for name, df in loaded_datasets.items() if '_DT_' in name}\n",
        "lasso_datasets = {name: df for name, df in loaded_datasets.items() if '_Lasso_' in name}\n",
        "fr_datasets = {name: df for name, df in loaded_datasets.items() if '_FR_' in name}\n",
        "\n",
        "# Datasets without feature selection\n",
        "without_feature_selection_datasets = {name: df for name, df in loaded_datasets.items() if 'combined_df' in name.lower()}\n",
        "\n",
        "# Update the handling for FinalGrade specifically\n",
        "for target_key, target_column in target_column_mapping.items():\n",
        "    if target_column == 'FinalGrade':\n",
        "        # Use the specific dataset for FinalGrade\n",
        "        base_key = 'filtered_combined_df_FinalGrade'\n",
        "    else:\n",
        "        base_key = f'combined_df_{target_column}'\n",
        "\n",
        "    if base_key in without_feature_selection_datasets:\n",
        "        base_df = without_feature_selection_datasets[base_key]\n",
        "    else:\n",
        "        print(f\"No base dataset found for {target_column}\")\n",
        "        continue\n",
        "\n",
        "    # Iterate through each feature selection method and replace datasets\n",
        "    for method, datasets in [('MI', mi_datasets), ('RF', rf_datasets), ('DT', dt_datasets), ('Lasso', lasso_datasets), ('FR', fr_datasets)]:\n",
        "        for dataset_name, dataset_df in datasets.items():\n",
        "            if f'predict_{target_column.lower()}' in dataset_name.lower() and f'_{method}_' in dataset_name:\n",
        "                selected_columns = list(dataset_df.columns)\n",
        "                # FinalGrade handling: Ensure we're removing the correct target column\n",
        "                selected_columns.remove(target_column)\n",
        "\n",
        "                # Create the filtered DataFrame using the utility function\n",
        "                filtered_df = create_filtered_dataframe(base_df, selected_columns, target_column)\n",
        "\n",
        "                # Replace the original DataFrame in loaded_datasets with the filtered DataFrame\n",
        "                loaded_datasets[dataset_name] = filtered_df\n",
        "\n",
        "                # Optional: print to verify the replacement\n",
        "                print(f'Replaced dataset: {dataset_name}')\n",
        "                print(loaded_datasets[dataset_name].columns)\n",
        "                print(len(loaded_datasets[dataset_name]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU3WETK3uyrf",
        "outputId": "ecbfdffb-751e-4ca7-9d92-eebc61064bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaced dataset: predict_passed_MI_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '51', '1_8', '43', '19', '139', '23', '33', '10', '7', '50',\n",
            "       '37', '1_11', '49', '135', '140', '44', '25', '94_1', '12', '2_11',\n",
            "       '56', '5', '41', '4', '147', '52', '30', '11', '137', '36', '72_1',\n",
            "       '39', '35', '46', '53', '27', '8', '32', '29', '22', '34', '28', '45',\n",
            "       'Passed'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_passed_MI_more_restricted\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '3', '46',\n",
            "       '52', '42', '37', '53', '27', '45', '1_11', '5', '49', '30', '20',\n",
            "       '135', '38', '140', '44', '11', '25', '31', '133', '18', '9', '144',\n",
            "       '137', '8', '36', '51', '72_1', '32', '94_1', '12', '1_8', '43', '29',\n",
            "       '19', '34', '28', '56', '139', '23', '39', '33', 'Passed'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_passed_MI_less_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '51', '58', '1_8', '43', '19', '139', '23', '33', '10', '7',\n",
            "       '50', '37', '1_11', '49', '135', '140', '44', '25', '94_1', '12', '56',\n",
            "       '5', '41', '4', '147', '52', '30', '137', '36', '39', '35', '46', '53',\n",
            "       '27', '45', '8', '32', '29', '22', '34', '28', 'Passed'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_passed_RF_more_restricted\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '46', '52',\n",
            "       '42', '37', '53', '27', '1_11', '49', '30', '20', '38', '140', '44',\n",
            "       '11', '25', '31', '133', '18', '9', '144', '8', '36', '23_1', '51',\n",
            "       '58', '138', '32', '94_1', '12', '43', '2_11', '29', '22', '15', '19',\n",
            "       '34', '28', '56', '23', '5', '2', '33', 'Passed'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_passed_RF_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '31', '133', '18', '51',\n",
            "       '1_8', '19', '23', '33', '10', '7', '50', '23_2', '37', '49', '135',\n",
            "       '140', '44', '25', '134', '94_1', '12', '2_11', '56', '41', '147', '52',\n",
            "       '30', '11', '16', '137', '36', '72_1', '35', '3', '46', '53', '27',\n",
            "       '32', '29', '22', '34', '28', 'Passed'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_passed_RF_less_restricted\n",
            "Index(['Year', '1', 'Passed'], dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_passed_DT_less_restricted\n",
            "Index(['Year', 'Passed'], dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_passed_DT_more_restricted\n",
            "Index(['Year', '10', '24', '41', '55', '50', '35', '1', '23_2', '46', '52',\n",
            "       '42', '37', '53', '27', '49', '30', '20', '38', '140', '44', '11', '25',\n",
            "       '31', '144', '8', '36', '23_1', '51', '134', '58', '138', '32', '94_1',\n",
            "       '12', '34', '28', '56', '23', '5', '2', 'Passed'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_passed_DT_restricted\n",
            "Index(['Year', '10', '41', '55', '35', '50', '1', '27', '25', '36', '12', '15',\n",
            "       '34', '28', '139', 'Passed'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_finalgrade_RF_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '18', '144',\n",
            "       '23_1', '51', '58', '1_8', '43', '19', '23', '33', '10', '50', '23_2',\n",
            "       '37', '49', '140', '44', '25', '94_1', '12', '15', '56', '5', '41',\n",
            "       '147', '52', '30', '11', '137', '36', '72_1', '35', '46', '53', '27',\n",
            "       '8', '32', '29', '22', '34', '28', '45', '2', 'FinalGrade'],\n",
            "      dtype='object')\n",
            "7498\n",
            "Replaced dataset: predict_finalgrade_RF_less_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '23_1', '51', '58', '1_8', '43', '19', '139', '23', '33', '10',\n",
            "       '50', '23_2', '49', '135', '140', '44', '25', '134', '94_1', '12',\n",
            "       '2_11', '15', '41', '52', '30', '11', '16', '137', '36', '138', '72_1',\n",
            "       '35', '46', '53', '27', '45', '32', '29', '22', '34', '28',\n",
            "       'FinalGrade'],\n",
            "      dtype='object')\n",
            "7498\n",
            "Replaced dataset: predict_finalgrade_RF_more_restricted\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '23_2',\n",
            "       '46', '52', '42', '37', '53', '27', '45', '49', '30', '20', '135', '38',\n",
            "       '140', '44', '11', '25', '31', '16', '18', '9', '144', '137', '8', '36',\n",
            "       '23_1', '134', '58', '138', '72_1', '32', '94_1', '12', '1_8', '43',\n",
            "       '2_11', '29', '22', '15', '19', '34', '28', '56', '139', '23', '5', '2',\n",
            "       '33', 'FinalGrade'],\n",
            "      dtype='object')\n",
            "7498\n",
            "Replaced dataset: predict_finalgrade_Lasso_less_restricted\n",
            "Index(['Year', '24', '31', '58', '1_8', '43', '23', '33', '10', '23_2', '49',\n",
            "       '134', '94_1', '2_11', '15', '41', '122_1', '30', '11', '16', '72_1',\n",
            "       '54', '35', '27', '32', '22', '34', '28', 'FinalGrade'],\n",
            "      dtype='object')\n",
            "7498\n",
            "Replaced dataset: predict_finalgrade_Lasso_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '23_1', '51', '58', '1_8', '43', '19', '139', '23', '33', '10',\n",
            "       '50', '23_2', '37', '1_11', '49', '135', '140', '44', '25', '134',\n",
            "       '94_1', '12', '15', '56', '5', '41', '4', '147', '52', '30', '11', '16',\n",
            "       '137', '36', '138', '72_1', '39', '54', '35', '3', '46', '53', '27',\n",
            "       '8', '32', '29', '22', '34', '28', '45', '2', 'FinalGrade'],\n",
            "      dtype='object')\n",
            "7498\n",
            "Replaced dataset: predict_finalgrade_Lasso_more_restricted\n",
            "Index(['Year', '10', '24', '41', '54', '7', '4', '55', '35', '50', '1', '147',\n",
            "       '3', '23_2', '46', '52', '42', '37', '53', '27', '45', '1_11', '122_1',\n",
            "       '49', '30', '20', '135', '38', '140', '44', '11', '25', '16', '31',\n",
            "       '18', '133', '9', '144', '8', '36', '23_1', '134', '58', '138', '72_1',\n",
            "       '32', '94_1', '12', '1_8', '43', '29', '22', '15', '19', '34', '28',\n",
            "       '56', '139', '23', '5', '2', '33', 'FinalGrade'],\n",
            "      dtype='object')\n",
            "7498\n",
            "Replaced dataset: predict_finalgrade_FR_more_restricted\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '46', '52',\n",
            "       '42', '37', '53', '27', '45', '49', '30', '20', '135', '38', '140',\n",
            "       '44', '9', '25', '31', '133', '18', '144', '137', '8', '36', '51', '32',\n",
            "       '94_1', '12', '1_8', '43', '29', '19', '34', '28', '56', '139', '23',\n",
            "       '5', '39', '2', '33', 'FinalGrade'],\n",
            "      dtype='object')\n",
            "7498\n",
            "Replaced dataset: predict_finalgrade_FR_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '51', '1_8', '43', '19', '139', '23', '33', '10', '50', '37',\n",
            "       '49', '135', '140', '44', '25', '94_1', '12', '56', '5', '41', '147',\n",
            "       '52', '30', '137', '36', '39', '35', '46', '53', '27', '8', '32', '29',\n",
            "       '34', '28', '45', '2', 'FinalGrade'],\n",
            "      dtype='object')\n",
            "7498\n",
            "Replaced dataset: predict_finalgrade_FR_less_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '51', '1_8', '43', '19', '139', '23', '33', '10', '7', '50',\n",
            "       '37', '49', '135', '140', '44', '25', '94_1', '12', '56', '5', '41',\n",
            "       '147', '52', '30', '137', '36', '39', '35', '46', '53', '27', '45', '8',\n",
            "       '32', '29', '34', '28', '2', 'FinalGrade'],\n",
            "      dtype='object')\n",
            "7498\n",
            "Replaced dataset: predict_total_ap_MI_more_restricted\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '46', '52',\n",
            "       '42', '37', '53', '27', '45', '1_11', '5', '49', '30', '20', '135',\n",
            "       '38', '140', '44', '11', '25', '31', '133', '18', '9', '144', '137',\n",
            "       '8', '36', '23_1', '51', '32', '94_1', '12', '43', '2_11', '29', '15',\n",
            "       '19', '34', '28', '56', '139', '23', '39', '2', '33', 'Total_AP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_ap_MI_less_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '51', '1_8', '43', '19', '139', '23', '33', '10', '7', '50',\n",
            "       '37', '1_11', '49', '135', '140', '44', '25', '94_1', '12', '2_11',\n",
            "       '56', '5', '41', '4', '147', '52', '30', '137', '36', '39', '35', '46',\n",
            "       '53', '27', '45', '8', '32', '29', '34', '28', '2', 'Total_AP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_ap_MI_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '51', '58', '1_8', '43', '19', '139', '23', '33', '10', '7',\n",
            "       '50', '37', '49', '135', '140', '44', '25', '94_1', '12', '56', '5',\n",
            "       '41', '147', '52', '30', '137', '36', '39', '35', '46', '53', '27', '8',\n",
            "       '32', '29', '34', '28', '45', '2', 'Total_AP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_ap_RF_more_restricted\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '23_2',\n",
            "       '46', '52', '42', '37', '53', '27', '5', '49', '30', '20', '135', '38',\n",
            "       '140', '44', '11', '25', '16', '31', '18', '9', '144', '137', '8', '36',\n",
            "       '23_1', '51', '134', '58', '72_1', '32', '94_1', '12', '1_8', '43',\n",
            "       '2_11', '22', '15', '19', '34', '28', '56', '45', '23', '2', '33',\n",
            "       'Total_AP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_ap_RF_less_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '31', '18', '144', '23_1',\n",
            "       '51', '58', '43', '23', '33', '10', '50', '23_2', '49', '44', '25',\n",
            "       '94_1', '12', '15', '41', '52', '30', '11', '16', '138', '72_1', '54',\n",
            "       '35', '3', '46', '27', '45', '32', '29', '22', '34', '28', 'Total_AP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_ap_RF_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '31', '18', '144', '23_1',\n",
            "       '51', '58', '43', '19', '23', '33', '10', '7', '50', '23_2', '37', '49',\n",
            "       '135', '44', '25', '94_1', '12', '2_11', '56', '41', '147', '52', '30',\n",
            "       '11', '137', '36', '72_1', '39', '35', '46', '53', '27', '32', '29',\n",
            "       '22', '34', '28', '45', 'Total_AP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_ap_RF_more_restricted_regression\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '23_2',\n",
            "       '46', '52', '42', '37', '53', '27', '49', '30', '20', '135', '38',\n",
            "       '140', '44', '11', '25', '16', '31', '18', '9', '144', '137', '8', '36',\n",
            "       '23_1', '51', '134', '58', '138', '72_1', '32', '94_1', '12', '1_8',\n",
            "       '43', '2_11', '22', '15', '19', '34', '28', '56', '45', '23', '5', '2',\n",
            "       '33', 'Total_AP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_ap_DT_more_restricted\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '23_2', '46',\n",
            "       '52', '42', '37', '53', '27', '1_11', '49', '30', '20', '135', '38',\n",
            "       '44', '11', '25', '31', '16', '18', '144', '8', '36', '23_1', '58',\n",
            "       '72_1', '32', '94_1', '12', '1_8', '43', '22', '15', '19', '34', '28',\n",
            "       '56', '45', '23', '5', '2', '33', 'Total_AP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_ap_DT_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '31', '133', '18', '23_1',\n",
            "       '51', '58', '1_8', '43', '23', '33', '10', '7', '50', '23_2', '37',\n",
            "       '49', '44', '25', '94_1', '12', '2_11', '56', '41', '52', '30', '11',\n",
            "       '16', '137', '36', '138', '72_1', '39', '35', '46', '27', '32', '29',\n",
            "       '22', '34', '28', '45', '2', 'Total_AP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_ap_DT_less_restricted\n",
            "Index(['Year', '55', '1', '42', '20', '38', '31', '18', '23_1', '51', '58',\n",
            "       '43', '19', '23', '33', '10', '7', '50', '23_2', '49', '44', '25',\n",
            "       '134', '94_1', '12', '2_11', '15', '41', '147', '52', '30', '11', '16',\n",
            "       '138', '72_1', '39', '54', '35', '46', '32', '29', '22', '34', '2',\n",
            "       'Total_AP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_ap_Lasso_more_restricted_regression\n",
            "Index(['Year', '10', '24', '41', '54', '7', '4', '55', '35', '50', '1', '147',\n",
            "       '3', '23_2', '46', '52', '42', '37', '53', '27', '1_11', '122_1', '49',\n",
            "       '30', '20', '135', '38', '140', '44', '11', '25', '16', '31', '18', '9',\n",
            "       '144', '137', '36', '23_1', '51', '134', '58', '138', '72_1', '32',\n",
            "       '94_1', '12', '1_8', '43', '2_11', '29', '22', '15', '19', '34', '28',\n",
            "       '56', '45', '23', '5', '39', '2', '33', 'Total_AP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_ap_FR_more_restricted_regression\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '46', '52',\n",
            "       '42', '37', '53', '27', '45', '1_11', '49', '30', '20', '135', '38',\n",
            "       '140', '44', '9', '25', '31', '133', '18', '144', '137', '8', '36',\n",
            "       '51', '32', '94_1', '12', '1_8', '43', '29', '19', '34', '28', '56',\n",
            "       '139', '23', '5', '39', '2', '33', 'Total_AP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_pp_MI_more_restricted\n",
            "Index(['Year', '10', '24', '41', '54', '55', '7', '35', '50', '1', '147', '3',\n",
            "       '46', '52', '42', '37', '53', '27', '45', '1_11', '5', '49', '30', '20',\n",
            "       '135', '38', '140', '44', '11', '25', '31', '133', '18', '9', '144',\n",
            "       '137', '8', '36', '51', '32', '94_1', '12', '1_8', '43', '29', '19',\n",
            "       '34', '28', '56', '139', '23', '39', '2', '33', 'Total_PP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_pp_MI_less_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '51', '1_8', '43', '19', '139', '23', '33', '10', '7', '50',\n",
            "       '37', '1_11', '49', '135', '140', '44', '25', '94_1', '12', '56', '5',\n",
            "       '41', '147', '52', '30', '137', '36', '39', '35', '46', '53', '27',\n",
            "       '45', '8', '32', '29', '34', '28', '2', 'Total_PP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_pp_MI_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '51', '1_8', '43', '19', '139', '23', '33', '10', '7', '50',\n",
            "       '37', '1_11', '49', '135', '140', '44', '25', '94_1', '12', '56', '5',\n",
            "       '41', '147', '52', '30', '137', '36', '39', '35', '46', '53', '27', '8',\n",
            "       '32', '29', '34', '28', '45', '2', 'Total_PP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_pp_RF_less_restricted\n",
            "Index(['Year', '55', '1', '42', '20', '38', '31', '23_1', '58', '43', '19',\n",
            "       '23', '33', '10', '50', '23_2', '49', '25', '134', '94_1', '12', '2_11',\n",
            "       '15', '5', '41', '147', '30', '11', '16', '138', '72_1', '35', '3',\n",
            "       '46', '53', '45', '32', '29', '22', '34', '28', '2', 'Total_PP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_pp_RF_more_restricted_regression\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '3',\n",
            "       '23_2', '46', '52', '42', '37', '53', '27', '1_11', '49', '30', '20',\n",
            "       '38', '140', '44', '11', '25', '16', '31', '18', '133', '144', '137',\n",
            "       '8', '36', '23_1', '51', '134', '58', '138', '72_1', '32', '94_1', '12',\n",
            "       '1_8', '43', '2_11', '29', '22', '15', '19', '34', '28', '56', '45',\n",
            "       '23', '5', '2', '33', 'Total_PP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_pp_RF_more_restricted\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '3',\n",
            "       '23_2', '46', '52', '42', '37', '53', '27', '1_11', '49', '30', '20',\n",
            "       '38', '140', '44', '11', '25', '16', '31', '18', '133', '144', '8',\n",
            "       '36', '23_1', '51', '134', '58', '138', '72_1', '32', '94_1', '12',\n",
            "       '1_8', '43', '2_11', '29', '22', '15', '19', '34', '28', '56', '45',\n",
            "       '23', '5', '2', '33', 'Total_PP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_pp_RF_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '31', '18', '144', '23_1',\n",
            "       '51', '58', '43', '19', '23', '33', '10', '7', '50', '23_2', '37', '49',\n",
            "       '140', '44', '25', '134', '94_1', '12', '2_11', '15', '56', '5', '41',\n",
            "       '4', '147', '52', '30', '11', '16', '36', '138', '72_1', '54', '35',\n",
            "       '3', '46', '27', '45', '8', '32', '29', '22', '34', '28', 'Total_PP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_pp_DT_more_restricted\n",
            "Index(['Year', '10', '24', '41', '54', '55', '7', '35', '50', '1', '147', '3',\n",
            "       '23_2', '46', '52', '37', '53', '27', '5', '49', '30', '20', '38', '44',\n",
            "       '11', '25', '31', '16', '18', '144', '36', '23_1', '134', '58', '138',\n",
            "       '72_1', '32', '94_1', '12', '1_8', '43', '2_11', '29', '22', '15', '19',\n",
            "       '34', '28', '45', '23', '2', '33', 'Total_PP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_pp_DT_restricted\n",
            "Index(['Year', '24', '1', '42', '20', '38', '31', '18', '23_1', '58', '43',\n",
            "       '19', '23', '33', '10', '7', '50', '23_2', '49', '140', '25', '134',\n",
            "       '94_1', '12', '2_11', '15', '56', '5', '41', '4', '147', '52', '30',\n",
            "       '11', '16', '36', '138', '72_1', '39', '54', '35', '3', '46', '53',\n",
            "       '27', '32', '29', '22', '34', '28', 'Total_PP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_pp_DT_less_restricted\n",
            "Index(['Year', '1', '42', '20', '38', '31', '18', '144', '23_1', '58', '43',\n",
            "       '19', '139', '23', '33', '10', '7', '50', '23_2', '37', '94_1', '12',\n",
            "       '15', '41', '30', '11', '16', '36', '138', '72_1', '54', '35', '3',\n",
            "       '46', '53', '8', '32', '29', '22', '34', '28', '2', 'Total_PP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_pp_Lasso_more_restricted_regression\n",
            "Index(['Year', '10', '24', '41', '54', '7', '4', '55', '35', '50', '1', '147',\n",
            "       '3', '23_2', '46', '52', '42', '37', '53', '27', '45', '1_11', '122_1',\n",
            "       '49', '30', '20', '38', '140', '44', '11', '25', '16', '31', '18',\n",
            "       '133', '9', '144', '8', '36', '23_1', '51', '134', '58', '138', '72_1',\n",
            "       '32', '94_1', '12', '43', '2_11', '29', '22', '15', '19', '34', '28',\n",
            "       '56', '139', '23', '5', '39', '2', '33', 'Total_PP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_pp_FR_more_restricted_regression\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '46', '52',\n",
            "       '42', '37', '53', '27', '45', '1_11', '49', '30', '20', '135', '38',\n",
            "       '140', '44', '9', '25', '31', '133', '18', '144', '137', '8', '36',\n",
            "       '51', '32', '94_1', '12', '1_8', '43', '29', '19', '34', '28', '56',\n",
            "       '139', '23', '5', '39', '2', '33', 'Total_PP'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_spa_MI_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '23_1', '51', '58', '43', '19', '139', '23', '33', '10', '7',\n",
            "       '50', '37', '1_11', '49', '135', '140', '44', '25', '94_1', '12', '15',\n",
            "       '56', '5', '41', '4', '147', '52', '122_1', '30', '11', '137', '36',\n",
            "       '72_1', '39', '35', '46', '53', '27', '8', '32', '29', '34', '28', '45',\n",
            "       '2', 'Total_SPA'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_spa_MI_more_restricted\n",
            "Index(['Year', '10', '24', '41', '54', '55', '7', '35', '50', '1', '147', '46',\n",
            "       '52', '42', '37', '53', '27', '45', '1_11', '5', '49', '30', '20',\n",
            "       '135', '38', '140', '44', '11', '25', '31', '133', '18', '9', '144',\n",
            "       '137', '8', '36', '23_1', '51', '58', '32', '94_1', '12', '1_8', '43',\n",
            "       '29', '22', '19', '34', '28', '56', '139', '23', '39', '33',\n",
            "       'Total_SPA'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_spa_MI_less_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '23_1', '51', '58', '43', '19', '139', '23', '33', '10', '7',\n",
            "       '50', '37', '1_11', '49', '135', '140', '44', '25', '94_1', '12', '15',\n",
            "       '56', '5', '41', '4', '147', '52', '122_1', '30', '11', '137', '36',\n",
            "       '72_1', '39', '35', '46', '53', '27', '8', '32', '29', '34', '28', '45',\n",
            "       '2', 'Total_SPA'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_spa_RF_more_restricted_regression\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '3',\n",
            "       '23_2', '46', '52', '42', '37', '53', '27', '49', '30', '20', '135',\n",
            "       '38', '140', '44', '11', '25', '31', '16', '18', '133', '9', '144',\n",
            "       '137', '8', '36', '23_1', '51', '134', '58', '138', '72_1', '32',\n",
            "       '94_1', '12', '43', '2_11', '29', '22', '15', '19', '34', '28', '56',\n",
            "       '45', '23', '5', '2', '33', 'Total_SPA'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_spa_RF_more_restricted\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '3',\n",
            "       '23_2', '46', '52', '42', '37', '53', '27', '49', '30', '20', '135',\n",
            "       '38', '140', '44', '11', '25', '31', '16', '18', '133', '144', '9', '8',\n",
            "       '36', '23_1', '51', '134', '58', '138', '72_1', '32', '94_1', '12',\n",
            "       '43', '2_11', '22', '15', '19', '34', '28', '56', '45', '23', '5', '2',\n",
            "       '33', 'Total_SPA'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_spa_RF_less_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '23_1', '51', '58', '43', '19', '23', '33', '10', '7', '50',\n",
            "       '23_2', '49', '135', '140', '44', '25', '134', '94_1', '12', '2_11',\n",
            "       '15', '56', '5', '41', '147', '52', '30', '11', '36', '138', '72_1',\n",
            "       '39', '35', '46', '53', '27', '32', '29', '22', '34', '28', '45', '2',\n",
            "       'Total_SPA'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_spa_RF_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '20', '38', '9', '31', '133', '18',\n",
            "       '144', '23_1', '51', '58', '43', '19', '23', '33', '10', '7', '50',\n",
            "       '23_2', '49', '135', '140', '44', '25', '134', '94_1', '12', '2_11',\n",
            "       '15', '56', '5', '41', '147', '52', '30', '11', '36', '138', '72_1',\n",
            "       '39', '35', '46', '53', '27', '32', '29', '22', '34', '28', '45', '2',\n",
            "       'Total_SPA'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_spa_DT_less_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '38', '31', '133', '18', '144', '23_1',\n",
            "       '58', '43', '19', '23', '33', '10', '7', '50', '23_2', '49', '44', '25',\n",
            "       '134', '94_1', '12', '15', '41', '147', '52', '30', '11', '16', '36',\n",
            "       '138', '35', '3', '46', '53', '27', '32', '29', '22', '34', '28', '45',\n",
            "       '2', 'Total_SPA'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_spa_DT_restricted\n",
            "Index(['Year', '24', '55', '1', '42', '38', '31', '133', '18', '144', '23_1',\n",
            "       '58', '43', '19', '23', '33', '10', '7', '50', '23_2', '49', '44', '25',\n",
            "       '134', '94_1', '12', '15', '41', '147', '52', '30', '11', '16', '36',\n",
            "       '138', '35', '3', '46', '53', '27', '32', '29', '22', '34', '28', '45',\n",
            "       '2', 'Total_SPA'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_spa_DT_more_restricted\n",
            "Index(['Year', '10', '24', '41', '54', '7', '35', '50', '1', '147', '3',\n",
            "       '23_2', '46', '52', '42', '37', '53', '27', '1_11', '49', '30', '135',\n",
            "       '38', '44', '11', '25', '31', '16', '18', '133', '144', '36', '23_1',\n",
            "       '51', '134', '58', '138', '72_1', '32', '94_1', '12', '43', '29', '22',\n",
            "       '15', '19', '34', '28', '56', '45', '23', '5', '39', '2', '33',\n",
            "       'Total_SPA'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_spa_Lasso_more_restricted_regression\n",
            "Index(['Year', '10', '24', '41', '54', '7', '4', '55', '35', '50', '1', '147',\n",
            "       '3', '23_2', '46', '52', '42', '37', '53', '27', '1_11', '122_1', '49',\n",
            "       '30', '20', '38', '140', '44', '11', '25', '16', '31', '18', '133', '9',\n",
            "       '144', '8', '36', '23_1', '51', '134', '58', '72_1', '32', '94_1', '12',\n",
            "       '1_8', '43', '29', '22', '15', '19', '34', '28', '56', '45', '23', '5',\n",
            "       '39', '2', '33', 'Total_SPA'],\n",
            "      dtype='object')\n",
            "7890\n",
            "Replaced dataset: predict_total_spa_FR_more_restricted_regression\n",
            "Index(['Year', '10', '24', '41', '7', '55', '35', '50', '1', '147', '46', '52',\n",
            "       '42', '37', '53', '27', '45', '1_11', '49', '30', '20', '135', '38',\n",
            "       '140', '44', '9', '25', '31', '133', '18', '144', '137', '8', '36',\n",
            "       '51', '32', '94_1', '12', '1_8', '43', '29', '19', '34', '28', '56',\n",
            "       '139', '23', '5', '39', '2', '33', 'Total_SPA'],\n",
            "      dtype='object')\n",
            "7890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQp9Bzjah_zw",
        "outputId": "ab8a7e17-f210-4716-a30d-f2d4f62413ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of loaded datasets = 59\n",
            "Number of MI datasets = 12\n",
            "Number of RF datasets = 18\n",
            "Number of DT datasets = 12\n",
            "Number of Lasso datasets = 6\n",
            "Number of FR datasets = 6\n",
            "Number of datasets for predicting Passed column = 9\n",
            "Number of datasets for predicting FinalGrade column = 9\n",
            "Number of datasets for predicting Total_AP column = 12\n",
            "Number of datasets for predicting Total_PP column = 12\n",
            "Number of datasets for predicting Total_SPA column = 12\n",
            "Available datasets: ['predict_finalgrade_Lasso_less_restricted', 'predict_total_ap_Lasso_more_restricted_regression', 'predict_total_ap_RF_more_restricted', 'predict_passed_DT_less_restricted', 'predict_total_pp_RF_less_restricted', 'predict_total_spa_DT_less_restricted', 'combined_df_Passed', 'predict_passed_RF_more_restricted', 'predict_total_spa_DT_restricted', 'predict_total_pp_MI_more_restricted', 'predict_total_spa_MI_restricted', 'predict_total_pp_MI_less_restricted', 'predict_finalgrade_Lasso_restricted', 'predict_total_pp_DT_more_restricted', 'predict_total_ap_RF_less_restricted', 'predict_total_ap_MI_more_restricted', 'predict_finalgrade_FR_more_restricted', 'predict_total_spa_MI_more_restricted', 'predict_total_pp_Lasso_more_restricted_regression', 'predict_total_ap_DT_more_restricted', 'predict_total_pp_RF_more_restricted_regression', 'predict_passed_RF_restricted', 'predict_total_spa_MI_less_restricted', 'predict_total_ap_MI_less_restricted', 'combined_df_Total_SPA', 'predict_passed_MI_restricted', 'predict_total_spa_DT_more_restricted', 'predict_total_pp_MI_restricted', 'predict_total_ap_DT_restricted', 'predict_total_ap_RF_restricted', 'predict_passed_MI_more_restricted', 'predict_passed_RF_less_restricted', 'predict_total_spa_FR_more_restricted_regression', 'predict_total_ap_DT_less_restricted', 'filtered_combined_df_FinalGrade', 'combined_df_Total_AP', 'predict_total_ap_FR_more_restricted_regression', 'predict_total_spa_RF_more_restricted_regression', 'predict_finalgrade_Lasso_more_restricted', 'predict_total_pp_RF_more_restricted', 'predict_finalgrade_RF_restricted', 'predict_finalgrade_FR_restricted', 'predict_total_pp_RF_restricted', 'predict_passed_DT_more_restricted', 'predict_total_spa_RF_more_restricted', 'predict_passed_DT_restricted', 'predict_total_pp_FR_more_restricted_regression', 'predict_finalgrade_FR_less_restricted', 'combined_df_Total_PP', 'predict_total_spa_RF_less_restricted', 'predict_finalgrade_RF_less_restricted', 'predict_total_ap_RF_more_restricted_regression', 'predict_finalgrade_RF_more_restricted', 'predict_passed_MI_less_restricted', 'predict_total_pp_DT_restricted', 'predict_total_ap_MI_restricted', 'predict_total_pp_DT_less_restricted', 'predict_total_spa_Lasso_more_restricted_regression', 'predict_total_spa_RF_restricted']\n",
            "MI FinalGrade datasets: []\n",
            "RF FinalGrade datasets: ['predict_finalgrade_RF_restricted', 'predict_finalgrade_RF_less_restricted', 'predict_finalgrade_RF_more_restricted']\n",
            "DT FinalGrade datasets: []\n",
            "Lasso FinalGrade datasets: ['predict_finalgrade_Lasso_less_restricted', 'predict_finalgrade_Lasso_restricted', 'predict_finalgrade_Lasso_more_restricted']\n",
            "FR FinalGrade datasets: ['predict_finalgrade_FR_more_restricted', 'predict_finalgrade_FR_restricted', 'predict_finalgrade_FR_less_restricted']\n",
            "MI Passed datasets: ['predict_passed_MI_restricted', 'predict_passed_MI_more_restricted', 'predict_passed_MI_less_restricted']\n",
            "RF Passed datasets: ['predict_passed_RF_more_restricted', 'predict_passed_RF_restricted', 'predict_passed_RF_less_restricted']\n",
            "DT Passed datasets: ['predict_passed_DT_less_restricted', 'predict_passed_DT_more_restricted', 'predict_passed_DT_restricted']\n",
            "Lasso Passed datasets: []\n",
            "FR Passed datasets: []\n",
            "Number of MI Total_AP datasets = 3\n",
            "Number of RF Total_AP datasets = 4\n",
            "Number of DT Total_AP datasets = 3\n",
            "Number of Lasso Total_AP datasets = 1\n",
            "Number of FR Total_AP datasets = 1\n",
            "Number of MI Total_PP datasets = 3\n",
            "Number of RF Total_PP datasets = 4\n",
            "Number of DT Total_PP datasets = 3\n",
            "Number of Lasso Total_PP datasets = 1\n",
            "Number of FR Total_PP datasets = 1\n",
            "Number of MI Total_SPA datasets = 3\n",
            "Number of RF Total_SPA datasets = 4\n",
            "Number of DT Total_SPA datasets = 3\n",
            "Number of Lasso Total_SPA datasets = 1\n",
            "Number of FR Total_SPA datasets = 1\n",
            "Number of datasets without feature selection = 5\n"
          ]
        }
      ],
      "source": [
        "# Filter datasets based on the target column\n",
        "passed_datasets = {name: df for name, df in loaded_datasets.items() if 'predict_passed' in name.lower()}\n",
        "finalgrade_datasets = {name: df for name, df in loaded_datasets.items() if 'predict_finalgrade' in name.lower()}\n",
        "total_ap_datasets = {name: df for name, df in loaded_datasets.items() if 'predict_total_ap' in name.lower()}\n",
        "total_pp_datasets = {name: df for name, df in loaded_datasets.items() if 'predict_total_pp' in name.lower()}\n",
        "total_spa_datasets = {name: df for name, df in loaded_datasets.items() if 'predict_total_spa' in name.lower()}\n",
        "without_feature_selection_datasets = {name: df for name, df in loaded_datasets.items() if 'combined_df' in name.lower()}\n",
        "\n",
        "# Combine feature selection methods with target columns\n",
        "mi_finalgrade_datasets = {name: df for name, df in mi_datasets.items() if 'predict_finalgrade' in name.lower()}\n",
        "rf_finalgrade_datasets = {name: df for name, df in rf_datasets.items() if 'predict_finalgrade' in name.lower()}\n",
        "dt_finalgrade_datasets = {name: df for name, df in dt_datasets.items() if 'predict_finalgrade' in name.lower()}\n",
        "lasso_finalgrade_datasets = {name: df for name, df in lasso_datasets.items() if 'predict_finalgrade' in name.lower()}\n",
        "fr_finalgrade_datasets = {name: df for name, df in fr_datasets.items() if 'predict_finalgrade' in name.lower()}\n",
        "\n",
        "mi_passed_datasets = {name: df for name, df in mi_datasets.items() if 'predict_passed' in name.lower()}\n",
        "rf_passed_datasets = {name: df for name, df in rf_datasets.items() if 'predict_passed' in name.lower()}\n",
        "dt_passed_datasets = {name: df for name, df in dt_datasets.items() if 'predict_passed' in name.lower()}\n",
        "lasso_passed_datasets = {name: df for name, df in lasso_datasets.items() if 'predict_passed' in name.lower()}\n",
        "fr_passed_datasets = {name: df for name, df in fr_datasets.items() if 'predict_passed' in name.lower()}\n",
        "\n",
        "mi_total_ap_datasets = {name: df for name, df in mi_datasets.items() if 'predict_total_ap' in name.lower()}\n",
        "rf_total_ap_datasets = {name: df for name, df in rf_datasets.items() if 'predict_total_ap' in name.lower()}\n",
        "dt_total_ap_datasets = {name: df for name, df in dt_datasets.items() if 'predict_total_ap' in name.lower()}\n",
        "lasso_total_ap_datasets = {name: df for name, df in lasso_datasets.items() if 'predict_total_ap' in name.lower()}\n",
        "fr_total_ap_datasets = {name: df for name, df in fr_datasets.items() if 'predict_total_ap' in name.lower()}\n",
        "\n",
        "mi_total_pp_datasets = {name: df for name, df in mi_datasets.items() if 'predict_total_pp' in name.lower()}\n",
        "rf_total_pp_datasets = {name: df for name, df in rf_datasets.items() if 'predict_total_pp' in name.lower()}\n",
        "dt_total_pp_datasets = {name: df for name, df in dt_datasets.items() if 'predict_total_pp' in name.lower()}\n",
        "lasso_total_pp_datasets = {name: df for name, df in lasso_datasets.items() if 'predict_total_pp' in name.lower()}\n",
        "fr_total_pp_datasets = {name: df for name, df in fr_datasets.items() if 'predict_total_pp' in name.lower()}\n",
        "\n",
        "mi_total_spa_datasets = {name: df for name, df in mi_datasets.items() if 'predict_total_spa' in name.lower()}\n",
        "rf_total_spa_datasets = {name: df for name, df in rf_datasets.items() if 'predict_total_spa' in name.lower()}\n",
        "dt_total_spa_datasets = {name: df for name, df in dt_datasets.items() if 'predict_total_spa' in name.lower()}\n",
        "lasso_total_spa_datasets = {name: df for name, df in lasso_datasets.items() if 'predict_total_spa' in name.lower()}\n",
        "fr_total_spa_datasets = {name: df for name, df in fr_datasets.items() if 'predict_total_spa' in name.lower()}\n",
        "\n",
        "# Print the count of loaded datasets in each category\n",
        "print(f'Number of loaded datasets = {len(loaded_datasets)}')\n",
        "print(f'Number of MI datasets = {len(mi_datasets)}')\n",
        "print(f'Number of RF datasets = {len(rf_datasets)}')\n",
        "print(f'Number of DT datasets = {len(dt_datasets)}')\n",
        "print(f'Number of Lasso datasets = {len(lasso_datasets)}')\n",
        "print(f'Number of FR datasets = {len(fr_datasets)}')\n",
        "\n",
        "print(f'Number of datasets for predicting Passed column = {len(passed_datasets)}')\n",
        "print(f'Number of datasets for predicting FinalGrade column = {len(finalgrade_datasets)}')\n",
        "print(f'Number of datasets for predicting Total_AP column = {len(total_ap_datasets)}')\n",
        "print(f'Number of datasets for predicting Total_PP column = {len(total_pp_datasets)}')\n",
        "print(f'Number of datasets for predicting Total_SPA column = {len(total_spa_datasets)}')\n",
        "\n",
        "print(f\"Available datasets: {list(loaded_datasets.keys())}\")\n",
        "print(f\"MI FinalGrade datasets: {list(mi_finalgrade_datasets.keys())}\")\n",
        "print(f\"RF FinalGrade datasets: {list(rf_finalgrade_datasets.keys())}\")\n",
        "print(f\"DT FinalGrade datasets: {list(dt_finalgrade_datasets.keys())}\")\n",
        "print(f\"Lasso FinalGrade datasets: {list(lasso_finalgrade_datasets.keys())}\")\n",
        "print(f\"FR FinalGrade datasets: {list(fr_finalgrade_datasets.keys())}\")\n",
        "\n",
        "print(f\"MI Passed datasets: {list(mi_passed_datasets.keys())}\")\n",
        "print(f\"RF Passed datasets: {list(rf_passed_datasets.keys())}\")\n",
        "print(f\"DT Passed datasets: {list(dt_passed_datasets.keys())}\")\n",
        "print(f\"Lasso Passed datasets: {list(lasso_passed_datasets.keys())}\")\n",
        "print(f\"FR Passed datasets: {list(fr_passed_datasets.keys())}\")\n",
        "\n",
        "print(f'Number of MI Total_AP datasets = {len(mi_total_ap_datasets)}')\n",
        "print(f'Number of RF Total_AP datasets = {len(rf_total_ap_datasets)}')\n",
        "print(f'Number of DT Total_AP datasets = {len(dt_total_ap_datasets)}')\n",
        "print(f'Number of Lasso Total_AP datasets = {len(lasso_total_ap_datasets)}')\n",
        "print(f'Number of FR Total_AP datasets = {len(fr_total_ap_datasets)}')\n",
        "\n",
        "print(f'Number of MI Total_PP datasets = {len(mi_total_pp_datasets)}')\n",
        "print(f'Number of RF Total_PP datasets = {len(rf_total_pp_datasets)}')\n",
        "print(f'Number of DT Total_PP datasets = {len(dt_total_pp_datasets)}')\n",
        "print(f'Number of Lasso Total_PP datasets = {len(lasso_total_pp_datasets)}')\n",
        "print(f'Number of FR Total_PP datasets = {len(fr_total_pp_datasets)}')\n",
        "\n",
        "print(f'Number of MI Total_SPA datasets = {len(mi_total_spa_datasets)}')\n",
        "print(f'Number of RF Total_SPA datasets = {len(rf_total_spa_datasets)}')\n",
        "print(f'Number of DT Total_SPA datasets = {len(dt_total_spa_datasets)}')\n",
        "print(f'Number of Lasso Total_SPA datasets = {len(lasso_total_spa_datasets)}')\n",
        "print(f'Number of FR Total_SPA datasets = {len(fr_total_spa_datasets)}')\n",
        "\n",
        "print(f'Number of datasets without feature selection = {len(without_feature_selection_datasets)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1eqq4aiCIvS"
      },
      "source": [
        "# 3. Auxiliary Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQOy2Zfth96u"
      },
      "source": [
        "###### evaluate_combined_dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTU48g78iBU9"
      },
      "outputs": [],
      "source": [
        "def evaluate_combined_dfs(datasets):\n",
        "    \"\"\"\n",
        "    Evaluate the state of combined dataframes.\n",
        "\n",
        "    Args:\n",
        "    - datasets (dict): Dictionary containing the dataframes to be evaluated.\n",
        "\n",
        "    Returns:\n",
        "    - summary (dict): Dictionary containing the evaluation summary of each dataframe.\n",
        "    \"\"\"\n",
        "    summary = {}\n",
        "\n",
        "    # Loop through each dataframe in the provided datasets\n",
        "    for df_name, df in datasets.items():\n",
        "        if 'combined_df' in df_name:\n",
        "            print(f\"Evaluating dataframe: {df_name}\")\n",
        "            df_summary = {}\n",
        "\n",
        "            # Shape of the dataframe\n",
        "            df_summary['shape'] = df.shape\n",
        "            print(f\"Shape: {df.shape}\")\n",
        "\n",
        "            # Checking for missing data\n",
        "            missing_data = df.isnull().sum()\n",
        "            total_missing = missing_data.sum()\n",
        "            df_summary['total_missing_values'] = total_missing\n",
        "            df_summary['missing_percentage'] = (total_missing / df.size) * 100\n",
        "            print(f\"Total missing values: {total_missing}\")\n",
        "            print(f\"Missing data percentage: {df_summary['missing_percentage']:.2f}%\")\n",
        "\n",
        "            # Numerical and categorical columns\n",
        "            numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "            categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "            df_summary['numerical_columns'] = numerical_columns\n",
        "            df_summary['categorical_columns'] = categorical_columns\n",
        "            print(f\"Numerical columns: {numerical_columns}\")\n",
        "            print(f\"Categorical columns: {categorical_columns}\")\n",
        "\n",
        "            # Adding dataframe summary to the main summary\n",
        "            summary[df_name] = df_summary\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5tgu9shinc0"
      },
      "source": [
        "## 3.1 Output handling methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBwSA0d5CFsV",
        "outputId": "f0f39577-b944-47a7-afbb-71cd05228786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auxiliary methods defined successfully.\n"
          ]
        }
      ],
      "source": [
        "# Custom print function to capture output to a Word document\n",
        "def print_to_doc(*args, **kwargs):\n",
        "    text = ' '.join(map(str, args))\n",
        "    doc.add_paragraph(text)\n",
        "\n",
        "# Custom display function to capture output to a Word document\n",
        "def custom_display(*args, **kwargs):\n",
        "    if output_to_docx == 1:\n",
        "        for arg in args:\n",
        "            if isinstance(arg, pd.DataFrame):\n",
        "                buf = StringIO()\n",
        "                arg.to_string(buf)\n",
        "                text = buf.getvalue()\n",
        "            else:\n",
        "                text = str(arg)\n",
        "            doc.add_paragraph(text)\n",
        "    else:\n",
        "        ipy_display(*args, **kwargs)\n",
        "\n",
        "# Function to save the document with the current date\n",
        "def save_document(filename='output_modeling.docx'):\n",
        "    current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    doc.add_paragraph(f\"Document saved on {current_date}\")\n",
        "    doc.save(filename)\n",
        "    print(f\"Document saved as {filename}\")\n",
        "\n",
        "print(\"Auxiliary methods defined successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIYyxENTmeI4"
      },
      "source": [
        "### Set Printing Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrvA6h9vOAKM",
        "outputId": "c85bcda2-8c6b-4f15-d4fb-435db38b56ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<docx.text.paragraph.Paragraph at 0x7cd6ed591ae0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Change Printing Mode\n",
        "output_to_docx = 0  # Control output (0 for notebook, 1 for docx)\n",
        "\n",
        "if output_to_docx == 1:\n",
        "    print = print_to_doc\n",
        "    display = custom_display\n",
        "\n",
        "# Initialize a Word document to capture output\n",
        "doc = Document()\n",
        "doc.add_heading('Prediction Analysis Report', 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIs9yzOYCUVH"
      },
      "source": [
        "# 4. Feature Selection and Predictive Modeling Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8HVqkiKGYy8"
      },
      "source": [
        "## 4.1 Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvh_Kj30jU_g"
      },
      "source": [
        "###### get_target_column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrYU759-h_s6"
      },
      "outputs": [],
      "source": [
        "# Define the target column based on the dataset name suffix\n",
        "def get_target_column(dataset_name, target_column_mapping):\n",
        "    for key, column in target_column_mapping.items():\n",
        "        if key in dataset_name:\n",
        "            return column\n",
        "    raise ValueError(f\"Unknown target column for dataset: {dataset_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSIfJ-ZEWC5"
      },
      "source": [
        "###### get_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oL5GWNMEZqY"
      },
      "outputs": [],
      "source": [
        "def get_models(task_type, class_weight_dict):\n",
        "    if task_type == 'classification':\n",
        "        return {\n",
        "            'Logistic Regression': LogisticRegression(max_iter=10000, solver='saga', tol=1e-3, random_state=2024, C=0.3, class_weight=class_weight_dict),\n",
        "            'Decision Tree': DecisionTreeClassifier(random_state=2024, class_weight=class_weight_dict),\n",
        "            'Random Forest': RandomForestClassifier(random_state=2024, class_weight=class_weight_dict),\n",
        "            'Naive Bayes': GaussianNB(),\n",
        "            'Support Vector Machine': SVC(random_state=2024, probability=True, class_weight=class_weight_dict),\n",
        "            'k-Nearest Neighbors': KNeighborsClassifier(),\n",
        "            'Neural Network': MLPClassifier(max_iter=1000, random_state=42)\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'Decision Tree Regressor': DecisionTreeRegressor(random_state=2024),\n",
        "            'Random Forest Regressor': RandomForestRegressor(random_state=2024),\n",
        "            'Support Vector Regressor': SVR(),\n",
        "            'k-Nearest Neighbors Regressor': KNeighborsRegressor(),\n",
        "            'Neural Network Regressor': MLPRegressor(max_iter=1000, random_state=2024)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fymEa7HAEbrr"
      },
      "source": [
        "###### get_test_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHFqX9M3EgA8"
      },
      "outputs": [],
      "source": [
        "def get_test_scores(task_type, y_test, y_pred, model, X_test):\n",
        "    if task_type == 'classification':\n",
        "        return {\n",
        "            'Accuracy': accuracy_score(y_test, y_pred),\n",
        "            'F1-Score': f1_score(y_test, y_pred, average='weighted'),\n",
        "            'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "            'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "            'ROC AUC': roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]) if hasattr(model, 'predict_proba') else None\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            'MSE': mean_squared_error(y_test, y_pred),\n",
        "            'R2': r2_score(y_test, y_pred),\n",
        "            'MAE': mean_absolute_error(y_test, y_pred)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VDm4nyKEidk"
      },
      "source": [
        "###### is_better_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA9PTpTLEKgb"
      },
      "outputs": [],
      "source": [
        "def is_better_model(best_model_info, test_scores, task_type):\n",
        "    if task_type == 'classification':\n",
        "        return test_scores['F1-Score'] > best_model_info.get('test_scores', {}).get('F1-Score', 0)\n",
        "    else:\n",
        "        return test_scores['R2'] > best_model_info.get('test_scores', {}).get('R2', 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tVRoO44jqDy"
      },
      "source": [
        "### 4.1.1 Dataset Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1o5XyOkNVV2"
      },
      "source": [
        "###### preprocess_data_with_indicators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98cBcp_SWTSf"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess data with missing data indicators, imputation, and scaling\n",
        "def preprocess_data_with_indicators(df, target_column, apply_pca=False, n_components=None):\n",
        "    X = df.drop(columns=[target_column])  # Remove the target column from the feature set\n",
        "    y = df[target_column]  # Extract the target column\n",
        "\n",
        "    # Create indicator variables for missing data\n",
        "    indicator_columns = pd.get_dummies(X.isnull().astype(int), prefix_sep='_missing_', drop_first=True)\n",
        "\n",
        "    # Impute missing values with zero (this can be adjusted based on the specific strategy needed)\n",
        "    X_imputed = X.fillna(0)\n",
        "\n",
        "    # Combine imputed data with the indicator variables\n",
        "    X_combined = pd.concat([X_imputed, indicator_columns], axis=1)\n",
        "\n",
        "    # Scale numeric features\n",
        "    scaler = StandardScaler()\n",
        "    X_combined_scaled = scaler.fit_transform(X_combined)\n",
        "\n",
        "    # If PCA is requested, apply it to reduce dimensionality\n",
        "    if apply_pca and n_components:\n",
        "        pca = PCA(n_components=n_components)\n",
        "        X_combined_scaled = pca.fit_transform(X_combined_scaled)\n",
        "\n",
        "    return X_combined_scaled, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK810ddOglbd"
      },
      "source": [
        "###### preprocess_data_remove_missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpQVmutGgi0J"
      },
      "outputs": [],
      "source": [
        "def preprocess_data_remove_missing(df, target_column):\n",
        "    \"\"\"\n",
        "    Preprocess data by removing columns with any missing values and applying a variance threshold.\n",
        "\n",
        "    Args:\n",
        "    - df: DataFrame containing the dataset.\n",
        "    - target_column: The name of the target column.\n",
        "\n",
        "    Returns:\n",
        "    - X_train, X_test, y_train, y_test: Processed training and testing data.\n",
        "    \"\"\"\n",
        "    # Separate features and target\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
        "\n",
        "    X_train = X_train.dropna(axis=1)\n",
        "    X_test = X_test[X_train.columns]\n",
        "\n",
        "    # Apply variance threshold to remove low variance features\n",
        "    if X_train.shape[1] == 0 or X_test.shape[1] == 0:\n",
        "        raise ValueError(\"No features remaining after preprocessing. Check data or adjust preprocessing steps.\")\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('variance_threshold', VarianceThreshold(threshold=0.01))  # Adjust the threshold as needed\n",
        "    ])\n",
        "\n",
        "    # Apply the pipeline\n",
        "    X_train = pipeline.fit_transform(X_train)\n",
        "    X_test = pipeline.transform(X_test)\n",
        "\n",
        "    # Check if after the variance threshold, no features remain\n",
        "    if X_train.shape[1] == 0 or X_test.shape[1] == 0:\n",
        "        raise ValueError(\"All features removed after applying variance threshold. Adjust the threshold or preprocessing.\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQao7MeKimsZ"
      },
      "source": [
        "###### preprocess_data_with_custom_imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy__33ebilyI"
      },
      "outputs": [],
      "source": [
        "def preprocess_data_with_custom_imputation(df, target_column, imputation_strategy='', apply_pca=False, n_components=None, apply_scaling=True, columns_to_scale=None, group_col='Year'):\n",
        "    \"\"\"\n",
        "    Preprocess data with custom imputation, optional Min-Max scaling, and PCA, ensuring no data leakage.\n",
        "\n",
        "    Args:\n",
        "    - df: DataFrame containing the dataset.\n",
        "    - target_column: The name of the target column.\n",
        "    - imputation_strategy: Strategy for imputing missing values ('mean', 'median', 'zero', 'knn', 'most_frequent', 'constant'). If empty or None, uses remove_missing approach.\n",
        "    - apply_pca: Whether to apply PCA for dimensionality reduction.\n",
        "    - n_components: Number of PCA components if PCA is applied.\n",
        "    - apply_scaling: Whether to apply Min-Max scaling after imputation.\n",
        "    - columns_to_scale: List of columns to scale if apply_scaling is True.\n",
        "    - group_col: Column to group by before applying scaling, if applicable. Defaults to 'Year'.\n",
        "\n",
        "    Returns:\n",
        "    - X_train, X_test, y_train, y_test: Processed training and testing data.\n",
        "    \"\"\"\n",
        "\n",
        "    # If no imputation strategy is provided, use preprocess_data_remove_missing\n",
        "    if not imputation_strategy:\n",
        "        return preprocess_data_remove_missing(df, target_column)\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
        "\n",
        "    # Choose the imputation strategy\n",
        "    if imputation_strategy == 'mean':\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "    elif imputation_strategy == 'median':\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "    elif imputation_strategy == 'zero':\n",
        "        imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
        "    elif imputation_strategy == 'knn':\n",
        "        imputer = KNNImputer(n_neighbors=5)\n",
        "    elif imputation_strategy == 'most_frequent':\n",
        "        imputer = SimpleImputer(strategy='most_frequent')\n",
        "    elif imputation_strategy == 'constant':\n",
        "        imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported imputation strategy. Choose from 'mean', 'median', 'zero', 'knn', 'most_frequent', or 'constant'.\")\n",
        "\n",
        "    # Apply imputation\n",
        "    X_train = imputer.fit_transform(X_train)\n",
        "    X_test = imputer.transform(X_test)\n",
        "\n",
        "    if apply_scaling:\n",
        "        scaler = MinMaxScaler()\n",
        "\n",
        "        # Check if the group_col exists in the dataframe before applying scaling\n",
        "        if group_col in df.columns and columns_to_scale:\n",
        "            print(f\"Applying scaling grouped by '{group_col}'.\")\n",
        "            # Apply scaling within each group\n",
        "            X_train = pd.DataFrame(X_train, columns=df.drop(columns=[target_column]).columns)\n",
        "            X_test = pd.DataFrame(X_test, columns=df.drop(columns=[target_column]).columns)\n",
        "\n",
        "            X_train[columns_to_scale] = X_train.groupby(group_col)[columns_to_scale].transform(lambda x: scaler.fit_transform(x))\n",
        "            X_test[columns_to_scale] = X_test.groupby(group_col)[columns_to_scale].transform(lambda x: scaler.transform(x))\n",
        "        else:\n",
        "            if group_col not in df.columns:\n",
        "                print(f\"Column '{group_col}' not found. Scaling without grouping.\")\n",
        "            else:\n",
        "                print(f\"No specific columns to scale provided. Scaling all columns except the target.\")\n",
        "\n",
        "            # Apply scaling to the entire dataset or selected columns\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Optionally apply PCA\n",
        "    if apply_pca and n_components:\n",
        "        pca = PCA(n_components=n_components)\n",
        "        X_train = pca.fit_transform(X_train)\n",
        "        X_test = pca.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWa_fArqjnPC"
      },
      "source": [
        "### 4.1.2 Model Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH74-n-0fGSS"
      },
      "source": [
        "###### perform_grid_search_cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYECpBlpfFQ9"
      },
      "outputs": [],
      "source": [
        "# Function to perform grid search cross-validation\n",
        "def perform_grid_search_cv(model, param_grid, X, y, cv=5):\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=cv, n_jobs=-1, error_score='raise')\n",
        "    grid_search.fit(X, y)\n",
        "    return grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4z1DgUNH4d0"
      },
      "source": [
        "###### evaluate_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kDyhUNYHTAS"
      },
      "outputs": [],
      "source": [
        "# Define a function to evaluate a model and print the classification report and accuracy score\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    print(classification_report(y_test, predictions, zero_division=0))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))\n",
        "    print(\"Accuracy Score:\", accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDH4TGGwGhEQ"
      },
      "source": [
        "###### perform_cross_validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_HOuwIIh_XM"
      },
      "outputs": [],
      "source": [
        "# Define a function to perform cross-validation and return the scores\n",
        "def perform_cross_validation(model, X, y, cv=5):\n",
        "    scores = cross_val_score(model, X, y, cv=cv)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpZU6YPhacNH"
      },
      "source": [
        "###### convert_to_multiclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh1uBPN7aavk"
      },
      "outputs": [],
      "source": [
        "def convert_to_multiclass(y, bins=3, strategy='uniform'):\n",
        "    \"\"\"\n",
        "    Convert a continuous target variable into a multiclass target by binning the values.\n",
        "\n",
        "    Args:\n",
        "    - y (pd.Series or np.ndarray): The continuous target variable.\n",
        "    - bins (int): The number of bins to create for multiclass conversion.\n",
        "    - strategy (str): The binning strategy ('uniform', 'quantile', 'kmeans').\n",
        "\n",
        "    Returns:\n",
        "    - y_multiclass (pd.Series or np.ndarray): The multiclass target variable.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        labels = range(bins)\n",
        "\n",
        "        if strategy == 'uniform':\n",
        "            y_multiclass = pd.cut(y, bins=bins, labels=labels)\n",
        "        elif strategy == 'quantile':\n",
        "            y_multiclass = pd.qcut(y, q=bins, labels=labels, duplicates='drop')\n",
        "        elif strategy == 'kmeans':\n",
        "            from sklearn.cluster import KMeans\n",
        "            kmeans = KMeans(n_clusters=bins)\n",
        "            y_multiclass = kmeans.fit_predict(y.values.reshape(-1, 1))\n",
        "            y_multiclass = pd.Series(y_multiclass).map(dict(enumerate(labels)))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported strategy: {strategy}\")\n",
        "\n",
        "        if len(pd.unique(y_multiclass)) < bins:\n",
        "            print(f\"Warning: Only {len(pd.unique(y_multiclass))} unique bins were created instead of {bins}.\")\n",
        "\n",
        "        return y_multiclass\n",
        "    except ValueError as e:\n",
        "        print(f\"Error in convert_to_multiclass: {e}\")\n",
        "        return y  #\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFkqIa9vgspB"
      },
      "source": [
        "###### determine_task_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCFCJb9Vgqjl"
      },
      "outputs": [],
      "source": [
        "# Function to determine task type\n",
        "def determine_task_type(target_column):\n",
        "    if target_column == 'Passed':\n",
        "        return 'classification'\n",
        "    elif target_column in ['Total_AP', 'Total_PP', 'Total_SPA']:\n",
        "        return 'both'\n",
        "    elif target_column == 'FinalGrade':\n",
        "        return 'regression'\n",
        "    else:\n",
        "        return 'both'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dyys72afZbD"
      },
      "source": [
        "###### train_and_evaluate_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vhrsp_aoWW3B"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_models(X_train, X_test, y_train, y_test, task_type='classification', scoring=None, random_state=2024, cv=5):\n",
        "    # Define models for classification and regression with random_state where applicable\n",
        "    if task_type == 'classification':\n",
        "        models = {\n",
        "            'Logistic Regression': LogisticRegression(solver='saga', max_iter=1000, random_state=random_state),\n",
        "            'Decision Tree': DecisionTreeClassifier(random_state=random_state),\n",
        "            'Random Forest': RandomForestClassifier(random_state=random_state),\n",
        "            'Support Vector Machine': SVC(probability=True, random_state=random_state),\n",
        "            'k-Nearest Neighbors': KNeighborsClassifier(),\n",
        "            'Neural Network': MLPClassifier(max_iter=1000, random_state=random_state)\n",
        "        }\n",
        "    elif task_type == 'regression':\n",
        "        models = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'Decision Tree Regressor': DecisionTreeRegressor(random_state=random_state),\n",
        "            'Random Forest Regressor': RandomForestRegressor(random_state=random_state),\n",
        "            'Support Vector Machine': SVR(),\n",
        "            'k-Nearest Neighbors': KNeighborsRegressor(),\n",
        "            'Neural Network': MLPRegressor(max_iter=1000, random_state=random_state)\n",
        "        }\n",
        "\n",
        "    # Refined hyperparameter grids based on your findings and best practices\n",
        "    param_grids = {\n",
        "        'Logistic Regression': {'C': [0.1, 0.15, 0.3]},\n",
        "        'Decision Tree': {'max_depth': [5, 10, None]},\n",
        "        'Random Forest': {'n_estimators': [100, 200], 'max_depth': [10, 20, None]},\n",
        "        'Support Vector Machine': {'C': [0.1, 0.3, 1]},\n",
        "        'k-Nearest Neighbors': {'n_neighbors': [3, 5, 7]},\n",
        "        'Neural Network': {'hidden_layer_sizes': [(50,), (100,)]}\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    test_scores = {}\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"Evaluating model: {model_name}\")\n",
        "\n",
        "        # Perform grid search if the model has a hyperparameter grid\n",
        "        if model_name in param_grids:\n",
        "            grid_search = GridSearchCV(model, param_grids[model_name], cv=cv, scoring=scoring)\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            model = grid_search.best_estimator_\n",
        "            print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
        "\n",
        "        # Continue with evaluation after optimization\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\n",
        "        results[model_name] = scores\n",
        "        print(f\"Cross-validation scores for {model_name}: {scores}\")\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        if task_type == 'classification':\n",
        "            test_scores[model_name] = {\n",
        "                'Accuracy': accuracy_score(y_test, y_pred),\n",
        "                'F1-Score': f1_score(y_test, y_pred, average='weighted')\n",
        "            }\n",
        "        elif task_type == 'regression':\n",
        "            test_scores[model_name] = {\n",
        "                'MSE': mean_squared_error(y_test, y_pred),\n",
        "                'R2': r2_score(y_test, y_pred),\n",
        "                'MAE': mean_absolute_error(y_test, y_pred)\n",
        "            }\n",
        "\n",
        "        print(f\"Test set scores for {model_name}: {test_scores[model_name]}\")\n",
        "\n",
        "    return results, test_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6-MdeOF9XR7"
      },
      "source": [
        "###### analyze_datasets_for_target_column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i-5d181Rar3"
      },
      "outputs": [],
      "source": [
        "def analyze_datasets_for_target_column(datasets_dict, target_column, exclusion_type, feature_set_name=None, imputation_strategy=None, apply_pca=False, apply_scaling=True, columns_to_scale=None, group_col=None, cv=5):\n",
        "    \"\"\"\n",
        "    Analyze datasets for a specific target column. Optionally apply feature selection and exclusions.\n",
        "\n",
        "    Parameters:\n",
        "    - datasets_dict: Dictionary containing the datasets.\n",
        "    - target_column: The target column to predict (e.g., 'Total_AP').\n",
        "    - exclusion_type: The exclusion type to filter datasets (e.g., 'more_restricted').\n",
        "    - feature_set_name: The feature selection method to use (e.g., 'MI'). If None, use the original dataset without feature selection.\n",
        "    - imputation_strategy: Strategy for imputing missing data.\n",
        "    - apply_pca: Boolean to apply PCA for dimensionality reduction.\n",
        "    - apply_scaling: Boolean to apply scaling to the features.\n",
        "    - columns_to_scale: Specific columns to scale (optional).\n",
        "    - group_col: Column to group by (optional).\n",
        "    - cv: Number of cross-validation folds.\n",
        "\n",
        "    Returns:\n",
        "    - results: Dictionary with the results of the analysis.\n",
        "    - best_model_info: Tuple containing information about the best regression and classification models.\n",
        "    \"\"\"\n",
        "    print(f\"Processing datasets for target column: {target_column} with exclusion type: {exclusion_type}\")\n",
        "\n",
        "    # Filter datasets based on exclusion type, feature set, and target column\n",
        "    target_column_lower = target_column.lower()\n",
        "\n",
        "    if feature_set_name:\n",
        "        selected_datasets = {\n",
        "            name: df for name, df in datasets_dict.items()\n",
        "            if (f\"_{exclusion_type}\" in name.lower() and\n",
        "                f\"_more_{exclusion_type}\" not in name.lower() and\n",
        "                f\"_less_{exclusion_type}\" not in name.lower() and\n",
        "                f\"_{feature_set_name.lower()}_\" in name.lower() and\n",
        "                target_column_lower in name.lower())\n",
        "        }\n",
        "    else:\n",
        "        # Adjust for FinalGrade specifically\n",
        "        base_key = f'filtered_combined_df_{target_column}' if target_column == 'FinalGrade' else f'combined_df_{target_column}'\n",
        "        selected_datasets = {\n",
        "            base_key: datasets_dict.get(base_key)\n",
        "        }\n",
        "\n",
        "    if not selected_datasets or all(df is None for df in selected_datasets.values()):\n",
        "        print(f\"No datasets found for the given criteria: {exclusion_type}, {feature_set_name}, {target_column}\")\n",
        "        return {}, None\n",
        "\n",
        "    results = {}\n",
        "    best_regression_model_info = None\n",
        "    best_classification_model_info = None\n",
        "    best_regression_score = float('inf')\n",
        "    best_classification_score = float('-inf')\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for dataset_name, df in selected_datasets.items():\n",
        "        if df is None:\n",
        "            continue\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Processing dataset: {dataset_name}\")\n",
        "\n",
        "        # Determine the task type (classification, regression, or both)\n",
        "        task_type = determine_task_type(target_column)\n",
        "\n",
        "        # Handle missing data, scaling, and preprocessing\n",
        "        X_train, X_test, y_train, y_test = preprocess_data_with_custom_imputation(\n",
        "            df,\n",
        "            target_column,\n",
        "            imputation_strategy=imputation_strategy,\n",
        "            apply_pca=apply_pca,\n",
        "            n_components=None,\n",
        "            apply_scaling=apply_scaling,\n",
        "            columns_to_scale=columns_to_scale,\n",
        "            group_col=group_col\n",
        "        )\n",
        "\n",
        "        # Check and handle missing values in the target variable\n",
        "        if y_train.isna().sum() > 0 or y_test.isna().sum() > 0:\n",
        "            print(f\"Warning: NaN values found in the target variable {target_column}.\")\n",
        "            print(\"Dropping rows with NaN values in the target variable.\")\n",
        "            valid_indices_train = ~y_train.isna()\n",
        "            valid_indices_test = ~y_test.isna()\n",
        "            X_train, y_train = X_train[valid_indices_train], y_train[valid_indices_train]\n",
        "            X_test, y_test = X_test[valid_indices_test], y_test[valid_indices_test]\n",
        "\n",
        "        # Perform regression analysis if applicable\n",
        "        if task_type in ['regression', 'both']:\n",
        "            print(\"Performing regression analysis...\")\n",
        "            regression_results, regression_test_scores = train_and_evaluate_models(\n",
        "                X_train,\n",
        "                X_test,\n",
        "                y_train,\n",
        "                y_test,\n",
        "                task_type='regression',\n",
        "                scoring='neg_mean_squared_error',\n",
        "                cv=cv\n",
        "            )\n",
        "            results.update({f\"{dataset_name}_regression\": regression_results})\n",
        "\n",
        "            # Identify the best model based on MSE (for regression)\n",
        "            for model_name, test_scores in regression_test_scores.items():\n",
        "                if test_scores['MSE'] < best_regression_score:\n",
        "                    best_regression_score = test_scores['MSE']\n",
        "                    best_regression_model_info = {\n",
        "                        'model_name': model_name,\n",
        "                        'params': regression_results[model_name],\n",
        "                        'dataset': dataset_name,\n",
        "                        'test_scores': test_scores\n",
        "                    }\n",
        "\n",
        "        # Perform classification analysis if applicable\n",
        "        if task_type in ['classification', 'both']:\n",
        "            if task_type == 'both':\n",
        "                # Convert to multiclass if the task type is 'both'\n",
        "                y_train_class = convert_to_multiclass(y_train)\n",
        "                y_test_class = convert_to_multiclass(y_test)\n",
        "            else:\n",
        "                y_train_class, y_test_class = y_train, y_test\n",
        "\n",
        "            print(\"Performing classification analysis...\")\n",
        "            classification_results, classification_test_scores = train_and_evaluate_models(\n",
        "                X_train,\n",
        "                X_test,\n",
        "                y_train_class,\n",
        "                y_test_class,\n",
        "                task_type='classification',\n",
        "                scoring='f1_weighted'\n",
        "            )\n",
        "            results.update({f\"{dataset_name}_classification\": classification_results})\n",
        "\n",
        "            # Identify the best model based on F1-Score (for classification)\n",
        "            for model_name, test_scores in classification_test_scores.items():\n",
        "                if test_scores['F1-Score'] > best_classification_score:\n",
        "                    best_classification_score = test_scores['F1-Score']\n",
        "                    best_classification_model_info = {\n",
        "                        'model_name': model_name,\n",
        "                        'params': classification_results[model_name],\n",
        "                        'dataset': dataset_name,\n",
        "                        'test_scores': test_scores\n",
        "                    }\n",
        "\n",
        "    # Save the best regression model\n",
        "    if best_regression_model_info:\n",
        "        os.makedirs(\"best_models\", exist_ok=True)\n",
        "        regression_model_save_path = os.path.join(\"best_models\", f\"{best_regression_model_info['dataset']}_regression_{best_regression_model_info['model_name']}.pkl\")\n",
        "        with open(regression_model_save_path, 'wb') as f:\n",
        "            pickle.dump(best_regression_model_info['params'], f)\n",
        "        print(f\"Best regression model saved to {regression_model_save_path}\")\n",
        "\n",
        "    # Save the best classification model\n",
        "    if best_classification_model_info:\n",
        "        os.makedirs(\"best_models\", exist_ok=True)\n",
        "        classification_model_save_path = os.path.join(\"best_models\", f\"{best_classification_model_info['dataset']}_classification_{best_classification_model_info['model_name']}.pkl\")\n",
        "        with open(classification_model_save_path, 'wb') as f:\n",
        "            pickle.dump(best_classification_model_info['params'], f)\n",
        "        print(f\"Best classification model saved to {classification_model_save_path}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Processing complete for datasets. Total time: {end_time - start_time:.2f} seconds\")\n",
        "    print(f\"Best regression model: {best_regression_model_info}\")\n",
        "    print(f\"Best classification model: {best_classification_model_info}\")\n",
        "\n",
        "    return results, (best_regression_model_info, best_classification_model_info)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZXUlzya8IYA"
      },
      "source": [
        "###### analyze_final_grade_with_advanced_ensembles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSQYjMBs8Fyj"
      },
      "outputs": [],
      "source": [
        "def analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict, target_column='FinalGrade', exclusion_type='more_restricted',\n",
        "    feature_set_name=None, imputation_strategy=None, apply_pca=False, apply_scaling=True,\n",
        "    columns_to_scale=None, group_col=None, cv=5\n",
        "):\n",
        "    print(f\"Processing datasets for target column: {target_column} with exclusion type: {exclusion_type}\")\n",
        "\n",
        "    # Step 1: Filter datasets based on exclusion type, feature set, and target column\n",
        "    if feature_set_name:\n",
        "        selected_datasets = {\n",
        "            name: df for name, df in datasets_dict.items()\n",
        "            if (f\"_{exclusion_type}\" in name and\n",
        "                f\"_more_{exclusion_type}\" not in name and\n",
        "                f\"_less_{exclusion_type}\" not in name and\n",
        "                f\"_{feature_set_name}_\" in name and\n",
        "                f\"{target_column.lower()}\" in name.lower())\n",
        "        }\n",
        "    else:\n",
        "        base_key = f'filtered_combined_df_{target_column}' if target_column == 'FinalGrade' else f'combined_df_{target_column}'\n",
        "        selected_datasets = {\n",
        "            base_key: datasets_dict.get(base_key)\n",
        "        }\n",
        "\n",
        "    if not selected_datasets:\n",
        "        print(f\"No datasets found for the given criteria: {exclusion_type}, {feature_set_name}, {target_column}\")\n",
        "        return {}, None\n",
        "\n",
        "    results = {}\n",
        "    best_model_info = None\n",
        "    best_score = float('inf')\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for dataset_name, df in selected_datasets.items():\n",
        "        if df is None:\n",
        "            continue\n",
        "\n",
        "        # Preprocess the data (imputation, scaling, PCA)\n",
        "        X_train, X_test, y_train, y_test = preprocess_data_with_custom_imputation(\n",
        "            df, target_column, imputation_strategy=imputation_strategy,\n",
        "            apply_pca=apply_pca, n_components=None, apply_scaling=apply_scaling,\n",
        "            columns_to_scale=columns_to_scale, group_col=group_col\n",
        "        )\n",
        "\n",
        "        # Check and handle missing values in the target variable\n",
        "        if y_train.isna().sum() > 0 or y_test.isna().sum() > 0:\n",
        "            print(f\"Warning: NaN values found in the target variable {target_column}.\")\n",
        "            print(\"Dropping rows with NaN values in the target variable.\")\n",
        "            valid_indices_train = ~y_train.isna()\n",
        "            valid_indices_test = ~y_test.isna()\n",
        "            X_train, y_train = X_train[valid_indices_train], y_train[valid_indices_train]\n",
        "            X_test, y_test = X_test[valid_indices_test], y_test[valid_indices_test]\n",
        "\n",
        "        # Check for NaNs and infinities in features after preprocessing\n",
        "        if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):\n",
        "            raise ValueError(\"Training features contain NaN or infinity values.\")\n",
        "        if np.any(np.isnan(X_test)) or np.any(np.isinf(X_test)):\n",
        "            raise ValueError(\"Test features contain NaN or infinity values.\")\n",
        "\n",
        "        # Define advanced ensemble models for regression analysis\n",
        "        ensemble_models = {\n",
        "            'XGBoost': xgb.XGBRegressor(random_state=2024, verbosity=0),  # Suppress XGBoost logs\n",
        "            'LightGBM': lgb.LGBMRegressor(random_state=2024, verbosity=-1),  # Suppress LightGBM logs\n",
        "            'CatBoost': cb.CatBoostRegressor(random_state=2024, silent=True),  # Ensure CatBoost is silent\n",
        "            'Stacking': StackingRegressor(\n",
        "                estimators=[\n",
        "                    ('xgb', xgb.XGBRegressor(random_state=2024, verbosity=0)),\n",
        "                    ('lgbm', lgb.LGBMRegressor(random_state=2024, verbosity=-1)),\n",
        "                    ('mlp', MLPRegressor(random_state=2024, max_iter=1000))\n",
        "                ],\n",
        "                final_estimator=LinearRegression()\n",
        "            )\n",
        "        }\n",
        "\n",
        "        param_grids = {\n",
        "            'XGBoost': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 6, 10]},\n",
        "            'LightGBM': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1], 'num_leaves': [31, 50]},\n",
        "            'CatBoost': {'iterations': [200, 500], 'learning_rate': [0.01, 0.1], 'depth': [4, 6, 10]},\n",
        "            'Stacking': {}\n",
        "        }\n",
        "\n",
        "        for model_name, model in ensemble_models.items():\n",
        "            try:\n",
        "                # Perform grid search if the model has a hyperparameter grid\n",
        "                if model_name in param_grids and param_grids[model_name]:\n",
        "                    grid_search = GridSearchCV(model, param_grids[model_name], cv=cv, scoring='neg_mean_squared_error')\n",
        "                    grid_search.fit(X_train, y_train)\n",
        "                    model = grid_search.best_estimator_\n",
        "\n",
        "                # Evaluate the model\n",
        "                scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
        "                results[f\"{dataset_name}_{model_name}\"] = scores\n",
        "\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                test_scores = {\n",
        "                    'MSE': mean_squared_error(y_test, y_pred),\n",
        "                    'R2': r2_score(y_test, y_pred),\n",
        "                    'MAE': mean_absolute_error(y_test, y_pred)\n",
        "                }\n",
        "\n",
        "                if test_scores['MSE'] < best_score:\n",
        "                    best_score = test_scores['MSE']\n",
        "                    best_model_info = {\n",
        "                        'model_name': f\"{dataset_name}_{model_name}\",\n",
        "                        'params': model,\n",
        "                        'dataset': dataset_name,\n",
        "                        'test_scores': test_scores\n",
        "                    }\n",
        "\n",
        "                # Save the best model\n",
        "                if best_model_info:\n",
        "                    os.makedirs(\"best_models\", exist_ok=True)\n",
        "                    model_save_path = os.path.join(\"best_models\", f\"{best_model_info['dataset']}_{best_model_info['model_name']}.pkl\")\n",
        "                    with open(model_save_path, 'wb') as f:\n",
        "                        pickle.dump(best_model_info['params'], f)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error with model {model_name} on dataset {dataset_name}: {e}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Processing complete for datasets. Total time: {end_time - start_time:.2f} seconds\")\n",
        "    print(f\"Best model: {best_model_info}\")\n",
        "\n",
        "    return results, best_model_info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j5lq4dT88vz"
      },
      "source": [
        "###### analyze_final_grade_with_imputation_strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE6KZK2j853k"
      },
      "outputs": [],
      "source": [
        "def analyze_final_grade_with_imputation_strategies(\n",
        "    datasets_dict, target_column='FinalGrade', exclusion_type='more_restricted',\n",
        "    feature_set_name=None, imputation_strategies=['mean', 'median', 'zero', 'knn', 'most_frequent', 'constant'],\n",
        "    apply_pca=False, apply_scaling=True, columns_to_scale=None, group_col=None, cv=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Analyze final grade prediction using different imputation strategies with advanced ensemble techniques.\n",
        "\n",
        "    Parameters:\n",
        "    - datasets_dict: Dictionary containing the datasets.\n",
        "    - target_column: The target column to predict (default is 'FinalGrade').\n",
        "    - exclusion_type: The exclusion type to filter datasets (e.g., 'more_restricted').\n",
        "    - feature_set_name: The feature selection method to use (e.g., 'Lasso'). If None, use the original dataset without feature selection.\n",
        "    - imputation_strategies: List of strategies for imputing missing data.\n",
        "    - apply_pca: Boolean to apply PCA for dimensionality reduction.\n",
        "    - apply_scaling: Boolean to apply scaling to the features.\n",
        "    - columns_to_scale: Specific columns to scale (optional).\n",
        "    - group_col: Column to group by (optional).\n",
        "    - cv: Number of cross-validation folds (default is 5).\n",
        "\n",
        "    Returns:\n",
        "    - best_model_info_overall: Information about the best model found across all imputation strategies.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Processing datasets for target column: {target_column} with exclusion type: {exclusion_type}\")\n",
        "\n",
        "    # Step 1: Filter datasets based on exclusion type, feature set, and target column\n",
        "    if feature_set_name:\n",
        "        selected_datasets = {\n",
        "            name: df for name, df in datasets_dict.items()\n",
        "            if (f\"_{exclusion_type}\" in name and\n",
        "                f\"_{feature_set_name}_\" in name and\n",
        "                f\"{target_column.lower()}\" in name.lower())\n",
        "        }\n",
        "    else:\n",
        "        # Use the original dataset without feature selection\n",
        "        base_key = f'combined_df_{target_column}'\n",
        "        selected_datasets = {base_key: datasets_dict.get(base_key)}\n",
        "\n",
        "    if not selected_datasets:\n",
        "        print(f\"No datasets found for the given criteria: {exclusion_type}, {feature_set_name}, {target_column}\")\n",
        "        return None\n",
        "\n",
        "    # Initialize variables to track the best model across all imputation strategies\n",
        "    best_model_info_overall = None\n",
        "    best_score_overall = float('inf')\n",
        "\n",
        "    # Step 2: Iterate over each imputation strategy\n",
        "    for imputation_strategy in imputation_strategies:\n",
        "        print(f\"Testing imputation strategy: {imputation_strategy}\")\n",
        "\n",
        "        # Call the updated method for the current imputation strategy\n",
        "        _, best_model_info = analyze_final_grade_with_advanced_ensembles(\n",
        "            datasets_dict=datasets_dict,\n",
        "            target_column=target_column,\n",
        "            exclusion_type=exclusion_type,\n",
        "            feature_set_name=feature_set_name,\n",
        "            imputation_strategy=imputation_strategy,\n",
        "            apply_pca=apply_pca,\n",
        "            apply_scaling=apply_scaling,\n",
        "            columns_to_scale=columns_to_scale,\n",
        "            group_col=group_col,\n",
        "            cv=cv\n",
        "        )\n",
        "\n",
        "        # Step 3: Update the best model across all strategies if the current one is better\n",
        "        if best_model_info and best_model_info['test_scores']['MSE'] < best_score_overall:\n",
        "            best_score_overall = best_model_info['test_scores']['MSE']\n",
        "            best_model_info_overall = best_model_info\n",
        "            best_model_info_overall['imputation_strategy'] = imputation_strategy\n",
        "\n",
        "    # Step 4: Save the best model found across all imputation strategies\n",
        "    if best_model_info_overall:\n",
        "        os.makedirs(\"best_models\", exist_ok=True)\n",
        "        model_save_path = os.path.join(\n",
        "            \"best_models\", f\"{best_model_info_overall['dataset']}_{best_model_info_overall['model_name']}_overall.pkl\"\n",
        "        )\n",
        "        with open(model_save_path, 'wb') as f:\n",
        "            pickle.dump(best_model_info_overall['params'], f)\n",
        "        print(f\"Best overall model saved to {model_save_path}\")\n",
        "\n",
        "    print(f\"Best overall model: {best_model_info_overall}\")\n",
        "\n",
        "    return best_model_info_overall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyLT0B_cDLDD"
      },
      "source": [
        "## 4.2 Applying Predictive Modeling Methods\n",
        "\n",
        "### Method Differences\n",
        "\n",
        "#### 1. **analyze_datasets_for_target_column**\n",
        "- **Purpose**: A general method designed for predicting any target column, applicable for both classification and regression tasks depending on the target column type.\n",
        "- **Unique Aspects**: Versatile and adaptable to different types of target columns (e.g., `Total_AP`, `Total_PP`, `FinalGrade`). It supports different feature selection methods, imputation strategies, and can handle both regression and classification tasks.\n",
        "\n",
        "#### 2. **analyze_final_grade_with_advanced_ensembles**\n",
        "- **Purpose**: Focused on predicting the `FinalGrade` target column using advanced ensemble techniques, specifically optimized for regression tasks.\n",
        "- **Unique Aspects**: Utilizes advanced models like XGBoost, LightGBM, and Stacking to improve prediction accuracy. The method is specifically tailored for enhancing the predictive performance for the `FinalGrade` column.\n",
        "\n",
        "#### 3. **analyze_final_grade_with_imputation_strategies**\n",
        "- **Purpose**: Extends the analysis of `FinalGrade` by testing various imputation strategies to determine the most effective approach for missing data, in conjunction with advanced ensemble models.\n",
        "- **Unique Aspects**: Iteratively applies different imputation strategies (e.g., `mean`, `median`, `zero`, `knn`, `most_frequent`, `constant`) and evaluates ensemble models, selecting the best combination overall for predicting `FinalGrade`.\n",
        "\n",
        "### When to Use Each Method\n",
        "\n",
        "- **General Prediction**: Use `analyze_datasets_for_target_column` when you need a flexible tool to predict any column, with support for both regression and classification. This method is versatile and can be adapted to various target columns and tasks.\n",
        "- **Optimized Predictions for FinalGrade**: Utilize `analyze_final_grade_with_advanced_ensembles` if you want to leverage advanced ensemble techniques specifically for predicting `FinalGrade` and achieve better predictive performance in regression tasks.\n",
        "- **Best Imputation Strategy for FinalGrade**: Apply `analyze_final_grade_with_imputation_strategies` when you need to determine the most effective imputation strategy in addition to optimizing model performance for predicting `FinalGrade`.\n",
        "\n",
        "### Common Parameters\n",
        "\n",
        "These methods share a set of common parameters that allow for flexible and customizable analysis of datasets. Below are the shared parameters that you will find in each method:\n",
        "\n",
        "#### **Parameters:**\n",
        "- `datasets_dict`: A dictionary containing the datasets to be analyzed.\n",
        "- `target_column`: The column you want to predict (e.g., 'Total_AP', 'Passed', 'FinalGrade').\n",
        "- `exclusion_type`: Specifies the dataset filtering criteria (e.g., 'more_restricted', 'restricted', 'less_restricted').\n",
        "- `feature_set_name`: The name of the feature selection method to be used (e.g., 'MI' for Mutual Information). If `None`, no feature selection is applied.\n",
        "- `imputation_strategy`: Specifies how to handle missing data (e.g., 'mean', 'median', 'zero', 'knn', 'most_frequent', 'constant').\n",
        "- `apply_pca`: A boolean indicating whether to apply PCA for dimensionality reduction.\n",
        "- `apply_scaling`: A boolean indicating whether to scale the features.\n",
        "- `columns_to_scale`: Specific columns to scale. If `None`, all columns except the target column are scaled.\n",
        "- `group_col`: The column by which to group data before applying scaling (optional).\n",
        "- `cv`: Number of cross-validation folds (default is 5).\n",
        "\n",
        "### Example of Usage\n",
        "```python\n",
        "# Example for analyzing a target column with a specific feature set and exclusion type\n",
        "results, best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=my_datasets_dict,   # Dictionary of datasets to analyze\n",
        "    target_column='Total_AP',         # The target column to predict\n",
        "    feature_set_name='MI',            # Feature selection method to apply\n",
        "    exclusion_type='more_restricted', # The exclusion type for filtering datasets\n",
        "    imputation_strategy='mean',       # Strategy for imputing missing values\n",
        "    apply_pca=True,                   # Whether to apply PCA\n",
        "    apply_scaling=True,               # Whether to apply Min-Max scaling\n",
        "    group_col='Year',                 # Column to group by before scaling (if applicable)\n",
        "    cv=5                              # Number of cross-validation folds\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(without_feature_selection_datasets.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s93pYs18A0TD",
        "outputId": "115d1540-b701-4e54-db06-2387e24d6a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['combined_df_Total_SPA', 'combined_df_Total_PP', 'filtered_combined_df_FinalGrade', 'combined_df_Passed', 'combined_df_Total_AP'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2Ickk4nzij_"
      },
      "source": [
        "### 4.2.1 Predicting Total_AP feature"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Several Imputations and Test - CPU"
      ],
      "metadata": {
        "id": "hnADC1rqrAl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for analyzing 'Total_AP' with 'MI' feature set and 'more_restricted' exclusion type\n",
        "mi_more_restricted_total_ap_results_median, mi_more_restricted_total_ap_best_model_median = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_ap_datasets,  # This dictionary was created using your structure\n",
        "    target_column='Total_AP',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='median',\n",
        "    feature_set_name='MI',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4xA9OHHnar-",
        "outputId": "780c0f56-da60-4727-b277-b42ed37a1a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_MI_more_restricted\n",
            "Column 'Year' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-182.0651522  -202.92519892 -194.73290528 -174.8535481  -179.66125447]\n",
            "Test set scores for Linear Regression: {'MSE': 157.79729581078337, 'R2': 0.793325348103151, 'MAE': 8.013893906197902}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-271.59360865 -287.7456871  -329.10705485 -260.03785542 -308.27860947]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 230.98798894926023, 'R2': 0.6974640030226729, 'MAE': 7.77646991051817}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-149.22889691 -162.21479643 -165.92455361 -141.61944823 -160.97752081]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 105.21486908131573, 'R2': 0.8621950627859399, 'MAE': 5.93837142553542}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-207.47940398 -225.93419794 -214.90401772 -198.99046809 -205.77853739]\n",
            "Test set scores for Support Vector Machine: {'MSE': 172.52799529288052, 'R2': 0.7740318477169963, 'MAE': 7.485262677591578}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-186.21902823 -191.7855123  -182.96917753 -182.2953362  -195.26952683]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 128.19630894182768, 'R2': 0.8320951738185455, 'MAE': 6.2954010501539015}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-151.38789448 -157.94434122 -162.91723782 -174.15278987 -154.55778713]\n",
            "Test set scores for Neural Network: {'MSE': 113.82764871488968, 'R2': 0.8509144941077058, 'MAE': 6.456089816553299}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.78133424 0.77545333 0.79161687 0.78540464 0.79173383]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8212927756653993, 'F1-Score': 0.8120024052257131}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.76936919 0.75593622 0.80271191 0.79286443 0.77114552]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8111533586818758, 'F1-Score': 0.8047298251773822}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.80457111 0.78897071 0.8281761  0.81038986 0.81046364]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8422053231939164, 'F1-Score': 0.8356737981873823}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.79345188 0.78421072 0.81510268 0.79959271 0.80405545]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8371356147021546, 'F1-Score': 0.8284196112493091}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.7697621  0.77664601 0.79631714 0.79086484 0.78980449]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8231939163498099, 'F1-Score': 0.818156875862864}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.8166664  0.80666359 0.82234024 0.82353884 0.81186299]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8314321926489227, 'F1-Score': 0.83379205040097}\n",
            "Best regression model saved to best_models/predict_total_ap_MI_more_restricted_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/predict_total_ap_MI_more_restricted_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 1099.27 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-149.22889691, -162.21479643, -165.92455361, -141.61944823,\n",
            "       -160.97752081]), 'dataset': 'predict_total_ap_MI_more_restricted', 'test_scores': {'MSE': 105.21486908131573, 'R2': 0.8621950627859399, 'MAE': 5.93837142553542}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.80457111, 0.78897071, 0.8281761 , 0.81038986, 0.81046364]), 'dataset': 'predict_total_ap_MI_more_restricted', 'test_scores': {'Accuracy': 0.8422053231939164, 'F1-Score': 0.8356737981873823}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_more_restricted_total_ap_results_median, rf_more_restricted_total_ap_best_model_median = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_ap_datasets,  # This dictionary was created using your structure\n",
        "    target_column='Total_AP',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='median',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL2X1r8Enqcz",
        "outputId": "cf912aef-c2a5-4edd-ad0a-28d5ad2daee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "No datasets found for the given criteria: more_restricted, RF, Total_AP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_more_restricted_total_ap_results_median, lasso_more_restricted_total_ap_best_model_median = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_ap_datasets,  # This dictionary was created using your structure\n",
        "    target_column='Total_AP',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='median',\n",
        "    feature_set_name='Lasso',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4MF_h2gnv7o",
        "outputId": "104a1052-ea76-418c-b98e-29b511863c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "No datasets found for the given criteria: more_restricted, Lasso, Total_AP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fr_more_restricted_total_ap_results_median, fr_more_restricted_total_ap_best_model_median = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_ap_datasets,  # This dictionary was created using your structure\n",
        "    target_column='Total_AP',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='median',\n",
        "    feature_set_name='FR',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TGvnrCen1Fy",
        "outputId": "c2b41a31-ac69-44e5-dc80-1c687cf3a4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "No datasets found for the given criteria: more_restricted, FR, Total_AP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A testar antes..."
      ],
      "metadata": {
        "id": "yTFyq87fn7hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results, original_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01493463-143b-40e9-f0c5-9f5678b0945e",
        "id": "3b22JVoerAmD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_AP\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-200.79392161 -219.86076691 -220.56014285 -192.3899982  -212.80426943]\n",
            "Test set scores for Linear Regression: {'MSE': 190.43678728142854, 'R2': 0.7505758478463489, 'MAE': 8.76275384891383}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-354.36191135 -302.01173979 -343.09948464 -327.15033932 -322.63158691]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 286.7096894179701, 'R2': 0.6244826316480938, 'MAE': 8.480553324595014}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-212.86366694 -192.833936   -221.04861179 -182.50848612 -215.69472974]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 175.1178489818794, 'R2': 0.770639792695474, 'MAE': 7.380838738554794}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-719.98338227 -803.33774807 -805.37767011 -767.65203367 -719.50384844]\n",
            "Test set scores for Support Vector Machine: {'MSE': 775.2412380015862, 'R2': -0.01537046105140516, 'MAE': 16.30049026458048}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-213.07321408 -210.27727956 -221.97475662 -207.87599858 -221.45334584]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 186.9037660691653, 'R2': 0.755203214401884, 'MAE': 7.2975737823646565}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-277.29973969 -268.80778745 -278.0838138  -254.79620958 -253.2021109 ]\n",
            "Test set scores for Neural Network: {'MSE': 235.828761671811, 'R2': 0.6911238118793175, 'MAE': 10.462820829738842}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.59706518 0.6004644  0.61051634 0.59419147 0.60671514]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7287705956907478, 'F1-Score': 0.6539691850107289}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.76509855 0.77358082 0.79542125 0.78411377 0.77967773]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.811787072243346, 'F1-Score': 0.8047204014879236}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.78940792 0.80663052 0.80912303 0.79466036 0.8034326 ]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8231939163498099, 'F1-Score': 0.8142354277268302}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.1}\n",
            "Cross-validation scores for Support Vector Machine: [0.51116967 0.51116967 0.51182043 0.5108182  0.5108182 ]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6615969581749049, 'F1-Score': 0.5268552435809312}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.78006625 0.78320078 0.78924195 0.77860211 0.78225018]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8105196451204055, 'F1-Score': 0.8034203231365205}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.57473587 0.7090834  0.58390396 0.50086996 0.32922835]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6628643852978454, 'F1-Score': 0.6756029035089596}\n",
            "Best regression model saved to best_models/combined_df_Total_AP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_AP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 414.97 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-212.86366694, -192.833936  , -221.04861179, -182.50848612,\n",
            "       -215.69472974]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'MSE': 175.1178489818794, 'R2': 0.770639792695474, 'MAE': 7.380838738554794}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.78940792, 0.80663052, 0.80912303, 0.79466036, 0.8034326 ]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8231939163498099, 'F1-Score': 0.8142354277268302}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results_mean, original_total_ap_best_model_mean = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    imputation_strategy='mean',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a552f368-6e0d-4764-92a1-3d7d2429c47d",
        "id": "z6DOKAdFrAmD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_AP\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-167.19714914 -184.97225746 -179.2603239  -152.36950061 -175.22779997]\n",
            "Test set scores for Linear Regression: {'MSE': 2.3857261581400292e+23, 'R2': -3.124699448880559e+20, 'MAE': 12295800185.869598}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-318.65696456 -302.13278375 -366.06097411 -287.02215034 -339.94054338]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 324.70895040822154, 'R2': 0.5747131853648384, 'MAE': 9.057737089343552}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-176.8344524  -163.70263964 -177.84505413 -154.13326366 -165.13546512]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 139.75092673011127, 'R2': 0.8169615392595719, 'MAE': 6.684721704676833}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-176.61067983 -195.11658887 -181.42944984 -165.79604078 -185.00828798]\n",
            "Test set scores for Support Vector Machine: {'MSE': 153.88595608811417, 'R2': 0.798448216473483, 'MAE': 6.624091934725391}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-203.56270299 -191.050673   -203.92457712 -180.0698923  -212.07768686]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 166.2910426528026, 'R2': 0.782200682354891, 'MAE': 6.81079123664675}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-181.12434234 -165.36316138 -174.35650431 -147.05950367 -177.20155929]\n",
            "Test set scores for Neural Network: {'MSE': 145.12685637236132, 'R2': 0.809920427548974, 'MAE': 7.029600675765378}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.80883263 0.82282357 0.82941835 0.78743137 0.80741925]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.829531051964512, 'F1-Score': 0.8254066313151681}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.76268347 0.78658622 0.77742263 0.76316812 0.7930617 ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8092522179974652, 'F1-Score': 0.8040037342455629}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.81976851 0.81274973 0.8378243  0.81284507 0.81013883]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8466413181242078, 'F1-Score': 0.8418112738306026}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.81112939 0.81498902 0.83593598 0.80961812 0.81493557]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8358681875792142, 'F1-Score': 0.8302992104823192}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.79259164 0.80662146 0.80879711 0.78789829 0.78711536]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.811787072243346, 'F1-Score': 0.8047377933881568}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.8139995  0.81051612 0.80513624 0.81538316 0.81321019]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8244613434727504, 'F1-Score': 0.8137082083952846}\n",
            "Best regression model saved to best_models/combined_df_Total_AP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_AP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 1283.34 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-176.8344524 , -163.70263964, -177.84505413, -154.13326366,\n",
            "       -165.13546512]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'MSE': 139.75092673011127, 'R2': 0.8169615392595719, 'MAE': 6.684721704676833}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.81976851, 0.81274973, 0.8378243 , 0.81284507, 0.81013883]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8466413181242078, 'F1-Score': 0.8418112738306026}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results_median, original_total_ap_best_model_median = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    imputation_strategy='median',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4e9030-b7c5-417a-c11d-73ec636122b4",
        "id": "N_mfQGiRrAmE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_AP\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-167.06263551 -181.84860998 -177.01012095 -150.67951069 -174.07183842]\n",
            "Test set scores for Linear Regression: {'MSE': 3.342304840190633e+23, 'R2': -4.377576217832418e+20, 'MAE': 14553569685.334995}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-321.21245281 -243.6119102  -320.89251664 -308.56417138 -361.38701006]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 305.85814574906976, 'R2': 0.5994029842038339, 'MAE': 8.82145648500211}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-179.75377033 -167.20026191 -185.4526863  -153.25860877 -164.75818497]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 135.49196247127253, 'R2': 0.8225397080812487, 'MAE': 6.586813693443572}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-179.20713805 -199.64396196 -183.86094822 -168.27964167 -188.15610381]\n",
            "Test set scores for Support Vector Machine: {'MSE': 156.86052822136028, 'R2': 0.7945522773382656, 'MAE': 6.751617474999728}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-203.88874885 -191.80149304 -204.78408099 -180.45407355 -207.14625311]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 166.7824810532578, 'R2': 0.781557022019516, 'MAE': 6.801647655259822}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-176.60769624 -158.80340092 -177.79058862 -151.58897515 -170.18137742]\n",
            "Test set scores for Neural Network: {'MSE': 145.23100462614468, 'R2': 0.8097840196087381, 'MAE': 6.950538205421891}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.81153864 0.81539058 0.82873517 0.80509947 0.80679505]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8314321926489227, 'F1-Score': 0.8280703509416728}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.76627517 0.7806411  0.791997   0.76134113 0.77534022]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8193916349809885, 'F1-Score': 0.8155329802399698}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.82214946 0.81642669 0.83269124 0.81148738 0.81399984]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8428390367553865, 'F1-Score': 0.8367126899489952}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.80531044 0.81447106 0.83678119 0.8009109  0.8124732 ]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8358681875792142, 'F1-Score': 0.8300549240863535}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.79114815 0.79524085 0.79519034 0.79027007 0.78057975]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.811787072243346, 'F1-Score': 0.8052145792952392}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.80577545 0.81667581 0.81865496 0.81998262 0.81677738]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8282636248415716, 'F1-Score': 0.8254933144275836}\n",
            "Best regression model saved to best_models/combined_df_Total_AP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_AP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 1345.96 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-179.75377033, -167.20026191, -185.4526863 , -153.25860877,\n",
            "       -164.75818497]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'MSE': 135.49196247127253, 'R2': 0.8225397080812487, 'MAE': 6.586813693443572}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.82214946, 0.81642669, 0.83269124, 0.81148738, 0.81399984]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8428390367553865, 'F1-Score': 0.8367126899489952}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results_knn, original_total_ap_best_model_knn = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    imputation_strategy='knn',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0810a1d7-aafe-4da9-e2d7-b985c91aed58",
        "id": "KIACjEumrAmE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_AP\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-178.66424449 -193.95289288 -188.79909246 -158.09985219 -184.54164346]\n",
            "Test set scores for Linear Regression: {'MSE': 2.5432288229346357e+23, 'R2': -3.33098820846922e+20, 'MAE': 12695190415.500898}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-306.02539549 -337.55465332 -404.58273811 -329.52585121 -321.79215586]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 246.08429196520183, 'R2': 0.6776916542335635, 'MAE': 8.287206027320096}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-170.16709114 -181.07153827 -178.24348575 -150.91977336 -178.40032283]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 140.71556250486648, 'R2': 0.8156981097316424, 'MAE': 6.605829596554561}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-185.32246204 -203.45245052 -195.54499687 -177.87609667 -196.92600651]\n",
            "Test set scores for Support Vector Machine: {'MSE': 166.4951361299198, 'R2': 0.7819333713840604, 'MAE': 6.568225232222153}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-193.99623507 -205.04445199 -197.41212523 -185.09303341 -221.49621592]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 160.99737461524535, 'R2': 0.7891340521144813, 'MAE': 6.748958899149012}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-188.49539945 -166.30038876 -188.69814501 -159.80554882 -181.54853554]\n",
            "Test set scores for Neural Network: {'MSE': 165.25393131392852, 'R2': 0.7835590365892823, 'MAE': 7.341067129950189}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.81196733 0.81320562 0.82980483 0.79796592 0.80964562]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8288973384030418, 'F1-Score': 0.8259427526761459}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.76393214 0.77256238 0.78189875 0.77804043 0.77585217]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8010139416983524, 'F1-Score': 0.7971085295911304}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.80927217 0.80742489 0.8241518  0.80126866 0.80985641]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8460076045627376, 'F1-Score': 0.8415984578308399}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.8071223  0.81161598 0.83193937 0.80048149 0.8059764 ]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8339670468948035, 'F1-Score': 0.8289771239705671}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.78177459 0.78242295 0.79787217 0.79589481 0.76880962]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8079847908745247, 'F1-Score': 0.8004774079477981}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.80575379 0.79945993 0.80945465 0.80381987 0.79895463]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8301647655259823, 'F1-Score': 0.8264382034384677}\n",
            "Best regression model saved to best_models/combined_df_Total_AP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_AP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 1268.34 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-170.16709114, -181.07153827, -178.24348575, -150.91977336,\n",
            "       -178.40032283]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'MSE': 140.71556250486648, 'R2': 0.8156981097316424, 'MAE': 6.605829596554561}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.80927217, 0.80742489, 0.8241518 , 0.80126866, 0.80985641]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8460076045627376, 'F1-Score': 0.8415984578308399}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results_zero, original_total_ap_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf8ef80-130a-41ed-95f6-9a1e92b4368a",
        "id": "3YQsNTz9rAmE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_AP\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-163.07572659 -174.0228268  -170.5257389  -143.02765517 -167.08778843]\n",
            "Test set scores for Linear Regression: {'MSE': 3.8050004681922454e+23, 'R2': -4.9835907718846965e+20, 'MAE': 15528298039.934677}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-310.76045009 -327.45640215 -348.52434244 -249.91526533 -308.51394212]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 332.28800300993987, 'R2': 0.5647865383325197, 'MAE': 9.071120767698714}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-181.95008019 -167.48348668 -177.21218851 -143.61851809 -166.97867058]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 138.68761132301677, 'R2': 0.8183542142131486, 'MAE': 6.662455254217081}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-175.31890888 -191.37643855 -180.27531466 -163.91314774 -189.02412296]\n",
            "Test set scores for Support Vector Machine: {'MSE': 157.16467319483553, 'R2': 0.7941539241459868, 'MAE': 6.208131260429188}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-197.8856141  -198.97106016 -190.44183188 -169.59175588 -214.05166726]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 163.7770880215204, 'R2': 0.7854933287569418, 'MAE': 6.699076588810429}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-176.4230974  -166.49992854 -192.65426592 -153.2788315  -166.09849604]\n",
            "Test set scores for Neural Network: {'MSE': 159.4477623907951, 'R2': 0.7911636532265776, 'MAE': 7.019302198027224}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.83002164 0.82343274 0.84738256 0.81867565 0.82910419]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8415716096324461, 'F1-Score': 0.8389613490273139}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.77071306 0.79373477 0.76616439 0.7684822  0.79316126]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8041825095057035, 'F1-Score': 0.7999492557481415}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.8301654  0.81152085 0.83179115 0.81085788 0.81111767]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8390367553865653, 'F1-Score': 0.8338608612804891}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.82054649 0.82408781 0.84342144 0.81959585 0.82039957]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8346007604562737, 'F1-Score': 0.8297945545335846}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.7957378  0.79679196 0.81146907 0.7965713  0.78427702]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8225602027883396, 'F1-Score': 0.8165805931036759}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.82357193 0.80387803 0.83127934 0.80707719 0.80649088]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8269961977186312, 'F1-Score': 0.8253592322310446}\n",
            "Best regression model saved to best_models/combined_df_Total_AP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_AP_classification_Logistic Regression.pkl\n",
            "Processing complete for datasets. Total time: 1151.46 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-181.95008019, -167.48348668, -177.21218851, -143.61851809,\n",
            "       -166.97867058]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'MSE': 138.68761132301677, 'R2': 0.8183542142131486, 'MAE': 6.662455254217081}}\n",
            "Best classification model: {'model_name': 'Logistic Regression', 'params': array([0.83002164, 0.82343274, 0.84738256, 0.81867565, 0.82910419]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8415716096324461, 'F1-Score': 0.8389613490273139}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results, original_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0Sb_pKNrAmE",
        "outputId": "43f7548b-3aa4-4b8f-8c9c-f3151c4a1987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_AP\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-200.79392161 -219.86076691 -220.56014285 -192.3899982  -212.80426943]\n",
            "Test set scores for Linear Regression: {'MSE': 190.43678728142854, 'R2': 0.7505758478463489, 'MAE': 8.76275384891383}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-354.36191135 -302.01173979 -343.09948464 -327.15033932 -322.63158691]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 286.7096894179701, 'R2': 0.6244826316480938, 'MAE': 8.480553324595014}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-212.86366694 -192.833936   -221.04861179 -182.50848612 -215.69472974]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 175.1178489818794, 'R2': 0.770639792695474, 'MAE': 7.380838738554794}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-719.98338227 -803.33774807 -805.37767011 -767.65203367 -719.50384844]\n",
            "Test set scores for Support Vector Machine: {'MSE': 775.2412380015862, 'R2': -0.01537046105140516, 'MAE': 16.30049026458048}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-213.07321408 -210.27727956 -221.97475662 -207.87599858 -221.45334584]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 186.9037660691653, 'R2': 0.755203214401884, 'MAE': 7.2975737823646565}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-277.29973969 -268.80778745 -278.0838138  -254.79620958 -253.2021109 ]\n",
            "Test set scores for Neural Network: {'MSE': 235.828761671811, 'R2': 0.6911238118793175, 'MAE': 10.462820829738842}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.59706518 0.6004644  0.61051634 0.59419147 0.60671514]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7287705956907478, 'F1-Score': 0.6539691850107289}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.76509855 0.77358082 0.79542125 0.78411377 0.77967773]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.811787072243346, 'F1-Score': 0.8047204014879236}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.78940792 0.80663052 0.80912303 0.79466036 0.8034326 ]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8231939163498099, 'F1-Score': 0.8142354277268302}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.1}\n",
            "Cross-validation scores for Support Vector Machine: [0.51116967 0.51116967 0.51182043 0.5108182  0.5108182 ]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6615969581749049, 'F1-Score': 0.5268552435809312}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.78006625 0.78320078 0.78924195 0.77860211 0.78225018]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8105196451204055, 'F1-Score': 0.8034203231365205}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.57473587 0.7090834  0.58390396 0.50086996 0.32922835]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6628643852978454, 'F1-Score': 0.6756029035089596}\n",
            "Best regression model saved to best_models/combined_df_Total_AP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_AP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 397.02 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-212.86366694, -192.833936  , -221.04861179, -182.50848612,\n",
            "       -215.69472974]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'MSE': 175.1178489818794, 'R2': 0.770639792695474, 'MAE': 7.380838738554794}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.78940792, 0.80663052, 0.80912303, 0.79466036, 0.8034326 ]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8231939163498099, 'F1-Score': 0.8142354277268302}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_more_restricted_total_ap_results, rf_more_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "194zD01-A8rq",
        "outputId": "17fa329e-8d58-44e7-8daf-0d98cc4ddb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_RF_more_restricted_regression\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-247.53847786 -280.12702593 -270.3252893  -248.64364402 -261.6316098 ]\n",
            "Test set scores for Linear Regression: {'MSE': 239.23405303163355, 'R2': 0.6866637392096149, 'MAE': 10.367065808842053}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-368.09742719 -380.52221886 -406.11780575 -368.00467648 -359.42184907]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 355.8814252776453, 'R2': 0.5338851067891017, 'MAE': 10.446852005820466}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-248.48920872 -259.35021075 -273.1279423  -226.7991639  -263.64932149]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 212.04031338025973, 'R2': 0.7222806783171174, 'MAE': 9.061500865165131}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-255.37131594 -269.54750184 -271.56121519 -252.77949427 -268.20220622]\n",
            "Test set scores for Support Vector Machine: {'MSE': 233.59914838875372, 'R2': 0.6940440428425466, 'MAE': 9.525785827889074}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-255.67658797 -269.5127248  -271.03636922 -260.69208254 -289.01279149]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 229.87352887923228, 'R2': 0.6989236645830469, 'MAE': 9.448216548977005}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-228.96796353 -227.95190343 -246.8459433  -217.13670685 -253.69686905]\n",
            "Test set scores for Neural Network: {'MSE': 207.2736066553645, 'R2': 0.7285238616872773, 'MAE': 9.52210984970238}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.71074256 0.73220196 0.74502767 0.70821068 0.72673898]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7604562737642585, 'F1-Score': 0.7375223857904403}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.72225434 0.73897691 0.73360178 0.72570345 0.7167622 ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.7718631178707225, 'F1-Score': 0.7485585484580918}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.76876292 0.744325   0.74754687 0.73768539 0.74258786]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.7807351077313055, 'F1-Score': 0.7683540633024609}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.70759251 0.73232196 0.73769074 0.72321785 0.72125445]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.7832699619771863, 'F1-Score': 0.7550049994458161}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.73429067 0.74119479 0.74205835 0.72862444 0.72870458]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7782002534854245, 'F1-Score': 0.7624569019904387}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.75837139 0.76155749 0.76265211 0.7367457  0.74121196]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.7839036755386565, 'F1-Score': 0.7605984376373711}\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_RF_more_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-245.25475019 -275.39960014 -270.79829502 -246.13460971 -257.7444634 ]\n",
            "Test set scores for Linear Regression: {'MSE': 238.22160089177666, 'R2': 0.6879897961137715, 'MAE': 10.257488238257768}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-289.51980115 -358.71452461 -293.83119115 -306.83965106 -313.52312419]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 275.56590799301387, 'R2': 0.6390781742078289, 'MAE': 8.843642301967263}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-196.22086974 -215.65527301 -208.42204321 -184.74007188 -203.92056146]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 165.42311622650394, 'R2': 0.7833374470320401, 'MAE': 7.765713148067143}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-232.61585387 -244.9654176  -249.36308877 -225.85221999 -241.40325032]\n",
            "Test set scores for Support Vector Machine: {'MSE': 209.34179930167085, 'R2': 0.7258150510385604, 'MAE': 8.521442770740618}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-226.62657165 -236.35113222 -234.45622821 -220.43882726 -260.6407607 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 202.60382762991125, 'R2': 0.7346400942219466, 'MAE': 8.173510773130547}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-179.74903228 -183.33546409 -188.16914096 -173.61392778 -198.40390776]\n",
            "Test set scores for Neural Network: {'MSE': 152.69275325974266, 'R2': 0.8000110111838038, 'MAE': 7.69205022624974}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.72711296 0.74279524 0.72887365 0.72209938 0.72874572]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7667934093789607, 'F1-Score': 0.7495521083108555}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.75526889 0.76294015 0.78201812 0.78426069 0.761529  ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8143219264892269, 'F1-Score': 0.8091868500508422}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.78086938 0.78278505 0.81029399 0.79600553 0.79134115]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8326996197718631, 'F1-Score': 0.8238086210469641}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.76998465 0.77315563 0.78933143 0.78513639 0.78008295]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8187579214195184, 'F1-Score': 0.8073068805052824}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.76618224 0.75020517 0.77674145 0.76793147 0.75762252]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7997465145754119, 'F1-Score': 0.7886888141779949}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.79463296 0.77881104 0.78843844 0.7956285  0.79414547]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8326996197718631, 'F1-Score': 0.833185216798152}\n",
            "Best regression model saved to best_models/predict_total_ap_RF_more_restricted_regression_Neural Network.pkl\n",
            "Best classification model saved to best_models/predict_total_ap_RF_more_restricted_classification_Neural Network.pkl\n",
            "Processing complete for datasets. Total time: 1568.13 seconds\n",
            "Best regression model: {'model_name': 'Neural Network', 'params': array([-179.74903228, -183.33546409, -188.16914096, -173.61392778,\n",
            "       -198.40390776]), 'dataset': 'predict_total_ap_RF_more_restricted', 'test_scores': {'MSE': 152.69275325974266, 'R2': 0.8000110111838038, 'MAE': 7.69205022624974}}\n",
            "Best classification model: {'model_name': 'Neural Network', 'params': array([0.79463296, 0.77881104, 0.78843844, 0.7956285 , 0.79414547]), 'dataset': 'predict_total_ap_RF_more_restricted', 'test_scores': {'Accuracy': 0.8326996197718631, 'F1-Score': 0.833185216798152}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Using selected features"
      ],
      "metadata": {
        "id": "omJhbTYtS0Vz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMG1dLC5HeAg"
      },
      "outputs": [],
      "source": [
        "dt_more_restricted_total_ap_results, dt_more_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=dt_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='DT',\n",
        "    imputation_strategy='median',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iEw5qXjHeAg"
      },
      "outputs": [],
      "source": [
        "lasso_more_restricted_total_ap_results, lasso_more_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=lasso_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='Lasso',\n",
        "    imputation_strategy='median',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmtTZR-THeAg"
      },
      "outputs": [],
      "source": [
        "fr_more_restricted_total_ap_results, fr_more_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=fr_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='FR',\n",
        "    imputation_strategy='median',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Several Imputations and Test - T4 GPU"
      ],
      "metadata": {
        "id": "iz2VItrNx_6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results, original_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxZSMuWAQ9Vk",
        "outputId": "0557fbfa-6d32-4797-b370-62c3aaa48547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_AP\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-200.79392161 -219.86076691 -220.56014285 -192.3899982  -212.80426943]\n",
            "Test set scores for Linear Regression: {'MSE': 190.43678728142854, 'R2': 0.7505758478463489, 'MAE': 8.76275384891383}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-354.36191135 -302.01173979 -343.09948464 -327.15033932 -322.63158691]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 286.7096894179701, 'R2': 0.6244826316480938, 'MAE': 8.480553324595014}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-212.86366694 -192.833936   -221.04861179 -182.50848612 -215.69472974]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 175.1178489818794, 'R2': 0.770639792695474, 'MAE': 7.380838738554794}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-719.98338227 -803.33774807 -805.37767011 -767.65203367 -719.50384844]\n",
            "Test set scores for Support Vector Machine: {'MSE': 775.2412380015862, 'R2': -0.01537046105140516, 'MAE': 16.30049026458048}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-213.07321408 -210.27727956 -221.97475662 -207.87599858 -221.45334584]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 186.9037660691653, 'R2': 0.755203214401884, 'MAE': 7.2975737823646565}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-277.29973969 -268.80778745 -278.0838138  -254.79620958 -253.2021109 ]\n",
            "Test set scores for Neural Network: {'MSE': 235.828761671811, 'R2': 0.6911238118793175, 'MAE': 10.462820829738842}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.60283665 0.60245428 0.6202204  0.60308859 0.61338771 0.61169595\n",
            " 0.59831689 0.60869477 0.61576142 0.62874007]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7287705956907478, 'F1-Score': 0.6539691850107289}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.76475643 0.77706701 0.76772583 0.77880405 0.77223434 0.81944649\n",
            " 0.78558594 0.77821251 0.78791131 0.77172271]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.811787072243346, 'F1-Score': 0.8047204014879236}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.79635861 0.79592074 0.79754893 0.80498463 0.79385187 0.82748448\n",
            " 0.79719691 0.79590151 0.81414474 0.79666462]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8212927756653993, 'F1-Score': 0.8111357069832889}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.1}\n",
            "Cross-validation scores for Support Vector Machine: [0.51052018 0.51052018 0.51182043 0.51182043 0.51182043 0.51182043\n",
            " 0.51182043 0.51182043 0.50981653 0.50981653]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6615969581749049, 'F1-Score': 0.5268552435809312}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.77550748 0.76845001 0.77214611 0.79232696 0.77198324 0.81282597\n",
            " 0.7952649  0.76681543 0.78391578 0.77340437]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8105196451204055, 'F1-Score': 0.8034203231365205}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.70358096 0.62110004 0.64568737 0.60888077 0.70896696 0.69349675\n",
            " 0.58501806 0.58456094 0.58296116 0.71026812]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6615969581749049, 'F1-Score': 0.5268552435809312}\n",
            "Best regression model saved to best_models/combined_df_Total_AP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_AP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 545.18 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-212.86366694, -192.833936  , -221.04861179, -182.50848612,\n",
            "       -215.69472974]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'MSE': 175.1178489818794, 'R2': 0.770639792695474, 'MAE': 7.380838738554794}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.79635861, 0.79592074, 0.79754893, 0.80498463, 0.79385187,\n",
            "       0.82748448, 0.79719691, 0.79590151, 0.81414474, 0.79666462]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8212927756653993, 'F1-Score': 0.8111357069832889}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results_mean, original_total_ap_best_model_mean = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    imputation_strategy='mean',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cbyIsy9yNyT",
        "outputId": "b7761725-fa01-4167-c8a9-3babedabed35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_AP\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-167.19714914 -184.97225746 -179.2603239  -152.36950061 -175.22779997]\n",
            "Test set scores for Linear Regression: {'MSE': 2.3857261581400292e+23, 'R2': -3.124699448880559e+20, 'MAE': 12295800185.869598}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-318.65696456 -302.13278375 -366.06097411 -287.02215034 -339.94054338]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 324.70895040822154, 'R2': 0.5747131853648384, 'MAE': 9.057737089343552}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-176.8344524  -163.70263964 -177.84505413 -154.13326366 -165.13546512]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 139.75092673011127, 'R2': 0.8169615392595719, 'MAE': 6.684721704676833}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-176.61067983 -195.11658887 -181.42944984 -165.79604078 -185.00828798]\n",
            "Test set scores for Support Vector Machine: {'MSE': 153.88595608811417, 'R2': 0.798448216473483, 'MAE': 6.624091934725391}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-203.56270299 -191.050673   -203.92457712 -180.0698923  -212.07768686]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 166.2910426528026, 'R2': 0.782200682354891, 'MAE': 6.81079123664675}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-181.12434234 -165.36316138 -174.35650431 -147.05950367 -177.20155929]\n",
            "Test set scores for Neural Network: {'MSE': 145.12685637236132, 'R2': 0.809920427548974, 'MAE': 7.029600675765378}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.80883263 0.82282357 0.82941835 0.78743137 0.80741925]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.829531051964512, 'F1-Score': 0.8254066313151681}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.76268347 0.78658622 0.77742263 0.76316812 0.7930617 ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8092522179974652, 'F1-Score': 0.8040037342455629}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.81976851 0.81274973 0.8378243  0.81284507 0.81013883]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8466413181242078, 'F1-Score': 0.8418112738306026}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.81112939 0.81498902 0.83593598 0.80961812 0.81493557]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8358681875792142, 'F1-Score': 0.8302992104823192}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.79259164 0.80662146 0.80879711 0.78789829 0.78711536]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.811787072243346, 'F1-Score': 0.8047377933881568}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.8139995  0.81051612 0.80513624 0.81538316 0.81321019]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8244613434727504, 'F1-Score': 0.8137082083952846}\n",
            "Best regression model saved to best_models/combined_df_Total_AP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_AP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 920.94 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-176.8344524 , -163.70263964, -177.84505413, -154.13326366,\n",
            "       -165.13546512]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'MSE': 139.75092673011127, 'R2': 0.8169615392595719, 'MAE': 6.684721704676833}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.81976851, 0.81274973, 0.8378243 , 0.81284507, 0.81013883]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8466413181242078, 'F1-Score': 0.8418112738306026}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results_median, original_total_ap_best_model_median = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    imputation_strategy='median',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6q3zNIKyPVV",
        "outputId": "a4804478-0326-4066-b804-991c17c45a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_AP\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-167.06263551 -181.84860998 -177.01012095 -150.67951069 -174.07183842]\n",
            "Test set scores for Linear Regression: {'MSE': 3.342304840190633e+23, 'R2': -4.377576217832418e+20, 'MAE': 14553569685.334995}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-321.21245281 -243.6119102  -320.89251664 -308.56417138 -361.38701006]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 305.85814574906976, 'R2': 0.5994029842038339, 'MAE': 8.82145648500211}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-179.75377033 -167.20026191 -185.4526863  -153.25860877 -164.75818497]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 135.49196247127253, 'R2': 0.8225397080812487, 'MAE': 6.586813693443572}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-179.20713805 -199.64396196 -183.86094822 -168.27964167 -188.15610381]\n",
            "Test set scores for Support Vector Machine: {'MSE': 156.86052822136028, 'R2': 0.7945522773382656, 'MAE': 6.751617474999728}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-203.88874885 -191.80149304 -204.78408099 -180.45407355 -207.14625311]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 166.7824810532578, 'R2': 0.781557022019516, 'MAE': 6.801647655259822}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-176.60769624 -158.80340092 -177.79058862 -151.58897515 -170.18137742]\n",
            "Test set scores for Neural Network: {'MSE': 145.23100462614468, 'R2': 0.8097840196087381, 'MAE': 6.950538205421891}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.81153864 0.81539058 0.82873517 0.80509947 0.80679505]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8314321926489227, 'F1-Score': 0.8280703509416728}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.76627517 0.7806411  0.791997   0.76134113 0.77534022]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8193916349809885, 'F1-Score': 0.8155329802399698}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.82214946 0.81642669 0.83269124 0.81148738 0.81399984]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8428390367553865, 'F1-Score': 0.8367126899489952}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.80531044 0.81447106 0.83678119 0.8009109  0.8124732 ]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8358681875792142, 'F1-Score': 0.8300549240863535}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.79114815 0.79524085 0.79519034 0.79027007 0.78057975]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.811787072243346, 'F1-Score': 0.8052145792952392}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.80577545 0.81667581 0.81865496 0.81998262 0.81677738]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8282636248415716, 'F1-Score': 0.8254933144275836}\n",
            "Best regression model saved to best_models/combined_df_Total_AP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_AP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 974.97 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-179.75377033, -167.20026191, -185.4526863 , -153.25860877,\n",
            "       -164.75818497]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'MSE': 135.49196247127253, 'R2': 0.8225397080812487, 'MAE': 6.586813693443572}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.82214946, 0.81642669, 0.83269124, 0.81148738, 0.81399984]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8428390367553865, 'F1-Score': 0.8367126899489952}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results_knn, original_total_ap_best_model_knn = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    imputation_strategy='knn',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS5miq3byTBE",
        "outputId": "51fe3439-0bb2-4397-c717-84dbdf272c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_AP\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-178.66424449 -193.95289288 -188.79909246 -158.09985219 -184.54164346]\n",
            "Test set scores for Linear Regression: {'MSE': 2.5432288229346357e+23, 'R2': -3.33098820846922e+20, 'MAE': 12695190415.500898}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-306.02539549 -337.55465332 -404.58273811 -329.52585121 -321.79215586]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 246.08429196520183, 'R2': 0.6776916542335635, 'MAE': 8.287206027320096}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-170.16709114 -181.07153827 -178.24348575 -150.91977336 -178.40032283]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 140.71556250486648, 'R2': 0.8156981097316424, 'MAE': 6.605829596554561}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-185.32246204 -203.45245052 -195.54499687 -177.87609667 -196.92600651]\n",
            "Test set scores for Support Vector Machine: {'MSE': 166.4951361299198, 'R2': 0.7819333713840604, 'MAE': 6.568225232222153}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-193.99623507 -205.04445199 -197.41212523 -185.09303341 -221.49621592]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 160.99737461524535, 'R2': 0.7891340521144813, 'MAE': 6.748958899149012}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-188.49539945 -166.30038876 -188.69814501 -159.80554882 -181.54853554]\n",
            "Test set scores for Neural Network: {'MSE': 165.25393131392852, 'R2': 0.7835590365892823, 'MAE': 7.341067129950189}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.81196733 0.81320562 0.82980483 0.79796592 0.80964562]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8288973384030418, 'F1-Score': 0.8259427526761459}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.76393214 0.77256238 0.78189875 0.77804043 0.77585217]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8010139416983524, 'F1-Score': 0.7971085295911304}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.80927217 0.80742489 0.8241518  0.80126866 0.80985641]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8460076045627376, 'F1-Score': 0.8415984578308399}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.8071223  0.81161598 0.83193937 0.80048149 0.8059764 ]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8339670468948035, 'F1-Score': 0.8289771239705671}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.78177459 0.78242295 0.79787217 0.79589481 0.76880962]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8079847908745247, 'F1-Score': 0.8004774079477981}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.80575379 0.79945993 0.80945465 0.80381987 0.79895463]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8301647655259823, 'F1-Score': 0.8264382034384677}\n",
            "Best regression model saved to best_models/combined_df_Total_AP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_AP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 891.39 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-170.16709114, -181.07153827, -178.24348575, -150.91977336,\n",
            "       -178.40032283]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'MSE': 140.71556250486648, 'R2': 0.8156981097316424, 'MAE': 6.605829596554561}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.80927217, 0.80742489, 0.8241518 , 0.80126866, 0.80985641]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8460076045627376, 'F1-Score': 0.8415984578308399}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results_zero, original_total_ap_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-Qc-9sGyLK3",
        "outputId": "eb249bcc-3602-49a7-84b3-2a3fdd67c969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_AP\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-163.07572659 -174.0228268  -170.5257389  -143.02765517 -167.08778843]\n",
            "Test set scores for Linear Regression: {'MSE': 3.8050004681922454e+23, 'R2': -4.9835907718846965e+20, 'MAE': 15528298039.934677}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-310.76045009 -327.45640215 -348.52434244 -249.91526533 -308.51394212]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 332.28800300993987, 'R2': 0.5647865383325197, 'MAE': 9.071120767698714}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-181.95008019 -167.48348668 -177.21218851 -143.61851809 -166.97867058]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 138.68761132301677, 'R2': 0.8183542142131486, 'MAE': 6.662455254217081}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-175.31890888 -191.37643855 -180.27531466 -163.91314774 -189.02412296]\n",
            "Test set scores for Support Vector Machine: {'MSE': 157.16467319483553, 'R2': 0.7941539241459868, 'MAE': 6.208131260429188}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-197.8856141  -198.97106016 -190.44183188 -169.59175588 -214.05166726]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 163.7770880215204, 'R2': 0.7854933287569418, 'MAE': 6.699076588810429}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-176.4230974  -166.49992854 -192.65426592 -153.2788315  -166.09849604]\n",
            "Test set scores for Neural Network: {'MSE': 159.4477623907951, 'R2': 0.7911636532265776, 'MAE': 7.019302198027224}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.83002164 0.82343274 0.84738256 0.81867565 0.82910419]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8415716096324461, 'F1-Score': 0.8389613490273139}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.77071306 0.79373477 0.76616439 0.7684822  0.79316126]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8041825095057035, 'F1-Score': 0.7999492557481415}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.8301654  0.81152085 0.83179115 0.81085788 0.81111767]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8390367553865653, 'F1-Score': 0.8338608612804891}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.82054649 0.82408781 0.84342144 0.81959585 0.82039957]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8346007604562737, 'F1-Score': 0.8297945545335846}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.7957378  0.79679196 0.81146907 0.7965713  0.78427702]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8225602027883396, 'F1-Score': 0.8165805931036759}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.82357193 0.80387803 0.83127934 0.80707719 0.80649088]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8269961977186312, 'F1-Score': 0.8253592322310446}\n",
            "Best regression model saved to best_models/combined_df_Total_AP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_AP_classification_Logistic Regression.pkl\n",
            "Processing complete for datasets. Total time: 880.98 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-181.95008019, -167.48348668, -177.21218851, -143.61851809,\n",
            "       -166.97867058]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'MSE': 138.68761132301677, 'R2': 0.8183542142131486, 'MAE': 6.662455254217081}}\n",
            "Best classification model: {'model_name': 'Logistic Regression', 'params': array([0.83002164, 0.82343274, 0.84738256, 0.81867565, 0.82910419]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8415716096324461, 'F1-Score': 0.8389613490273139}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Using selected features**"
      ],
      "metadata": {
        "id": "U_xnG-cDHeAZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "071eefd9-5b20-40b9-e519-7d6b3054aaf0",
        "id": "e-lLNQh8HeAf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_MI_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-182.0651522  -202.92519892 -194.73290528 -174.8535481  -179.66125447]\n",
            "Test set scores for Linear Regression: {'MSE': 157.79729581078337, 'R2': 0.793325348103151, 'MAE': 8.013893906197902}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-271.59360865 -287.7456871  -329.10705485 -260.03785542 -308.27860947]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 230.98798894926023, 'R2': 0.6974640030226729, 'MAE': 7.77646991051817}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-149.22889691 -162.21479643 -165.92455361 -141.61944823 -160.97752081]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 105.21486908131573, 'R2': 0.8621950627859399, 'MAE': 5.93837142553542}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-207.47940398 -225.93419794 -214.90401772 -198.99046809 -205.77853739]\n",
            "Test set scores for Support Vector Machine: {'MSE': 172.52799529288052, 'R2': 0.7740318477169963, 'MAE': 7.485262677591578}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-186.21902823 -191.7855123  -182.96917753 -182.2953362  -195.26952683]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 128.19630894182768, 'R2': 0.8320951738185455, 'MAE': 6.2954010501539015}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-151.38789448 -157.94434122 -162.91723782 -174.15278987 -154.55778713]\n",
            "Test set scores for Neural Network: {'MSE': 113.82764871488968, 'R2': 0.8509144941077058, 'MAE': 6.456089816553299}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.78133424 0.77545333 0.79161687 0.78540464 0.79173383]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8212927756653993, 'F1-Score': 0.8120024052257131}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.76936919 0.75593622 0.80271191 0.79286443 0.77114552]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8111533586818758, 'F1-Score': 0.8047298251773822}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.80457111 0.78897071 0.8281761  0.81038986 0.81046364]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8422053231939164, 'F1-Score': 0.8356737981873823}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.79345188 0.78421072 0.81510268 0.79959271 0.80405545]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8371356147021546, 'F1-Score': 0.8284196112493091}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.7697621  0.77664601 0.79631714 0.79086484 0.78980449]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8231939163498099, 'F1-Score': 0.818156875862864}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.8166664  0.80666359 0.82234024 0.82353884 0.81186299]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8314321926489227, 'F1-Score': 0.83379205040097}\n",
            "Best regression model saved to best_models/predict_total_ap_MI_more_restricted_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/predict_total_ap_MI_more_restricted_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 1245.27 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-149.22889691, -162.21479643, -165.92455361, -141.61944823,\n",
            "       -160.97752081]), 'dataset': 'predict_total_ap_MI_more_restricted', 'test_scores': {'MSE': 105.21486908131573, 'R2': 0.8621950627859399, 'MAE': 5.93837142553542}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.80457111, 0.78897071, 0.8281761 , 0.81038986, 0.81046364]), 'dataset': 'predict_total_ap_MI_more_restricted', 'test_scores': {'Accuracy': 0.8422053231939164, 'F1-Score': 0.8356737981873823}}\n"
          ]
        }
      ],
      "source": [
        "# Example for analyzing 'Total_AP' with 'MI' feature set and 'more_restricted' exclusion type\n",
        "mi_more_restricted_total_ap_results, mi_more_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_ap_datasets,  # This dictionary was created using your structure\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='MI',\n",
        "    imputation_strategy='median',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf72dd0-6b73-4f90-e2f3-b97a976183df",
        "id": "DxmfNmHjHeAg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_RF_more_restricted_regression\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-186.50792099 -210.86066602 -202.88906156 -179.0533142  -192.17091088]\n",
            "Test set scores for Linear Regression: {'MSE': 173.43917916934865, 'R2': 0.7728384266921589, 'MAE': 8.186496093239938}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-311.91505872 -341.09577201 -360.63305366 -299.80294424 -300.80977611]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 274.1890188187719, 'R2': 0.6408815516949071, 'MAE': 8.627498617422573}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-176.6087383  -178.68476898 -198.22774626 -156.20874578 -171.12979127]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 147.41664329487892, 'R2': 0.8069213842973283, 'MAE': 6.951164176983976}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-196.26498938 -218.64737847 -205.76707325 -189.58103842 -203.99962566]\n",
            "Test set scores for Support Vector Machine: {'MSE': 176.25463155640372, 'R2': 0.769150894285238, 'MAE': 7.725254060279069}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-204.1194435  -198.71882625 -201.13213558 -180.98756428 -214.16531906]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 164.27262616073045, 'R2': 0.7848442987979896, 'MAE': 6.894984609813507}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-181.31087039 -171.98990876 -188.0823747  -163.24688855 -176.59281027]\n",
            "Test set scores for Neural Network: {'MSE': 152.95404769073707, 'R2': 0.7996687813927869, 'MAE': 7.407754019818484}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.78321669 0.77592719 0.79807987 0.78019405 0.7825079 ]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8124207858048162, 'F1-Score': 0.8026534918460189}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.75841712 0.75426117 0.77495701 0.75241855 0.7543297 ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.7889733840304183, 'F1-Score': 0.7837679375212551}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.81618509 0.79898257 0.81559901 0.79207241 0.79690727]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8276299112801014, 'F1-Score': 0.8197648245178314}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.77644787 0.77703633 0.80403747 0.78180466 0.77512847]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8168567807351077, 'F1-Score': 0.8014105442616822}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.78828393 0.79338595 0.79661592 0.78430796 0.77264668]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8073510773130546, 'F1-Score': 0.7987718463545065}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.80442868 0.79688639 0.80783916 0.80016036 0.79723208]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8307984790874525, 'F1-Score': 0.8302755046762595}\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_RF_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-182.44834996 -204.18678963 -193.93311985 -175.39014923 -181.51790054]\n",
            "Test set scores for Linear Regression: {'MSE': 161.12019724206124, 'R2': 0.7889731854563311, 'MAE': 8.061458014763483}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-271.36524662 -321.98545609 -314.71160485 -268.8979689  -312.61398414]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 227.01593869505317, 'R2': 0.7026663868746079, 'MAE': 7.776112128226382}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-142.65563505 -155.43058128 -156.45184688 -135.95472427 -154.97192173]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 105.54446433805323, 'R2': 0.8617633761426228, 'MAE': 5.929897632187444}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-183.63931574 -203.44826396 -194.6616062  -177.53977096 -188.58717203]\n",
            "Test set scores for Support Vector Machine: {'MSE': 155.5721872701289, 'R2': 0.7962396789901892, 'MAE': 7.186831880041698}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-178.93195663 -183.78777449 -178.47554902 -172.85971409 -192.25235292]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 127.05051602390006, 'R2': 0.8335958735057095, 'MAE': 6.074597139235921}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-156.4744499  -160.83872434 -153.97919041 -144.07463496 -143.7422327 ]\n",
            "Test set scores for Neural Network: {'MSE': 119.26928355250259, 'R2': 0.8437873251658375, 'MAE': 6.64154719413452}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.78599595 0.78067052 0.80449617 0.78672146 0.78781984]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8193916349809885, 'F1-Score': 0.8114883565499054}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.77518746 0.75005509 0.80690156 0.78733657 0.76518237]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8079847908745247, 'F1-Score': 0.8016427816505864}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.80670372 0.80133829 0.83493937 0.81602496 0.81069496]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8504435994930292, 'F1-Score': 0.8446326656710378}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.79579018 0.78265514 0.81847729 0.8016929  0.79304042]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8434727503168568, 'F1-Score': 0.8356859010433524}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.79288503 0.79505063 0.81554215 0.80904444 0.79160597]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8250950570342205, 'F1-Score': 0.8176012712354741}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.81088722 0.8016645  0.81746566 0.81925362 0.82028418]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8472750316856781, 'F1-Score': 0.8395544470141111}\n",
            "Best regression model saved to best_models/predict_total_ap_RF_more_restricted_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/predict_total_ap_RF_more_restricted_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 2301.94 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-142.65563505, -155.43058128, -156.45184688, -135.95472427,\n",
            "       -154.97192173]), 'dataset': 'predict_total_ap_RF_more_restricted', 'test_scores': {'MSE': 105.54446433805323, 'R2': 0.8617633761426228, 'MAE': 5.929897632187444}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.80670372, 0.80133829, 0.83493937, 0.81602496, 0.81069496]), 'dataset': 'predict_total_ap_RF_more_restricted', 'test_scores': {'Accuracy': 0.8504435994930292, 'F1-Score': 0.8446326656710378}}\n"
          ]
        }
      ],
      "source": [
        "rf_more_restricted_total_ap_results, rf_more_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='RF',\n",
        "    imputation_strategy='median',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rh3FuK7DLDP"
      },
      "source": [
        "#### More Restricted - L4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing missing data (no imputation)**"
      ],
      "metadata": {
        "id": "qKnCcvgTvDGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results, original_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgIqEgiePmNE",
        "outputId": "70f9f7c3-eb19-400b-9ef3-603a2c36b085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_AP\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-200.79392161 -219.86076691 -220.56014285 -192.3899982  -212.80426943]\n",
            "Test set scores for Linear Regression: {'MSE': 190.43678728142862, 'R2': 0.7505758478463488, 'MAE': 8.762753848913828}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-354.36191135 -302.01173979 -343.09948464 -327.15033932 -322.63158691]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 286.7096894179701, 'R2': 0.6244826316480938, 'MAE': 8.480553324595014}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-212.86366694 -192.833936   -221.04861179 -182.50848612 -215.69472974]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 175.1178489818794, 'R2': 0.770639792695474, 'MAE': 7.380838738554794}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-719.98338227 -803.33774807 -805.37767011 -767.65203367 -719.50384844]\n",
            "Test set scores for Support Vector Machine: {'MSE': 775.2412380015862, 'R2': -0.01537046105140516, 'MAE': 16.30049026458048}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-213.07321408 -210.27727956 -221.97475662 -207.87599858 -221.45334584]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 186.9037660691653, 'R2': 0.755203214401884, 'MAE': 7.2975737823646565}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-277.29973969 -268.80778745 -278.0838138  -254.79620958 -253.2021109 ]\n",
            "Test set scores for Neural Network: {'MSE': 235.82876167181124, 'R2': 0.6911238118793173, 'MAE': 10.462820829738854}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.60283665 0.60245428 0.6202204  0.60308859 0.61338771 0.61169595\n",
            " 0.59831689 0.60869477 0.61576142 0.62874007]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7287705956907478, 'F1-Score': 0.6539691850107289}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.76475643 0.77706701 0.76772583 0.77880405 0.77223434 0.81944649\n",
            " 0.78558594 0.77821251 0.78791131 0.77172271]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.811787072243346, 'F1-Score': 0.8047204014879236}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.79635861 0.79592074 0.79754893 0.80498463 0.79385187 0.82748448\n",
            " 0.79719691 0.79590151 0.81414474 0.79666462]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8212927756653993, 'F1-Score': 0.8111357069832889}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.1}\n",
            "Cross-validation scores for Support Vector Machine: [0.51052018 0.51052018 0.51182043 0.51182043 0.51182043 0.51182043\n",
            " 0.51182043 0.51182043 0.50981653 0.50981653]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6615969581749049, 'F1-Score': 0.5268552435809312}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.77550748 0.76845001 0.77214611 0.79232696 0.77198324 0.81282597\n",
            " 0.7952649  0.76681543 0.78391578 0.77340437]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8105196451204055, 'F1-Score': 0.8034203231365205}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.54080157 0.59022772 0.67724061 0.57536042 0.70400433 0.73215135\n",
            " 0.58501806 0.62585355 0.58288773 0.58963688]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.7129277566539924, 'F1-Score': 0.6007742656375408}\n",
            "Best regression model saved to best_models/combined_df_Total_AP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_AP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 490.53 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-212.86366694, -192.833936  , -221.04861179, -182.50848612,\n",
            "       -215.69472974]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'MSE': 175.1178489818794, 'R2': 0.770639792695474, 'MAE': 7.380838738554794}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.79635861, 0.79592074, 0.79754893, 0.80498463, 0.79385187,\n",
            "       0.82748448, 0.79719691, 0.79590151, 0.81414474, 0.79666462]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8212927756653993, 'F1-Score': 0.8111357069832889}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing missing data (no imputation) and PCA**"
      ],
      "metadata": {
        "id": "1dtnNXlV5REd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results_pca, original_total_ap_best_model_pca = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    apply_pca=True,\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZC8ZLQoqbQuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero Imputation**"
      ],
      "metadata": {
        "id": "4C2AvmR-5T5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_AP' using the original dataset without feature selection\n",
        "original_total_ap_results_zero, original_total_ap_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_AP',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")\n"
      ],
      "metadata": {
        "id": "T51baelpxc1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using selected features**"
      ],
      "metadata": {
        "id": "Zq6TWKgD5cpH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsNjYKeeDLDP",
        "outputId": "bd2b088b-2cc8-4c00-f547-177a00c0ce78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_MI_more_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-245.25475019 -275.39960014 -270.79829502 -246.13460971 -257.7444634 ]\n",
            "Test set scores for Linear Regression: {'MSE': 238.22160089177677, 'R2': 0.6879897961137713, 'MAE': 10.257488238257768}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-289.51980115 -358.71452461 -293.83119115 -306.83965106 -313.52312419]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 275.56590799301387, 'R2': 0.6390781742078289, 'MAE': 8.843642301967263}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-196.22086974 -215.65527301 -208.42204321 -184.74007188 -203.92056146]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 165.42311622650394, 'R2': 0.7833374470320401, 'MAE': 7.765713148067143}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-232.61585387 -244.9654176  -249.36308877 -225.85221999 -241.40325032]\n",
            "Test set scores for Support Vector Machine: {'MSE': 209.34179930167085, 'R2': 0.7258150510385604, 'MAE': 8.521442770740618}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-226.62657165 -236.35113222 -234.45622821 -220.43882726 -260.6407607 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 202.60382762991125, 'R2': 0.7346400942219466, 'MAE': 8.173510773130547}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-179.74903228 -183.33546409 -188.16914096 -173.61392778 -198.40390776]\n",
            "Test set scores for Neural Network: {'MSE': 152.69275325974266, 'R2': 0.8000110111838038, 'MAE': 7.69205022624974}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.72711296 0.74279524 0.72887365 0.72209938 0.72874572]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7667934093789607, 'F1-Score': 0.7495521083108555}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.75526889 0.76294015 0.78201812 0.78426069 0.761529  ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8143219264892269, 'F1-Score': 0.8091868500508422}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.78086938 0.78278505 0.81029399 0.79600553 0.79134115]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8326996197718631, 'F1-Score': 0.8238086210469641}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.76998465 0.77315563 0.78933143 0.78513639 0.78008295]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8187579214195184, 'F1-Score': 0.8073068805052824}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.76618224 0.75020517 0.77674145 0.76793147 0.75762252]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7997465145754119, 'F1-Score': 0.7886888141779949}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.79463296 0.77881104 0.78843844 0.7956285  0.79414547]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8326996197718631, 'F1-Score': 0.833185216798152}\n",
            "Best model saved to best_models/predict_total_ap_MI_more_restricted_Neural Network.pkl\n",
            "Processing complete for 'predict_total_ap_MI_more_restricted'. Total time: 501.35 seconds\n",
            "Best model: {'model_name': 'Neural Network', 'params': array([0.79463296, 0.77881104, 0.78843844, 0.7956285 , 0.79414547]), 'dataset': 'predict_total_ap_MI_more_restricted', 'test_scores': {'Accuracy': 0.8326996197718631, 'F1-Score': 0.833185216798152}}\n"
          ]
        }
      ],
      "source": [
        "# Example for analyzing 'Total_AP' with 'MI' feature set and 'more_restricted' exclusion type\n",
        "mi_more_restricted_total_ap_results, mi_more_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_ap_datasets,  # This dictionary was created using your structure\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='MI',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djvNoLhkDLDP",
        "outputId": "0427d8ef-13a7-4693-de29-8f5dd7a4ee85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_RF_more_restricted_regression\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-247.53847786 -280.12702593 -270.3252893  -248.64364402 -261.6316098 ]\n",
            "Test set scores for Linear Regression: {'MSE': 239.23405303163352, 'R2': 0.6866637392096149, 'MAE': 10.367065808842053}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-368.09742719 -380.52221886 -406.11780575 -368.00467648 -359.42184907]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 355.8814252776453, 'R2': 0.5338851067891017, 'MAE': 10.446852005820466}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-248.48920872 -259.35021075 -273.1279423  -226.7991639  -263.64932149]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 212.04031338025973, 'R2': 0.7222806783171174, 'MAE': 9.061500865165131}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-255.37131594 -269.54750184 -271.56121519 -252.77949427 -268.20220622]\n",
            "Test set scores for Support Vector Machine: {'MSE': 233.59914838875372, 'R2': 0.6940440428425466, 'MAE': 9.525785827889074}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-255.67658797 -269.5127248  -271.03636922 -260.69208254 -289.01279149]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 229.87352887923228, 'R2': 0.6989236645830469, 'MAE': 9.448216548977005}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-228.96796353 -227.95190343 -246.8459433  -217.13670685 -253.69686905]\n",
            "Test set scores for Neural Network: {'MSE': 207.2736066553645, 'R2': 0.7285238616872773, 'MAE': 9.52210984970238}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.71074256 0.73220196 0.74502767 0.70821068 0.72673898]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7604562737642585, 'F1-Score': 0.7375223857904403}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.72225434 0.73897691 0.73360178 0.72570345 0.7167622 ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.7718631178707225, 'F1-Score': 0.7485585484580918}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.76876292 0.744325   0.74754687 0.73768539 0.74258786]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.7807351077313055, 'F1-Score': 0.7683540633024609}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.70759251 0.73232196 0.73769074 0.72321785 0.72125445]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.7832699619771863, 'F1-Score': 0.7550049994458161}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.73429067 0.74119479 0.74205835 0.72862444 0.72870458]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7782002534854245, 'F1-Score': 0.7624569019904387}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.75837139 0.76155749 0.76265211 0.7367457  0.74121196]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.7839036755386565, 'F1-Score': 0.7605984376373711}\n",
            "Best model saved to best_models/predict_total_ap_RF_more_restricted_regression_Random Forest.pkl\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_RF_more_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-245.25475019 -275.39960014 -270.79829502 -246.13460971 -257.7444634 ]\n",
            "Test set scores for Linear Regression: {'MSE': 238.22160089177677, 'R2': 0.6879897961137713, 'MAE': 10.257488238257768}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-289.51980115 -358.71452461 -293.83119115 -306.83965106 -313.52312419]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 275.56590799301387, 'R2': 0.6390781742078289, 'MAE': 8.843642301967263}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-196.22086974 -215.65527301 -208.42204321 -184.74007188 -203.92056146]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 165.42311622650394, 'R2': 0.7833374470320401, 'MAE': 7.765713148067143}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-232.61585387 -244.9654176  -249.36308877 -225.85221999 -241.40325032]\n",
            "Test set scores for Support Vector Machine: {'MSE': 209.34179930167085, 'R2': 0.7258150510385604, 'MAE': 8.521442770740618}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-226.62657165 -236.35113222 -234.45622821 -220.43882726 -260.6407607 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 202.60382762991125, 'R2': 0.7346400942219466, 'MAE': 8.173510773130547}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-179.74903228 -183.33546409 -188.16914096 -173.61392778 -198.40390776]\n",
            "Test set scores for Neural Network: {'MSE': 152.69275325974266, 'R2': 0.8000110111838038, 'MAE': 7.69205022624974}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.72711296 0.74279524 0.72887365 0.72209938 0.72874572]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7667934093789607, 'F1-Score': 0.7495521083108555}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.75526889 0.76294015 0.78201812 0.78426069 0.761529  ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8143219264892269, 'F1-Score': 0.8091868500508422}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.78086938 0.78278505 0.81029399 0.79600553 0.79134115]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8326996197718631, 'F1-Score': 0.8238086210469641}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.76998465 0.77315563 0.78933143 0.78513639 0.78008295]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8187579214195184, 'F1-Score': 0.8073068805052824}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.76618224 0.75020517 0.77674145 0.76793147 0.75762252]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7997465145754119, 'F1-Score': 0.7886888141779949}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.79463296 0.77881104 0.78843844 0.7956285  0.79414547]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8326996197718631, 'F1-Score': 0.833185216798152}\n",
            "Best model saved to best_models/predict_total_ap_RF_more_restricted_Neural Network.pkl\n",
            "Processing complete for 'predict_total_ap_RF_more_restricted'. Total time: 1054.42 seconds\n",
            "Best model: {'model_name': 'Neural Network', 'params': array([0.79463296, 0.77881104, 0.78843844, 0.7956285 , 0.79414547]), 'dataset': 'predict_total_ap_RF_more_restricted', 'test_scores': {'Accuracy': 0.8326996197718631, 'F1-Score': 0.833185216798152}}\n"
          ]
        }
      ],
      "source": [
        "rf_more_restricted_total_ap_results, rf_more_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VidHKmtiDLDQ",
        "outputId": "b808a31a-e11b-4c3d-f6c4-f9504378d291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_DT_more_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-245.25475019 -275.39960014 -270.79829502 -246.13460971 -257.7444634 ]\n",
            "Test set scores for Linear Regression: {'MSE': 238.22160089177677, 'R2': 0.6879897961137713, 'MAE': 10.257488238257768}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-289.51980115 -358.71452461 -293.83119115 -306.83965106 -313.52312419]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 275.56590799301387, 'R2': 0.6390781742078289, 'MAE': 8.843642301967263}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-196.22086974 -215.65527301 -208.42204321 -184.74007188 -203.92056146]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 165.42311622650394, 'R2': 0.7833374470320401, 'MAE': 7.765713148067143}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-232.61585387 -244.9654176  -249.36308877 -225.85221999 -241.40325032]\n",
            "Test set scores for Support Vector Machine: {'MSE': 209.34179930167085, 'R2': 0.7258150510385604, 'MAE': 8.521442770740618}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-226.62657165 -236.35113222 -234.45622821 -220.43882726 -260.6407607 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 202.60382762991125, 'R2': 0.7346400942219466, 'MAE': 8.173510773130547}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-179.74903228 -183.33546409 -188.16914096 -173.61392778 -198.40390776]\n",
            "Test set scores for Neural Network: {'MSE': 152.69275325974266, 'R2': 0.8000110111838038, 'MAE': 7.69205022624974}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.72711296 0.74279524 0.72887365 0.72209938 0.72874572]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7667934093789607, 'F1-Score': 0.7495521083108555}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.75526889 0.76294015 0.78201812 0.78426069 0.761529  ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8143219264892269, 'F1-Score': 0.8091868500508422}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.78086938 0.78278505 0.81029399 0.79600553 0.79134115]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8326996197718631, 'F1-Score': 0.8238086210469641}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.76998465 0.77315563 0.78933143 0.78513639 0.78008295]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8187579214195184, 'F1-Score': 0.8073068805052824}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.76618224 0.75020517 0.77674145 0.76793147 0.75762252]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7997465145754119, 'F1-Score': 0.7886888141779949}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.79463296 0.77881104 0.78843844 0.7956285  0.79414547]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8326996197718631, 'F1-Score': 0.833185216798152}\n",
            "Best model saved to best_models/predict_total_ap_DT_more_restricted_Neural Network.pkl\n",
            "Processing complete for 'predict_total_ap_DT_more_restricted'. Total time: 503.25 seconds\n",
            "Best model: {'model_name': 'Neural Network', 'params': array([0.79463296, 0.77881104, 0.78843844, 0.7956285 , 0.79414547]), 'dataset': 'predict_total_ap_DT_more_restricted', 'test_scores': {'Accuracy': 0.8326996197718631, 'F1-Score': 0.833185216798152}}\n"
          ]
        }
      ],
      "source": [
        "dt_more_restricted_total_ap_results, dt_more_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=dt_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='DT',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUyzCZR4DLDQ",
        "outputId": "52ad9a22-c96e-4f29-a285-318b435efdbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_Lasso_more_restricted_regression\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-247.53847786 -280.12702593 -270.3252893  -248.64364402 -261.6316098 ]\n",
            "Test set scores for Linear Regression: {'MSE': 239.23405303163352, 'R2': 0.6866637392096149, 'MAE': 10.367065808842053}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-368.09742719 -380.52221886 -406.11780575 -368.00467648 -359.42184907]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 355.8814252776453, 'R2': 0.5338851067891017, 'MAE': 10.446852005820466}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-248.48920872 -259.35021075 -273.1279423  -226.7991639  -263.64932149]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 212.04031338025973, 'R2': 0.7222806783171174, 'MAE': 9.061500865165131}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-255.37131594 -269.54750184 -271.56121519 -252.77949427 -268.20220622]\n",
            "Test set scores for Support Vector Machine: {'MSE': 233.59914838875372, 'R2': 0.6940440428425466, 'MAE': 9.525785827889074}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-255.67658797 -269.5127248  -271.03636922 -260.69208254 -289.01279149]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 229.87352887923228, 'R2': 0.6989236645830469, 'MAE': 9.448216548977005}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-228.96796353 -227.95190343 -246.8459433  -217.13670685 -253.69686905]\n",
            "Test set scores for Neural Network: {'MSE': 207.2736066553645, 'R2': 0.7285238616872773, 'MAE': 9.52210984970238}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.71074256 0.73220196 0.74502767 0.70821068 0.72673898]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7604562737642585, 'F1-Score': 0.7375223857904403}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.72225434 0.73897691 0.73360178 0.72570345 0.7167622 ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.7718631178707225, 'F1-Score': 0.7485585484580918}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.76876292 0.744325   0.74754687 0.73768539 0.74258786]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.7807351077313055, 'F1-Score': 0.7683540633024609}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.70759251 0.73232196 0.73769074 0.72321785 0.72125445]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.7832699619771863, 'F1-Score': 0.7550049994458161}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.73429067 0.74119479 0.74205835 0.72862444 0.72870458]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7782002534854245, 'F1-Score': 0.7624569019904387}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.75837139 0.76155749 0.76265211 0.7367457  0.74121196]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.7839036755386565, 'F1-Score': 0.7605984376373711}\n",
            "Best model saved to best_models/predict_total_ap_Lasso_more_restricted_regression_Random Forest.pkl\n",
            "Processing complete for 'predict_total_ap_Lasso_more_restricted_regression'. Total time: 545.42 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.76876292, 0.744325  , 0.74754687, 0.73768539, 0.74258786]), 'dataset': 'predict_total_ap_Lasso_more_restricted_regression', 'test_scores': {'Accuracy': 0.7807351077313055, 'F1-Score': 0.7683540633024609}}\n"
          ]
        }
      ],
      "source": [
        "lasso_more_restricted_total_ap_results, lasso_more_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=lasso_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='Lasso',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVzBaVSIDLDQ",
        "outputId": "90b0bdf5-1627-4a2f-81ec-63086bc26201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_FR_more_restricted_regression\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-249.10899991 -286.52302084 -272.92861045 -252.59483523 -266.59844778]\n",
            "Test set scores for Linear Regression: {'MSE': 244.04831214807925, 'R2': 0.6803582742019905, 'MAE': 10.523532605521442}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-319.03427932 -339.22061805 -385.00069775 -343.25391993 -343.78171522]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 320.5097883456202, 'R2': 0.5802130283950209, 'MAE': 9.922527737037315}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-241.87971715 -256.1791351  -266.80550439 -225.03788527 -261.78949986]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 217.79037958072487, 'R2': 0.7147495420941621, 'MAE': 9.121874950138839}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-256.10656577 -275.11139482 -267.9458448  -250.55986841 -267.56484407]\n",
            "Test set scores for Support Vector Machine: {'MSE': 235.05685346329614, 'R2': 0.6921348169125244, 'MAE': 9.539158382682803}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-250.9308417  -272.00195518 -253.95379864 -235.928248   -275.24514053]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 221.96245570471535, 'R2': 0.7092851748109177, 'MAE': 9.19038565996741}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-221.03793057 -241.32079494 -244.5064783  -217.45613801 -245.50501681]\n",
            "Test set scores for Neural Network: {'MSE': 206.79493395882, 'R2': 0.7291508021707789, 'MAE': 9.431326365998704}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.70995384 0.72269814 0.72962162 0.71364863 0.73069651]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7547528517110266, 'F1-Score': 0.7325676374029807}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': None}\n",
            "Cross-validation scores for Decision Tree: [0.74357315 0.73720952 0.72995928 0.72454593 0.74134294]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.7655259822560203, 'F1-Score': 0.7542800980421337}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.75983893 0.75512462 0.74745997 0.74205058 0.75468119]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.7877059569074778, 'F1-Score': 0.773448113340409}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.70207563 0.71807275 0.7307924  0.72136609 0.72006151]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.7826362484157161, 'F1-Score': 0.7521723385960721}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.74282744 0.74198387 0.73165549 0.73164329 0.73234507]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7839036755386565, 'F1-Score': 0.769337682801719}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.76015871 0.74790107 0.75699723 0.75153031 0.73970128]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.7953105196451205, 'F1-Score': 0.775255440190918}\n",
            "Best model saved to best_models/predict_total_ap_FR_more_restricted_regression_Neural Network.pkl\n",
            "Processing complete for 'predict_total_ap_FR_more_restricted_regression'. Total time: 499.65 seconds\n",
            "Best model: {'model_name': 'Neural Network', 'params': array([0.76015871, 0.74790107, 0.75699723, 0.75153031, 0.73970128]), 'dataset': 'predict_total_ap_FR_more_restricted_regression', 'test_scores': {'Accuracy': 0.7953105196451205, 'F1-Score': 0.775255440190918}}\n"
          ]
        }
      ],
      "source": [
        "fr_more_restricted_total_ap_results, fr_more_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=fr_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='FR',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J3ml3zu3Qpf",
        "outputId": "2859764f-19f8-4227-a7d0-c51a297e71e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column to Predict/Classify: Total_AP. Feature Set: more_restricted\n",
            "Best Model Info without feature selection: {'model_name': 'Random Forest', 'params': array([0.78940792, 0.80663052, 0.80912303, 0.79466036, 0.8034326 ]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8231939163498099, 'F1-Score': 0.8142354277268302}}\n",
            "Best Model Info without feature selection using PCA: {'model_name': 'Random Forest', 'params': array([0.78940792, 0.80663052, 0.80912303, 0.79466036, 0.8034326 ]), 'dataset': 'combined_df_Total_AP', 'test_scores': {'Accuracy': 0.8231939163498099, 'F1-Score': 0.8142354277268302}}\n",
            "Best Model Info using MI Feature Selection method: {'model_name': 'Neural Network', 'params': array([0.79463296, 0.77881104, 0.78843844, 0.7956285 , 0.79414547]), 'dataset': 'predict_total_ap_MI_more_restricted', 'test_scores': {'Accuracy': 0.8326996197718631, 'F1-Score': 0.833185216798152}}\n",
            "Best Model Info using RF Feature Selection method: {'model_name': 'Neural Network', 'params': array([0.79463296, 0.77881104, 0.78843844, 0.7956285 , 0.79414547]), 'dataset': 'predict_total_ap_RF_more_restricted', 'test_scores': {'Accuracy': 0.8326996197718631, 'F1-Score': 0.833185216798152}}\n",
            "Best Model Info using DT Feature Selection method: {'model_name': 'Neural Network', 'params': array([0.79463296, 0.77881104, 0.78843844, 0.7956285 , 0.79414547]), 'dataset': 'predict_total_ap_DT_more_restricted', 'test_scores': {'Accuracy': 0.8326996197718631, 'F1-Score': 0.833185216798152}}\n",
            "Best Model Info using Lasso Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.76876292, 0.744325  , 0.74754687, 0.73768539, 0.74258786]), 'dataset': 'predict_total_ap_Lasso_more_restricted_regression', 'test_scores': {'Accuracy': 0.7807351077313055, 'F1-Score': 0.7683540633024609}}\n",
            "Best Model Info using FR Feature Selection method: {'model_name': 'Neural Network', 'params': array([0.76015871, 0.74790107, 0.75699723, 0.75153031, 0.73970128]), 'dataset': 'predict_total_ap_FR_more_restricted_regression', 'test_scores': {'Accuracy': 0.7953105196451205, 'F1-Score': 0.775255440190918}}\n"
          ]
        }
      ],
      "source": [
        "print(\"Column to Predict/Classify: Total_AP. Feature Set: more_restricted\")\n",
        "print(\"Best Model Info without feature selection removing columns with missing data:\", original_total_ap_best_model)\n",
        "print(\"Best Model Info without feature selection using PCA:\", original_total_ap_best_model_pca)\n",
        "print(\"Best Model Info without feature selection using zero imputation:\", original_total_ap_best_model_zero)\n",
        "print(\"Best Model Info using MI Feature Selection method:\", mi_more_restricted_total_ap_best_model)\n",
        "print(\"Best Model Info using RF Feature Selection method:\", rf_more_restricted_total_ap_best_model)\n",
        "print(\"Best Model Info using DT Feature Selection method:\", dt_more_restricted_total_ap_best_model)\n",
        "print(\"Best Model Info using Lasso Feature Selection method:\", lasso_more_restricted_total_ap_best_model)\n",
        "print(\"Best Model Info using FR Feature Selection method:\", fr_more_restricted_total_ap_best_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILmyOeZWDLDQ"
      },
      "source": [
        "#### Restricted - L4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQOHwJOJuu-T",
        "outputId": "989d0477-19b1-42e3-d7d9-f5778cf051f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: restricted\n",
            "No datasets found for the given criteria: restricted, MI, Total_AP\n"
          ]
        }
      ],
      "source": [
        "mi_restricted_total_ap_results, mi_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='MI',\n",
        "    exclusion_type='restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFxQ55nMDLDQ",
        "outputId": "be957ff6-3a81-4003-ef6c-821cb9f73037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_RF_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-245.25475019 -275.39960014 -270.79829502 -246.13460971 -257.7444634 ]\n",
            "Test set scores for Linear Regression: {'MSE': 238.22160089177663, 'R2': 0.6879897961137715, 'MAE': 10.257488238257762}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-283.77648167 -358.7143409  -293.86290119 -298.52507102 -317.26647689]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 290.2103946850291, 'R2': 0.6198975908288237, 'MAE': 8.977355863437477}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-197.06990989 -216.06307053 -209.14376449 -183.90106605 -204.64034948]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 165.9080372016014, 'R2': 0.7827023228798118, 'MAE': 7.784995202013449}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-232.61585387 -244.9654176  -249.36308877 -225.85221999 -241.40325032]\n",
            "Test set scores for Support Vector Machine: {'MSE': 209.34179930167085, 'R2': 0.7258150510385604, 'MAE': 8.521442770740618}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-229.78798455 -232.83447251 -231.55268605 -226.06855008 -251.76702028]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 194.44089651069552, 'R2': 0.7453314748242152, 'MAE': 8.017834510229948}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-179.36619149 -190.3467991  -194.67118193 -173.13960855 -199.42001603]\n",
            "Test set scores for Neural Network: {'MSE': 159.25093767714617, 'R2': 0.7914214439508678, 'MAE': 8.101777443613054}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.72711296 0.74279524 0.72887365 0.72209938 0.72874572]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7667934093789607, 'F1-Score': 0.7495521083108555}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.75627538 0.76286993 0.78389908 0.78632123 0.76118634]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8155893536121673, 'F1-Score': 0.8105692290920461}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.78545542 0.78098583 0.81147609 0.79213062 0.79083029]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8288973384030418, 'F1-Score': 0.8200227508227805}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.76998465 0.77315563 0.78933143 0.78513639 0.78008295]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8187579214195184, 'F1-Score': 0.8073068805052824}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.76726367 0.76513273 0.79171324 0.76756619 0.76229877]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8010139416983524, 'F1-Score': 0.7901300329959415}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.78673139 0.77948068 0.77725191 0.77967683 0.79572989]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8200253485424588, 'F1-Score': 0.8179363850278629}\n",
            "Best model saved to best_models/predict_total_ap_RF_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_ap_RF_restricted'. Total time: 572.26 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.78545542, 0.78098583, 0.81147609, 0.79213062, 0.79083029]), 'dataset': 'predict_total_ap_RF_restricted', 'test_scores': {'Accuracy': 0.8288973384030418, 'F1-Score': 0.8200227508227805}}\n"
          ]
        }
      ],
      "source": [
        "rf_restricted_total_ap_results, rf_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxCPBCprDLDQ",
        "outputId": "b3e95a57-0c6b-46e3-bf93-5d0699565f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_DT_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-245.25475019 -275.39960014 -270.79829502 -246.13460971 -257.7444634 ]\n",
            "Test set scores for Linear Regression: {'MSE': 238.22160089177663, 'R2': 0.6879897961137715, 'MAE': 10.257488238257762}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-283.77648167 -358.7143409  -293.86290119 -298.52507102 -317.26647689]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 290.2103946850291, 'R2': 0.6198975908288237, 'MAE': 8.977355863437477}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-197.06990989 -216.06307053 -209.14376449 -183.90106605 -204.64034948]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 165.9080372016014, 'R2': 0.7827023228798118, 'MAE': 7.784995202013449}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-232.61585387 -244.9654176  -249.36308877 -225.85221999 -241.40325032]\n",
            "Test set scores for Support Vector Machine: {'MSE': 209.34179930167085, 'R2': 0.7258150510385604, 'MAE': 8.521442770740618}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-229.78798455 -232.83447251 -231.55268605 -226.06855008 -251.76702028]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 194.44089651069552, 'R2': 0.7453314748242152, 'MAE': 8.017834510229948}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-179.36619149 -190.3467991  -194.67118193 -173.13960855 -199.42001603]\n",
            "Test set scores for Neural Network: {'MSE': 159.25093767714617, 'R2': 0.7914214439508678, 'MAE': 8.101777443613054}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.72711296 0.74279524 0.72887365 0.72209938 0.72874572]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7667934093789607, 'F1-Score': 0.7495521083108555}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.75627538 0.76286993 0.78389908 0.78632123 0.76118634]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8155893536121673, 'F1-Score': 0.8105692290920461}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.78545542 0.78098583 0.81147609 0.79213062 0.79083029]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.8288973384030418, 'F1-Score': 0.8200227508227805}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.76998465 0.77315563 0.78933143 0.78513639 0.78008295]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8187579214195184, 'F1-Score': 0.8073068805052824}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.76726367 0.76513273 0.79171324 0.76756619 0.76229877]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8010139416983524, 'F1-Score': 0.7901300329959415}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.78673139 0.77948068 0.77725191 0.77967683 0.79572989]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8200253485424588, 'F1-Score': 0.8179363850278629}\n",
            "Best model saved to best_models/predict_total_ap_DT_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_ap_DT_restricted'. Total time: 573.91 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.78545542, 0.78098583, 0.81147609, 0.79213062, 0.79083029]), 'dataset': 'predict_total_ap_DT_restricted', 'test_scores': {'Accuracy': 0.8288973384030418, 'F1-Score': 0.8200227508227805}}\n"
          ]
        }
      ],
      "source": [
        "# Example for analyzing 'Total_AP' with 'DT' feature set and 'restricted' exclusion type\n",
        "dt_restricted_total_ap_results, dt_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=dt_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='DT',\n",
        "    exclusion_type='restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buGmZHxw29yW",
        "outputId": "0e8d8122-995c-4092-8808-9454d173580a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column to Predict/Classify: Total_AP. Feature Set: restricted\n",
            "Best Model Info using MI Feature Selection method: None\n",
            "Best Model Info using RF Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.78545542, 0.78098583, 0.81147609, 0.79213062, 0.79083029]), 'dataset': 'predict_total_ap_RF_restricted', 'test_scores': {'Accuracy': 0.8288973384030418, 'F1-Score': 0.8200227508227805}}\n",
            "Best Model Info using DT Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.78545542, 0.78098583, 0.81147609, 0.79213062, 0.79083029]), 'dataset': 'predict_total_ap_DT_restricted', 'test_scores': {'Accuracy': 0.8288973384030418, 'F1-Score': 0.8200227508227805}}\n"
          ]
        }
      ],
      "source": [
        "print(\"Column to Predict/Classify: Total_AP. Feature Set: restricted\")\n",
        "print(\"Best Model Info using MI Feature Selection method:\", mi_restricted_total_ap_best_model)\n",
        "print(\"Best Model Info using RF Feature Selection method:\", rf_restricted_total_ap_best_model)\n",
        "print(\"Best Model Info using DT Feature Selection method:\", dt_restricted_total_ap_best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEBiOHR6DLDR"
      },
      "source": [
        "#### Less Restricted - L4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZXXzedgDLDR",
        "outputId": "fadd7a2b-afb8-4c18-acd3-d32f2ed116b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: less_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_MI_less_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-74.23838131 -71.39632698 -65.34522975 -63.81411929 -67.51983334]\n",
            "Test set scores for Linear Regression: {'MSE': 58.4429924942444, 'R2': 0.9234544225393962, 'MAE': 3.479253771312736}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-82.12429261 -80.87805396 -84.08242259 -95.49206158 -94.46355866]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 72.36375919641264, 'R2': 0.9052217297830021, 'MAE': 2.966294555058158}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-52.63821555 -43.66917861 -42.86965633 -41.57614534 -45.05951159]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 36.78844783056077, 'R2': 0.9518164135187483, 'MAE': 2.178219410773491}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-77.89001852 -78.79681864 -71.06053216 -70.38063782 -76.13459799]\n",
            "Test set scores for Support Vector Machine: {'MSE': 58.40526684617892, 'R2': 0.9235038336217691, 'MAE': 1.4410999730868925}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-47.1232304  -46.46517815 -38.50408875 -44.71087163 -41.41280507]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 36.63183776932826, 'R2': 0.9520215331928369, 'MAE': 1.969835234474018}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-73.10723714 -88.19316155 -46.8785478  -75.75926373 -75.40249711]\n",
            "Test set scores for Neural Network: {'MSE': 67.30624169049406, 'R2': 0.9118458019169842, 'MAE': 1.9549158117424816}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.93382242 0.93891263 0.95019984 0.94835064 0.94193342]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9524714828897338, 'F1-Score': 0.9516757810789194}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.90706628 0.91152312 0.92004215 0.91018881 0.90662131]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9188846641318125, 'F1-Score': 0.9184147855405428}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.93280841 0.92914328 0.94508469 0.93465372 0.93803152]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9315589353612167, 'F1-Score': 0.9314531755288418}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.96614891 0.96952051 0.97203003 0.96545263 0.96774045]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9778200253485425, 'F1-Score': 0.9776532724743268}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.9405811  0.94774223 0.94473512 0.93549762 0.95031818]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9435994930291508, 'F1-Score': 0.9429608151572715}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.92256004 0.9309693  0.94914357 0.93607632 0.93838706]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9239543726235742, 'F1-Score': 0.9238931530447043}\n",
            "Best model saved to best_models/predict_total_ap_MI_less_restricted_Support Vector Machine.pkl\n",
            "Processing complete for 'predict_total_ap_MI_less_restricted'. Total time: 864.36 seconds\n",
            "Best model: {'model_name': 'Support Vector Machine', 'params': array([0.96614891, 0.96952051, 0.97203003, 0.96545263, 0.96774045]), 'dataset': 'predict_total_ap_MI_less_restricted', 'test_scores': {'Accuracy': 0.9778200253485425, 'F1-Score': 0.9776532724743268}}\n"
          ]
        }
      ],
      "source": [
        "mi_less_restricted_total_ap_results, mi_less_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='MI',\n",
        "    exclusion_type='less_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyq-3dOMDLDR",
        "outputId": "f1bcd4ec-0594-4750-e5d0-20354717e1bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: less_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_RF_less_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-74.30402834 -71.65318537 -65.78548918 -63.46329178 -67.75895145]\n",
            "Test set scores for Linear Regression: {'MSE': 58.33658488383781, 'R2': 0.923593789666868, 'MAE': 3.4834273917835987}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [ -80.27235278  -82.19792728  -90.29874588  -96.23532624 -101.36688671]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 65.88277060325674, 'R2': 0.9137101899594305, 'MAE': 2.9149637565790707}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-53.50747303 -43.52616263 -42.70516592 -40.63833552 -45.03299733]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 36.98838959228448, 'R2': 0.9515545402477802, 'MAE': 2.20003816869491}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-77.93382219 -78.74499706 -71.13187953 -70.35415961 -76.16059636]\n",
            "Test set scores for Support Vector Machine: {'MSE': 58.41584990536649, 'R2': 0.9234899724838938, 'MAE': 1.441986293594696}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-47.77203484 -47.17909739 -40.20770206 -45.58250396 -41.41762282]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 37.633865652724964, 'R2': 0.950709129435042, 'MAE': 2.0220532319391635}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-64.05894744 -64.93631404 -39.89624409 -57.21362165 -56.94031282]\n",
            "Test set scores for Neural Network: {'MSE': 54.22739184006289, 'R2': 0.9289757959777841, 'MAE': 2.350002151225151}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.93699781 0.93737133 0.94866492 0.94264311 0.94189754]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9531051964512041, 'F1-Score': 0.9522961685900924}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.90952366 0.90861298 0.92170981 0.90860005 0.90822911]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9182509505703422, 'F1-Score': 0.917706950929973}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.93605859 0.93062378 0.94033169 0.93379783 0.93553172]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9359949302915083, 'F1-Score': 0.9359213706171255}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.96611408 0.96952051 0.97444974 0.9653758  0.96854845]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9790874524714829, 'F1-Score': 0.9789181162978986}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.93739362 0.94765524 0.95367199 0.93562669 0.94367809]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.944233206590621, 'F1-Score': 0.9433917700484451}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.92568559 0.94125202 0.91999403 0.93307184 0.94382617]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9378960709759189, 'F1-Score': 0.9382793155240176}\n",
            "Best model saved to best_models/predict_total_ap_RF_less_restricted_Support Vector Machine.pkl\n",
            "Processing complete for 'predict_total_ap_RF_less_restricted'. Total time: 684.71 seconds\n",
            "Best model: {'model_name': 'Support Vector Machine', 'params': array([0.96611408, 0.96952051, 0.97444974, 0.9653758 , 0.96854845]), 'dataset': 'predict_total_ap_RF_less_restricted', 'test_scores': {'Accuracy': 0.9790874524714829, 'F1-Score': 0.9789181162978986}}\n"
          ]
        }
      ],
      "source": [
        "rf_less_restricted_total_ap_results, rf_less_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='less_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s9StpMDDLDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ab0cda-7eb0-4577-b7a5-21789016489f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_AP with exclusion type: less_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_ap_DT_less_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-75.46884559 -73.90988407 -69.63466354 -67.63512739 -70.57189598]\n",
            "Test set scores for Linear Regression: {'MSE': 61.414494550431634, 'R2': 0.9195625044306748, 'MAE': 3.085852589096458}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-83.40932824 -68.64765021 -87.79557614 -90.74958937 -84.54279786]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 64.94297331715062, 'R2': 0.9149410873329349, 'MAE': 2.9231995079400583}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-53.60462354 -43.48152608 -43.26933153 -40.18149287 -44.68856339]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 37.05137639928146, 'R2': 0.9514720434195342, 'MAE': 2.2007897745258176}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-77.92807322 -78.71946504 -71.12404884 -70.35903799 -76.15315901]\n",
            "Test set scores for Support Vector Machine: {'MSE': 58.41134089879139, 'R2': 0.9234958781450746, 'MAE': 1.4421428685422037}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-47.79246239 -47.0725574  -40.23787639 -44.50659271 -41.41011094]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 37.60063371356147, 'R2': 0.9507526548923269, 'MAE': 2.0188846641318126}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-62.76939725 -79.21771143 -60.77207707 -75.09154874 -57.98724818]\n",
            "Test set scores for Neural Network: {'MSE': 61.306512738926585, 'R2': 0.9197039333644809, 'MAE': 1.7844521337289252}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.92656923 0.93081327 0.94216603 0.92958078 0.93873124]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9461343472750317, 'F1-Score': 0.9450082013062114}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.91553387 0.91277499 0.9250382  0.90646545 0.90808241]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9195183776932826, 'F1-Score': 0.9191116581501634}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.92952895 0.93055544 0.94524705 0.93695099 0.94197905]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9334600760456274, 'F1-Score': 0.9333457954078442}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.96611408 0.96952051 0.97444974 0.9653758  0.96774045]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9790874524714829, 'F1-Score': 0.9789181162978986}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.93815812 0.94688297 0.95203663 0.93805351 0.94450121]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9416983523447402, 'F1-Score': 0.9408339290045888}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.91390847 0.90189933 0.91445983 0.92050481 0.92636426]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9017743979721166, 'F1-Score': 0.9041195185565036}\n",
            "Best model saved to best_models/predict_total_ap_DT_less_restricted_Support Vector Machine.pkl\n",
            "Processing complete for 'predict_total_ap_DT_less_restricted'. Total time: 775.20 seconds\n",
            "Best model: {'model_name': 'Support Vector Machine', 'params': array([0.96611408, 0.96952051, 0.97444974, 0.9653758 , 0.96774045]), 'dataset': 'predict_total_ap_DT_less_restricted', 'test_scores': {'Accuracy': 0.9790874524714829, 'F1-Score': 0.9789181162978986}}\n"
          ]
        }
      ],
      "source": [
        "dt_less_restricted_total_ap_results, dt_less_restricted_total_ap_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=dt_total_ap_datasets,\n",
        "    target_column='Total_AP',\n",
        "    feature_set_name='DT',\n",
        "    exclusion_type='less_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8DscEru2rPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0f3998-0620-4587-bafb-e95026569ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column to Predict/Classify: Total_AP. Feature Set: less_restricted\n",
            "Best Model Info using MI Feature Selection method: {'model_name': 'Support Vector Machine', 'params': array([0.96614891, 0.96952051, 0.97203003, 0.96545263, 0.96774045]), 'dataset': 'predict_total_ap_MI_less_restricted', 'test_scores': {'Accuracy': 0.9778200253485425, 'F1-Score': 0.9776532724743268}}\n",
            "Best Model Info using RF Feature Selection method: {'model_name': 'Support Vector Machine', 'params': array([0.96611408, 0.96952051, 0.97444974, 0.9653758 , 0.96854845]), 'dataset': 'predict_total_ap_RF_less_restricted', 'test_scores': {'Accuracy': 0.9790874524714829, 'F1-Score': 0.9789181162978986}}\n",
            "Best Model Info using DT Feature Selection method: {'model_name': 'Support Vector Machine', 'params': array([0.96611408, 0.96952051, 0.97444974, 0.9653758 , 0.96774045]), 'dataset': 'predict_total_ap_DT_less_restricted', 'test_scores': {'Accuracy': 0.9790874524714829, 'F1-Score': 0.9789181162978986}}\n"
          ]
        }
      ],
      "source": [
        "print(\"Column to Predict/Classify: Total_AP. Feature Set: less_restricted\")\n",
        "print(\"Best Model Info using MI Feature Selection method:\", mi_less_restricted_total_ap_best_model)\n",
        "print(\"Best Model Info using RF Feature Selection method:\", rf_less_restricted_total_ap_best_model)\n",
        "print(\"Best Model Info using DT Feature Selection method:\", dt_less_restricted_total_ap_best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgjhx2s_DLDR"
      },
      "source": [
        "### 4.3.2 Predicting Total_PP feature"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CPU Tests"
      ],
      "metadata": {
        "id": "AoAa9ztTlFHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_PP' using the original dataset without feature selection\n",
        "original_total_pp_results, original_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_PP',\n",
        "    group_col='Year',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWA8ZMBDlGwK",
        "outputId": "6d23905e-3683-4174-dbc8-0c9ec058efa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_PP\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-13.92239594 -14.53165422 -13.00202569 -13.96216371 -14.29714277]\n",
            "Test set scores for Linear Regression: {'MSE': 12.003530087648322, 'R2': 0.6113765452320685, 'MAE': 2.6125377517669355}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-17.78507223 -14.59411877 -17.33309909 -14.23705811 -16.71075504]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 15.907688412916242, 'R2': 0.48497643749310815, 'MAE': 2.5611386348080667}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-10.51686467 -10.69583911 -10.36967894  -9.63550355 -11.49259873]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 9.978363569450838, 'R2': 0.6769428580613339, 'MAE': 2.181653611568477}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-29.51895872 -33.03927216 -33.11895658 -30.80033444 -28.88834129]\n",
            "Test set scores for Support Vector Machine: {'MSE': 31.154199646461624, 'R2': -0.008641008830909902, 'MAE': 3.5849422662053754}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-10.84387674 -11.58711846 -10.41687959 -11.12479382 -11.0684854 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 8.888414681461938, 'R2': 0.7122307858024054, 'MAE': 2.0809342748506245}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-14.21804938 -15.84101529 -13.49797366 -15.25935845 -14.86222466]\n",
            "Test set scores for Neural Network: {'MSE': 12.1522454848854, 'R2': 0.6065617706591344, 'MAE': 2.5997300425667857}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.46584241 0.46688397 0.46715531 0.46421286 0.46344593]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.5944233206590621, 'F1-Score': 0.45147779002707333}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.69572644 0.71092896 0.69679615 0.69631817 0.71123883]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6863117870722434, 'F1-Score': 0.6831522546206292}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.69801915 0.72244605 0.71804355 0.66651445 0.70593546]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.6926489226869454, 'F1-Score': 0.6912534883098806}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.1}\n",
            "Cross-validation scores for Support Vector Machine: [0.40833481 0.40833481 0.40886518 0.40886518 0.40886518]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.5475285171102662, 'F1-Score': 0.3874403266038247}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.70101058 0.70503885 0.68211783 0.67938974 0.69340035]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7002534854245881, 'F1-Score': 0.7006237181946453}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.52239214 0.46763818 0.46645519 0.26594906 0.51961844]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.4828897338403042, 'F1-Score': 0.3807461051118721}\n",
            "Best regression model saved to best_models/combined_df_Total_PP_regression_k-Nearest Neighbors.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_PP_classification_k-Nearest Neighbors.pkl\n",
            "Processing complete for datasets. Total time: 445.29 seconds\n",
            "Best regression model: {'model_name': 'k-Nearest Neighbors', 'params': array([-10.84387674, -11.58711846, -10.41687959, -11.12479382,\n",
            "       -11.0684854 ]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'MSE': 8.888414681461938, 'R2': 0.7122307858024054, 'MAE': 2.0809342748506245}}\n",
            "Best classification model: {'model_name': 'k-Nearest Neighbors', 'params': array([0.70101058, 0.70503885, 0.68211783, 0.67938974, 0.69340035]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'Accuracy': 0.7002534854245881, 'F1-Score': 0.7006237181946453}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_PP' using the original dataset without feature selection\n",
        "original_total_pp_results_zero, original_total_pp_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_PP',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDBhtdVnlQCL",
        "outputId": "2396616a-6a4b-4d10-a93d-4cb3929bffe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_PP\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-8.73184816 -9.90397146 -8.47378366 -8.8973584  -9.43287418]\n",
            "Test set scores for Linear Regression: {'MSE': 2.2104109587554017e+24, 'R2': -7.156374308024205e+22, 'MAE': 37426827291.835464}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-17.63760165 -18.96464495 -17.01712801 -16.00094746 -18.02787075]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 15.976275534253743, 'R2': 0.4827558770536259, 'MAE': 2.68522014002052}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [ -9.87985039 -10.5549      -8.76449991  -9.17858814  -9.97204423]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 8.270981180408597, 'R2': 0.7322206669887505, 'MAE': 2.033851990501459}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-8.52472618 -9.83390546 -8.13184273 -8.6658229  -9.33547848]\n",
            "Test set scores for Support Vector Machine: {'MSE': 7.376290619104494, 'R2': 0.7611869572669782, 'MAE': 1.8429611397489323}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [ -9.3824228  -10.79554672  -9.20181765  -9.47288075 -10.52718393]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 8.030547580248829, 'R2': 0.7400048884347841, 'MAE': 1.9576317218902768}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [ -9.56377697 -11.55727192  -9.46934798  -9.75341629 -10.04564572]\n",
            "Test set scores for Neural Network: {'MSE': 8.856100384981056, 'R2': 0.7132769858323214, 'MAE': 2.015625802539921}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.74293385 0.75094367 0.73136235 0.73957211 0.72920848]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7325728770595691, 'F1-Score': 0.730309328872748}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.70358456 0.72254042 0.71191002 0.69156289 0.70183201]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6983523447401775, 'F1-Score': 0.691024547257806}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.73550313 0.7392107  0.72379111 0.72268714 0.72696505]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.7192648922686945, 'F1-Score': 0.7172996726660293}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.73518409 0.73606352 0.71461074 0.72915712 0.7141626 ]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.7122940430925222, 'F1-Score': 0.7037938457186363}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.7204582  0.72602357 0.7061433  0.71278454 0.71531409]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7243346007604563, 'F1-Score': 0.7236865606164904}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.7193348  0.72952543 0.72120549 0.71829591 0.71992522]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.7129277566539924, 'F1-Score': 0.704115853506285}\n",
            "Best regression model saved to best_models/combined_df_Total_PP_regression_Support Vector Machine.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_PP_classification_Logistic Regression.pkl\n",
            "Processing complete for datasets. Total time: 904.34 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-8.52472618, -9.83390546, -8.13184273, -8.6658229 , -9.33547848]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'MSE': 7.376290619104494, 'R2': 0.7611869572669782, 'MAE': 1.8429611397489323}}\n",
            "Best classification model: {'model_name': 'Logistic Regression', 'params': array([0.74293385, 0.75094367, 0.73136235, 0.73957211, 0.72920848]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'Accuracy': 0.7325728770595691, 'F1-Score': 0.730309328872748}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mi_more_restricted_total_pp_results, mi_more_restricted_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='MI',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyCRgz8xlnrr",
        "outputId": "64eeb499-4955-4630-d90a-181e1d2ade5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_MI_more_restricted\n",
            "Column 'Year' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-8.32258178 -9.39849158 -7.75409918 -8.02210934 -8.55002518]\n",
            "Test set scores for Linear Regression: {'MSE': 6.878656851411008, 'R2': 0.7772982305839189, 'MAE': 1.8742662950197957}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-15.78453628 -17.79985029 -14.3555728  -14.23971829 -17.50493258]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 14.26573804765775, 'R2': 0.5381358346741782, 'MAE': 2.5529858632077294}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [ -8.83721118 -10.27119779  -8.12361072  -8.50603947  -8.68495617]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 6.762339766401834, 'R2': 0.7810640850529568, 'MAE': 1.8826467219191911}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-8.10312407 -9.30535908 -7.50700826 -7.94467401 -8.4293858 ]\n",
            "Test set scores for Support Vector Machine: {'MSE': 6.554700204960557, 'R2': 0.7877865744477092, 'MAE': 1.751801837480141}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [ -8.65086367 -10.6158644   -8.66826223  -8.85434522  -9.93308322]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 7.041695765758774, 'R2': 0.7720197212043624, 'MAE': 1.8817671555314142}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-8.75256311 -9.58911034 -8.99734441 -8.57640815 -9.21508415]\n",
            "Test set scores for Neural Network: {'MSE': 6.840122016136209, 'R2': 0.7785458253084755, 'MAE': 1.8754614363174018}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.74395299 0.75407609 0.74038111 0.74047585 0.73976851]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7503168567807351, 'F1-Score': 0.7474348106921135}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.69150556 0.69593121 0.70544082 0.69294407 0.69540025]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6894803548795945, 'F1-Score': 0.6655668232584444}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.7390467  0.73341617 0.74179314 0.71375725 0.7400668 ]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.7496831432192649, 'F1-Score': 0.7476915477258106}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.74458828 0.75610131 0.73249667 0.74419838 0.72951157]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.738276299112801, 'F1-Score': 0.7327810891982047}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.72740023 0.7298103  0.71861732 0.71680295 0.73393653]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7249683143219265, 'F1-Score': 0.7250315307038635}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.72493821 0.75126658 0.73999527 0.74049624 0.74486159]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.756020278833967, 'F1-Score': 0.7537391220414255}\n",
            "Best regression model saved to best_models/predict_total_pp_MI_more_restricted_regression_Support Vector Machine.pkl\n",
            "Best classification model saved to best_models/predict_total_pp_MI_more_restricted_classification_Neural Network.pkl\n",
            "Processing complete for datasets. Total time: 849.21 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-8.10312407, -9.30535908, -7.50700826, -7.94467401, -8.4293858 ]), 'dataset': 'predict_total_pp_MI_more_restricted', 'test_scores': {'MSE': 6.554700204960557, 'R2': 0.7877865744477092, 'MAE': 1.751801837480141}}\n",
            "Best classification model: {'model_name': 'Neural Network', 'params': array([0.72493821, 0.75126658, 0.73999527, 0.74049624, 0.74486159]), 'dataset': 'predict_total_pp_MI_more_restricted', 'test_scores': {'Accuracy': 0.756020278833967, 'F1-Score': 0.7537391220414255}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_more_restricted_total_pp_results_zero, dt_more_restricted_total_pp_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=dt_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='DT',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFUh95JloSjP",
        "outputId": "4fcc4dd2-bbfd-41bc-cc3a-375ac0ef5dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_DT_more_restricted\n",
            "Column 'Year' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-8.22764161 -9.54050079 -8.03762141 -8.12853289 -8.62530522]\n",
            "Test set scores for Linear Regression: {'MSE': 7.176215143676656, 'R2': 0.7676645535996696, 'MAE': 1.9029376413391168}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-16.0754799  -16.70788001 -15.39934085 -14.98348894 -15.49162889]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 14.691581925803924, 'R2': 0.5243488138637558, 'MAE': 2.625281673333004}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [ -8.7226507  -10.13185355  -7.93172893  -8.33032682  -8.70636774]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 6.736687915856223, 'R2': 0.7818945833070066, 'MAE': 1.8588515393940495}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-7.9746653  -9.26021247 -7.54412351 -7.88059244 -8.38661434]\n",
            "Test set scores for Support Vector Machine: {'MSE': 6.574100440698319, 'R2': 0.7871584769979865, 'MAE': 1.747107792448611}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [ -8.84568649 -10.45049849  -8.6114525   -8.72164365  -9.97163556]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 7.269664519800315, 'R2': 0.7646390586719355, 'MAE': 1.9050334962882494}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-8.41106098 -9.82616599 -8.85443302 -8.58357753 -8.65841881]\n",
            "Test set scores for Neural Network: {'MSE': 6.956723897660161, 'R2': 0.774770750334747, 'MAE': 1.8898492823872448}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.73965627 0.7577613  0.75286437 0.74230732 0.73899779]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7522179974651457, 'F1-Score': 0.7506890929547487}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.6984619  0.6851892  0.7030541  0.69496059 0.69430469]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.7135614702154626, 'F1-Score': 0.7037626332631018}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.74694588 0.74717423 0.73562712 0.73403596 0.74532181]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.7427122940430925, 'F1-Score': 0.7402568302276963}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.75184039 0.75397462 0.73695782 0.73675249 0.73181982]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.7452471482889734, 'F1-Score': 0.7401022948772596}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.72085542 0.72247411 0.71342338 0.71839929 0.73301591]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7243346007604563, 'F1-Score': 0.7243813349933544}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.72478625 0.74150083 0.73410414 0.73219564 0.74598732]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.752851711026616, 'F1-Score': 0.7513457387861019}\n",
            "Best regression model saved to best_models/predict_total_pp_DT_more_restricted_regression_Support Vector Machine.pkl\n",
            "Best classification model saved to best_models/predict_total_pp_DT_more_restricted_classification_Neural Network.pkl\n",
            "Processing complete for datasets. Total time: 866.15 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-7.9746653 , -9.26021247, -7.54412351, -7.88059244, -8.38661434]), 'dataset': 'predict_total_pp_DT_more_restricted', 'test_scores': {'MSE': 6.574100440698319, 'R2': 0.7871584769979865, 'MAE': 1.747107792448611}}\n",
            "Best classification model: {'model_name': 'Neural Network', 'params': array([0.72478625, 0.74150083, 0.73410414, 0.73219564, 0.74598732]), 'dataset': 'predict_total_pp_DT_more_restricted', 'test_scores': {'Accuracy': 0.752851711026616, 'F1-Score': 0.7513457387861019}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Several Imputations and Test - T4 GPU"
      ],
      "metadata": {
        "id": "axhr0gkhyqNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_PP' using the original dataset without feature selection\n",
        "original_total_pp_results, original_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_PP',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0b9MqHJQ3cj",
        "outputId": "678cf230-de89-4c2c-9c2c-3dd7dc066a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_PP\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-13.92239594 -14.53165422 -13.00202569 -13.96216371 -14.29714277]\n",
            "Test set scores for Linear Regression: {'MSE': 12.003530087648322, 'R2': 0.6113765452320685, 'MAE': 2.6125377517669355}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-17.78507223 -14.59411877 -17.33309909 -14.23705811 -16.71075504]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 15.907688412916242, 'R2': 0.48497643749310815, 'MAE': 2.5611386348080667}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-10.51686467 -10.69583911 -10.36967894  -9.63550355 -11.49259873]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 9.978363569450838, 'R2': 0.6769428580613339, 'MAE': 2.181653611568477}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-29.51895872 -33.03927216 -33.11895658 -30.80033444 -28.88834129]\n",
            "Test set scores for Support Vector Machine: {'MSE': 31.154199646461624, 'R2': -0.008641008830909902, 'MAE': 3.5849422662053754}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-10.84387674 -11.58711846 -10.41687959 -11.12479382 -11.0684854 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 8.888414681461938, 'R2': 0.7122307858024054, 'MAE': 2.0809342748506245}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-14.21804938 -15.84101529 -13.49797366 -15.25935845 -14.86222466]\n",
            "Test set scores for Neural Network: {'MSE': 12.1522454848854, 'R2': 0.6065617706591344, 'MAE': 2.5997300425667857}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.46452432 0.46802276 0.46570247 0.46858335 0.46858335 0.46569623\n",
            " 0.46848892 0.46165148 0.46111646 0.46420616]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.5944233206590621, 'F1-Score': 0.45147779002707333}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.68941422 0.70700629 0.72329433 0.7270369  0.71486879 0.70447742\n",
            " 0.70132898 0.69562714 0.72759251 0.68017323]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6863117870722434, 'F1-Score': 0.6831522546206292}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.71735138 0.70438815 0.72938723 0.72703356 0.69256109 0.70471332\n",
            " 0.70666664 0.67552561 0.70879252 0.68448804]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.7002534854245881, 'F1-Score': 0.6970985759819898}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.1}\n",
            "Cross-validation scores for Support Vector Machine: [0.40780548 0.40780548 0.40886518 0.40886518 0.40886518 0.40886518\n",
            " 0.40886518 0.40886518 0.40886518 0.40886518]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.5475285171102662, 'F1-Score': 0.3874403266038247}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.71489875 0.71309761 0.70052523 0.71806678 0.6781376  0.68025947\n",
            " 0.68874825 0.66950238 0.71765259 0.67639395]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7002534854245881, 'F1-Score': 0.7006237181946453}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.46201616 0.46802276 0.46570247 0.47149663 0.46719937 0.46570247\n",
            " 0.53577601 0.43721364 0.40886518 0.46562571]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.4828897338403042, 'F1-Score': 0.3807461051118721}\n",
            "Best regression model saved to best_models/combined_df_Total_PP_regression_k-Nearest Neighbors.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_PP_classification_k-Nearest Neighbors.pkl\n",
            "Processing complete for datasets. Total time: 598.83 seconds\n",
            "Best regression model: {'model_name': 'k-Nearest Neighbors', 'params': array([-10.84387674, -11.58711846, -10.41687959, -11.12479382,\n",
            "       -11.0684854 ]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'MSE': 8.888414681461938, 'R2': 0.7122307858024054, 'MAE': 2.0809342748506245}}\n",
            "Best classification model: {'model_name': 'k-Nearest Neighbors', 'params': array([0.71489875, 0.71309761, 0.70052523, 0.71806678, 0.6781376 ,\n",
            "       0.68025947, 0.68874825, 0.66950238, 0.71765259, 0.67639395]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'Accuracy': 0.7002534854245881, 'F1-Score': 0.7006237181946453}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_PP' using the original dataset without feature selection\n",
        "original_total_pp_results_mean, original_total_pp_best_model_mean = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_PP',\n",
        "    imputation_strategy='mean',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJWy9sfPyqNf",
        "outputId": "6bdb0083-bd06-40d9-b32b-c1e87b17236e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_PP\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-10.35852318 -11.60801841 -10.14791043 -10.52713391 -10.85557483]\n",
            "Test set scores for Linear Regression: {'MSE': 2.4499675937647793e+24, 'R2': -7.931957210971414e+22, 'MAE': 39402762460.308716}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-18.5864338  -16.95110246 -17.41965441 -14.94560444 -17.40296474]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 15.988117337937615, 'R2': 0.48237248961470525, 'MAE': 2.6813197839338523}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [ -9.85744419 -10.46151     -8.58208462  -8.93622998  -9.84014038]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 8.119970337149972, 'R2': 0.7371097583798717, 'MAE': 2.0237139732924985}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [ -9.41698553 -10.61969108  -8.7930783   -9.57769528 -10.00256104]\n",
            "Test set scores for Support Vector Machine: {'MSE': 8.056684407601342, 'R2': 0.7391586886861903, 'MAE': 1.9665999928437183}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-10.07799699 -11.40229774 -10.14345548 -10.04532812 -10.6283515 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 8.577002664183544, 'R2': 0.7223129877152836, 'MAE': 2.017291327177259}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [ -9.79925156 -10.12237101  -9.69087329  -9.205716   -10.02098028]\n",
            "Test set scores for Neural Network: {'MSE': 8.434457244011767, 'R2': 0.7269280045681598, 'MAE': 2.0600213655926614}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.70710987 0.72558087 0.69942716 0.6805774  0.69140983]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7002534854245881, 'F1-Score': 0.6958306159758575}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.70914661 0.72030745 0.71713796 0.69471533 0.70488235]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.697084917617237, 'F1-Score': 0.6900699578057665}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.73322762 0.73093244 0.72493151 0.7219377  0.71918258]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.720532319391635, 'F1-Score': 0.7189692721059026}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.71687084 0.70614049 0.70604362 0.69648224 0.7005893 ]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.7084917617237009, 'F1-Score': 0.7016226808075259}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.69853813 0.70183744 0.70292818 0.70058814 0.71666746]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7110266159695817, 'F1-Score': 0.7108909051314195}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.71560664 0.71399708 0.70187647 0.71007642 0.69313064]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6989860583016476, 'F1-Score': 0.6921691734788894}\n",
            "Best regression model saved to best_models/combined_df_Total_PP_regression_Support Vector Machine.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_PP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 740.50 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([ -9.41698553, -10.61969108,  -8.7930783 ,  -9.57769528,\n",
            "       -10.00256104]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'MSE': 8.056684407601342, 'R2': 0.7391586886861903, 'MAE': 1.9665999928437183}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.73322762, 0.73093244, 0.72493151, 0.7219377 , 0.71918258]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'Accuracy': 0.720532319391635, 'F1-Score': 0.7189692721059026}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_PP' using the original dataset without feature selection\n",
        "original_total_pp_results_median, original_total_pp_best_model_median = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_PP',\n",
        "    imputation_strategy='median',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQjn5vPpyqNf",
        "outputId": "90b9d64e-be38-4ff4-e65c-783c3184f75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_PP\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [ -9.392372   -10.54915249  -9.11496976  -9.67853586 -10.16841876]\n",
            "Test set scores for Linear Regression: {'MSE': 2.2900913757241803e+24, 'R2': -7.414345743873896e+22, 'MAE': 38095432297.82949}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-16.27408687 -18.12695752 -20.53762949 -14.86089587 -19.1367335 ]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 15.909127672785537, 'R2': 0.48492984035554687, 'MAE': 2.696257317882793}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [ -9.8921477  -10.32563594  -9.05477964  -8.83258154  -9.62107503]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 8.120977752553365, 'R2': 0.7370771425367326, 'MAE': 2.019336533382222}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [ -9.21936825 -10.37646401  -8.62661408  -9.3523057   -9.83526057]\n",
            "Test set scores for Support Vector Machine: {'MSE': 7.814169436018841, 'R2': 0.7470102961217661, 'MAE': 1.9388820591466995}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-10.08318387 -11.44230614 -10.07171965 -10.03371713 -10.63064782]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 8.63011820697861, 'R2': 0.720593331448155, 'MAE': 2.0220894441426758}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [ -9.67686247 -10.47790028  -9.69702803  -9.3306085  -10.26398996]\n",
            "Test set scores for Neural Network: {'MSE': 8.794139994921782, 'R2': 0.7152830007852378, 'MAE': 2.089213083991875}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.71521318 0.71247417 0.69844662 0.69092717 0.70479202]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7002534854245881, 'F1-Score': 0.698985206594275}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.69322653 0.71869651 0.70716217 0.6923167  0.71350519]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6837769328263625, 'F1-Score': 0.678966165347545}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.73375738 0.73605843 0.72313438 0.72631041 0.72934836]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.7237008871989861, 'F1-Score': 0.7212412602031915}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.7156708  0.70147334 0.70063195 0.6921387  0.70339635]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6989860583016476, 'F1-Score': 0.6931684596348906}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.69549351 0.70604108 0.69654614 0.69186075 0.70170279]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7053231939163498, 'F1-Score': 0.7053879571602126}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.7127021  0.72236073 0.72218619 0.69880743 0.69754438]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.7084917617237009, 'F1-Score': 0.6981711009664171}\n",
            "Best regression model saved to best_models/combined_df_Total_PP_regression_Support Vector Machine.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_PP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 682.22 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([ -9.21936825, -10.37646401,  -8.62661408,  -9.3523057 ,\n",
            "        -9.83526057]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'MSE': 7.814169436018841, 'R2': 0.7470102961217661, 'MAE': 1.9388820591466995}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.73375738, 0.73605843, 0.72313438, 0.72631041, 0.72934836]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'Accuracy': 0.7237008871989861, 'F1-Score': 0.7212412602031915}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_PP' using the original dataset without feature selection\n",
        "original_total_pp_results_knn, original_total_pp_best_model_knn = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_PP',\n",
        "    imputation_strategy='knn',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyPYlS9GyqNf",
        "outputId": "c7987a41-f77b-4cf6-c85f-1699f664a0d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_PP\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-11.10637192 -12.4145497  -10.87063605 -11.38677867 -11.62684002]\n",
            "Test set scores for Linear Regression: {'MSE': 2.7735833270758973e+24, 'R2': -8.979687865023232e+22, 'MAE': 41924424485.32888}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-16.78093784 -16.96496693 -17.67198176 -16.02016642 -18.79559354]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 16.82990780507504, 'R2': 0.4551188802897047, 'MAE': 2.7307750920393503}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [ -9.30259575 -10.17555244  -8.83871221  -8.91639523  -9.92871881]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 7.965502546599957, 'R2': 0.7421107710799346, 'MAE': 1.980649937903554}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-10.34356559 -11.43345784  -9.95538244 -10.7549517  -10.78715253]\n",
            "Test set scores for Support Vector Machine: {'MSE': 9.023788266782542, 'R2': 0.7078479625806142, 'MAE': 2.090638778113888}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-10.89400036 -12.11503224 -10.7203176  -10.82098386 -11.02260746]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 8.90896510695533, 'R2': 0.7115654500808404, 'MAE': 2.0911642223429294}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-11.17394927 -11.1845339  -10.95097463 -10.75188996 -11.106354  ]\n",
            "Test set scores for Neural Network: {'MSE': 10.789868424691983, 'R2': 0.6506697685533332, 'MAE': 2.23060098410267}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.69640049 0.69203697 0.68058469 0.64853407 0.66557406]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.6799746514575412, 'F1-Score': 0.6723948005985125}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.69443844 0.70824771 0.71349775 0.68479145 0.71042884]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6774397972116604, 'F1-Score': 0.676204785371537}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.7141182  0.7217546  0.70451704 0.70321613 0.719269  ]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.7211660329531052, 'F1-Score': 0.717883362332519}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.6936509  0.69435134 0.69606058 0.66516202 0.68209795]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6856780735107731, 'F1-Score': 0.6721983510042482}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.67626524 0.68656623 0.67833299 0.6851095  0.69201053]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.6837769328263625, 'F1-Score': 0.6832695678571906}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.69978651 0.68791351 0.69582718 0.69438668 0.72015347]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6856780735107731, 'F1-Score': 0.6878247256379523}\n",
            "Best regression model saved to best_models/combined_df_Total_PP_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_PP_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 810.72 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([ -9.30259575, -10.17555244,  -8.83871221,  -8.91639523,\n",
            "        -9.92871881]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'MSE': 7.965502546599957, 'R2': 0.7421107710799346, 'MAE': 1.980649937903554}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.7141182 , 0.7217546 , 0.70451704, 0.70321613, 0.719269  ]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'Accuracy': 0.7211660329531052, 'F1-Score': 0.717883362332519}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_PP' using the original dataset without feature selection\n",
        "original_total_pp_results_zero, original_total_pp_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_PP',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzD4Z8GxyqNg",
        "outputId": "79c3b810-614c-46e7-a3c8-60d0f28b7cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_PP\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-8.73184816 -9.90397146 -8.47378366 -8.8973584  -9.43287418]\n",
            "Test set scores for Linear Regression: {'MSE': 2.2104109587554017e+24, 'R2': -7.156374308024205e+22, 'MAE': 37426827291.835464}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-17.63760165 -18.96464495 -17.01712801 -16.00094746 -18.02787075]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 15.976275534253743, 'R2': 0.4827558770536259, 'MAE': 2.68522014002052}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [ -9.87985039 -10.5549      -8.76449991  -9.17858814  -9.97204423]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 8.270981180408597, 'R2': 0.7322206669887505, 'MAE': 2.033851990501459}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-8.52472618 -9.83390546 -8.13184273 -8.6658229  -9.33547848]\n",
            "Test set scores for Support Vector Machine: {'MSE': 7.376290619104494, 'R2': 0.7611869572669782, 'MAE': 1.8429611397489323}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [ -9.3824228  -10.79554672  -9.20181765  -9.47288075 -10.52718393]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 8.030547580248829, 'R2': 0.7400048884347841, 'MAE': 1.9576317218902768}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [ -9.56377697 -11.55727192  -9.46934798  -9.75341629 -10.04564572]\n",
            "Test set scores for Neural Network: {'MSE': 8.856100384981056, 'R2': 0.7132769858323214, 'MAE': 2.015625802539921}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.74293385 0.75094367 0.73136235 0.73957211 0.72920848]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.7325728770595691, 'F1-Score': 0.730309328872748}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.70358456 0.72254042 0.71191002 0.69156289 0.70183201]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6983523447401775, 'F1-Score': 0.691024547257806}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.73550313 0.7392107  0.72379111 0.72268714 0.72696505]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.7192648922686945, 'F1-Score': 0.7172996726660293}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.73518409 0.73606352 0.71461074 0.72915712 0.7141626 ]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.7122940430925222, 'F1-Score': 0.7037938457186363}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.7204582  0.72602357 0.7061433  0.71278454 0.71531409]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7243346007604563, 'F1-Score': 0.7236865606164904}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.7193348  0.72952543 0.72120549 0.71829591 0.71992522]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.7129277566539924, 'F1-Score': 0.704115853506285}\n",
            "Best regression model saved to best_models/combined_df_Total_PP_regression_Support Vector Machine.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_PP_classification_Logistic Regression.pkl\n",
            "Processing complete for datasets. Total time: 725.68 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-8.52472618, -9.83390546, -8.13184273, -8.6658229 , -9.33547848]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'MSE': 7.376290619104494, 'R2': 0.7611869572669782, 'MAE': 1.8429611397489323}}\n",
            "Best classification model: {'model_name': 'Logistic Regression', 'params': array([0.74293385, 0.75094367, 0.73136235, 0.73957211, 0.72920848]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'Accuracy': 0.7325728770595691, 'F1-Score': 0.730309328872748}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7lZ2K_0JbhG"
      },
      "source": [
        "#### More Restricted - L4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_PP' using the original dataset without feature selection\n",
        "original_total_pp_results, original_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,  # Use the main dataset dictionary\n",
        "    target_column='Total_PP',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOKsgcjbSjNp",
        "outputId": "8809cc35-b56a-4ec3-fcdc-b7abfcf22b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_PP\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-13.92239594 -14.53165422 -13.00202569 -13.96216371 -14.29714277]\n",
            "Test set scores for Linear Regression: {'MSE': 12.003530087648329, 'R2': 0.6113765452320683, 'MAE': 2.6125377517669364}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-17.78507223 -14.59411877 -17.33309909 -14.23705811 -16.71075504]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 15.907688412916242, 'R2': 0.48497643749310815, 'MAE': 2.5611386348080667}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-10.51686467 -10.69583911 -10.36967894  -9.63550355 -11.49259873]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 9.978363569450838, 'R2': 0.6769428580613339, 'MAE': 2.181653611568477}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-29.51895872 -33.03927216 -33.11895658 -30.80033444 -28.88834129]\n",
            "Test set scores for Support Vector Machine: {'MSE': 31.154199646461624, 'R2': -0.008641008830909902, 'MAE': 3.5849422662053754}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-10.84387674 -11.58711846 -10.41687959 -11.12479382 -11.0684854 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 8.888414681461938, 'R2': 0.7122307858024054, 'MAE': 2.0809342748506245}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-14.21804938 -15.84101529 -13.49797366 -15.25935845 -14.86222466]\n",
            "Test set scores for Neural Network: {'MSE': 12.152245484885396, 'R2': 0.6065617706591346, 'MAE': 2.5997300425667866}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.46584241 0.46688397 0.46715531 0.46421286 0.46344593]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.5944233206590621, 'F1-Score': 0.45147779002707333}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.69572644 0.71092896 0.69679615 0.69631817 0.71123883]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6863117870722434, 'F1-Score': 0.6831522546206292}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.69801915 0.72244605 0.71804355 0.66651445 0.70593546]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.6926489226869454, 'F1-Score': 0.6912534883098806}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.1}\n",
            "Cross-validation scores for Support Vector Machine: [0.40833481 0.40833481 0.40886518 0.40886518 0.40886518]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.5475285171102662, 'F1-Score': 0.3874403266038247}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.70101058 0.70503885 0.68211783 0.67938974 0.69340035]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7002534854245881, 'F1-Score': 0.7006237181946453}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.46656717 0.46758957 0.46645519 0.2644259  0.50443252]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.4828897338403042, 'F1-Score': 0.3807461051118721}\n",
            "Best model saved to best_models/combined_df_Total_PP_k-Nearest Neighbors.pkl\n",
            "Processing complete for 'combined_df_Total_PP'. Total time: 305.18 seconds\n",
            "Best model: {'model_name': 'k-Nearest Neighbors', 'params': array([0.70101058, 0.70503885, 0.68211783, 0.67938974, 0.69340035]), 'dataset': 'combined_df_Total_PP', 'test_scores': {'Accuracy': 0.7002534854245881, 'F1-Score': 0.7006237181946453}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTi8gKXADLDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba2f7b9f-f8a3-4fc0-b8ae-22fb35d950be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_MI_more_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-13.49719437 -14.57016573 -12.27867205 -13.34393698 -13.85022322]\n",
            "Test set scores for Linear Regression: {'MSE': 11.80714458518987, 'R2': 0.6177346758715104, 'MAE': 2.5989238323213617}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-17.07747569 -20.56306126 -17.18126404 -17.36203008 -16.92458014]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 15.978281904775443, 'R2': 0.4826909192755394, 'MAE': 2.793854194336947}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-12.34316779 -14.33715205 -11.94624349 -11.77185138 -12.36259099]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 10.747320827314606, 'R2': 0.6520472795158696, 'MAE': 2.3937230606995774}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-12.54930618 -13.65084341 -11.7312359  -12.37640116 -12.49263174]\n",
            "Test set scores for Support Vector Machine: {'MSE': 10.88175711666978, 'R2': 0.6476948019668569, 'MAE': 2.3955363405338326}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-13.35285278 -14.8175061  -13.23702254 -13.53716162 -13.3995278 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 11.469090297716043, 'R2': 0.6286794416310745, 'MAE': 2.4377150099583558}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-12.17985466 -13.13956265 -11.19345148 -11.62816789 -12.16662674]\n",
            "Test set scores for Neural Network: {'MSE': 10.897353071244341, 'R2': 0.647189871025466, 'MAE': 2.4112848005862384}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.59198492 0.60589456 0.59234586 0.58953416 0.61034203]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.6191381495564005, 'F1-Score': 0.5882413969989239}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': None}\n",
            "Cross-validation scores for Decision Tree: [0.59855535 0.60183269 0.61554859 0.61390339 0.63608304]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6096324461343473, 'F1-Score': 0.6077200359812981}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.62135446 0.65083202 0.64466444 0.65527154 0.66386334]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.632446134347275, 'F1-Score': 0.6319746339361063}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.56835734 0.56255101 0.55364121 0.55896655 0.57908518]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6204055766793409, 'F1-Score': 0.5511773540128687}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.6242339  0.60895157 0.61825506 0.6238179  0.64328164]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.6242078580481623, 'F1-Score': 0.6249644050664117}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.61880804 0.62025956 0.60358579 0.65678492 0.64384519]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6375158428390367, 'F1-Score': 0.6362248650354846}\n",
            "Best model saved to best_models/predict_total_pp_MI_more_restricted_Neural Network.pkl\n",
            "Processing complete for 'predict_total_pp_MI_more_restricted'. Total time: 408.44 seconds\n",
            "Best model: {'model_name': 'Neural Network', 'params': array([0.61880804, 0.62025956, 0.60358579, 0.65678492, 0.64384519]), 'dataset': 'predict_total_pp_MI_more_restricted', 'test_scores': {'Accuracy': 0.6375158428390367, 'F1-Score': 0.6362248650354846}}\n"
          ]
        }
      ],
      "source": [
        "mi_more_restricted_total_pp_results, mi_more_restricted_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='MI',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBYdRcYfDLDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2213f83f-88e8-4c06-9051-8a82be63be87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_RF_more_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-13.49719437 -14.57016573 -12.27867205 -13.34393698 -13.85022322]\n",
            "Test set scores for Linear Regression: {'MSE': 11.80714458518987, 'R2': 0.6177346758715104, 'MAE': 2.5989238323213617}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-17.07747569 -20.56306126 -17.18126404 -17.36203008 -16.92458014]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 15.978281904775443, 'R2': 0.4826909192755394, 'MAE': 2.793854194336947}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-12.34316779 -14.33715205 -11.94624349 -11.77185138 -12.36259099]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 10.747320827314606, 'R2': 0.6520472795158696, 'MAE': 2.3937230606995774}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-12.54930618 -13.65084341 -11.7312359  -12.37640116 -12.49263174]\n",
            "Test set scores for Support Vector Machine: {'MSE': 10.88175711666978, 'R2': 0.6476948019668569, 'MAE': 2.3955363405338326}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-13.35285278 -14.8175061  -13.23702254 -13.53716162 -13.3995278 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 11.469090297716043, 'R2': 0.6286794416310745, 'MAE': 2.4377150099583558}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-12.17985466 -13.13956265 -11.19345148 -11.62816789 -12.16662674]\n",
            "Test set scores for Neural Network: {'MSE': 10.897353071244341, 'R2': 0.647189871025466, 'MAE': 2.4112848005862384}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.59198492 0.60589456 0.59234586 0.58953416 0.61034203]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.6191381495564005, 'F1-Score': 0.5882413969989239}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': None}\n",
            "Cross-validation scores for Decision Tree: [0.59855535 0.60183269 0.61554859 0.61390339 0.63608304]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6096324461343473, 'F1-Score': 0.6077200359812981}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.62135446 0.65083202 0.64466444 0.65527154 0.66386334]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.632446134347275, 'F1-Score': 0.6319746339361063}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.56835734 0.56255101 0.55364121 0.55896655 0.57908518]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6204055766793409, 'F1-Score': 0.5511773540128687}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.6242339  0.60895157 0.61825506 0.6238179  0.64328164]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.6242078580481623, 'F1-Score': 0.6249644050664117}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.61880804 0.62025956 0.60358579 0.65678492 0.64384519]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6375158428390367, 'F1-Score': 0.6362248650354846}\n",
            "Best model saved to best_models/predict_total_pp_RF_more_restricted_Neural Network.pkl\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_RF_more_restricted_regression\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-14.07259509 -14.8096461  -13.05723877 -14.07382443 -14.44467067]\n",
            "Test set scores for Linear Regression: {'MSE': 12.179176340373877, 'R2': 0.6056898636431678, 'MAE': 2.639137693667176}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-18.18457669 -19.60916364 -19.50221679 -17.53878177 -19.37935029]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 17.920182808049784, 'R2': 0.419820394326883, 'MAE': 2.9164365673026373}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-13.06670535 -14.4909809  -13.3149714  -12.69096625 -13.93424886]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 12.40245058433275, 'R2': 0.5984611894602887, 'MAE': 2.5499879853521454}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-13.14815406 -14.07359675 -12.47765276 -13.22198435 -13.38141548]\n",
            "Test set scores for Support Vector Machine: {'MSE': 11.413809678694768, 'R2': 0.630469193894691, 'MAE': 2.4442028936472173}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-13.77038797 -15.27810364 -13.79489958 -14.19413629 -14.35276044]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 12.21674297095264, 'R2': 0.6044736152851509, 'MAE': 2.5345826543545176}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-13.23707592 -13.73641524 -12.57485356 -12.57832681 -14.23640307]\n",
            "Test set scores for Neural Network: {'MSE': 11.371420081079174, 'R2': 0.6318415894942611, 'MAE': 2.445861992098806}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.55266215 0.56918612 0.59523893 0.57508779 0.57424644]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.6229404309252218, 'F1-Score': 0.5848762742816279}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': None}\n",
            "Cross-validation scores for Decision Tree: [0.59252662 0.5898788  0.62386004 0.60436083 0.61638819]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.5817490494296578, 'F1-Score': 0.5759564704486814}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.61773062 0.61084211 0.64566389 0.63231437 0.64767675]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.6140684410646388, 'F1-Score': 0.6090020548759759}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.54296835 0.54442316 0.552781   0.54049721 0.54220267]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6134347275031685, 'F1-Score': 0.5385850515657332}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.60639777 0.60822365 0.61626047 0.59760745 0.64408347]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.5937896070975919, 'F1-Score': 0.5931256880063529}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.632271   0.60884774 0.59428373 0.59555573 0.60690626]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6267427122940431, 'F1-Score': 0.606304759148641}\n",
            "Best model saved to best_models/predict_total_pp_RF_more_restricted_Neural Network.pkl\n",
            "Processing complete for 'predict_total_pp_RF_more_restricted_regression'. Total time: 832.09 seconds\n",
            "Best model: {'model_name': 'Neural Network', 'params': array([0.61880804, 0.62025956, 0.60358579, 0.65678492, 0.64384519]), 'dataset': 'predict_total_pp_RF_more_restricted', 'test_scores': {'Accuracy': 0.6375158428390367, 'F1-Score': 0.6362248650354846}}\n"
          ]
        }
      ],
      "source": [
        "rf_more_restricted_total_pp_results, rf_more_restricted_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V15GwdGtDLDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e18bf7-497a-45d6-9f05-3b66a809aa7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_DT_more_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-13.75131351 -14.7251896  -12.65377983 -13.58399406 -14.22168553]\n",
            "Test set scores for Linear Regression: {'MSE': 12.410501455872378, 'R2': 0.5982005363450154, 'MAE': 2.6624000568964763}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-16.74062635 -19.55588195 -17.1203406  -16.98335873 -17.02948865]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 15.908000948448603, 'R2': 0.4849663189165364, 'MAE': 2.7877590417489846}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-12.33154393 -14.33737304 -11.94844161 -11.66447108 -12.29074008]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 10.763827897826163, 'R2': 0.651512850500117, 'MAE': 2.4007992986873257}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-12.58542723 -13.66526625 -11.76127938 -12.37741515 -12.50171814]\n",
            "Test set scores for Support Vector Machine: {'MSE': 10.957827297907576, 'R2': 0.6452319717476136, 'MAE': 2.4048316189035392}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-13.29873802 -14.65689078 -13.17288722 -13.47585627 -13.41579611]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 11.338739298000569, 'R2': 0.6328996548076973, 'MAE': 2.4401593336954552}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-12.53217431 -13.1083303  -11.32148901 -11.89585921 -12.1674282 ]\n",
            "Test set scores for Neural Network: {'MSE': 10.52989731835022, 'R2': 0.6590865316845673, 'MAE': 2.3747882639215554}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.59446395 0.58945553 0.58639761 0.58330547 0.60242689]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.6115335868187579, 'F1-Score': 0.578823096304253}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': None}\n",
            "Cross-validation scores for Decision Tree: [0.6087787  0.59792426 0.62191738 0.60229019 0.6360214 ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6070975918884665, 'F1-Score': 0.6058781482725197}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.62387325 0.63931746 0.64905391 0.65082863 0.66396568]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.6273764258555133, 'F1-Score': 0.6266884689965861}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.56406411 0.56367719 0.55021186 0.56139848 0.58281122]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6178707224334601, 'F1-Score': 0.5500182842199515}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.62339276 0.60809505 0.61836555 0.62281063 0.64653079]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.6242078580481623, 'F1-Score': 0.624912227814876}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.63753954 0.59623082 0.61644769 0.62846829 0.58044793]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6185044359949303, 'F1-Score': 0.6149297313194547}\n",
            "Best model saved to best_models/predict_total_pp_DT_more_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_pp_DT_more_restricted'. Total time: 387.90 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.62387325, 0.63931746, 0.64905391, 0.65082863, 0.66396568]), 'dataset': 'predict_total_pp_DT_more_restricted', 'test_scores': {'Accuracy': 0.6273764258555133, 'F1-Score': 0.6266884689965861}}\n"
          ]
        }
      ],
      "source": [
        "dt_more_restricted_total_pp_results, dt_more_restricted_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=dt_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='DT',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7jBjhhMDLDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ddf066a-9c17-4a42-8639-f24cd536a0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_Lasso_more_restricted_regression\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-14.07259509 -14.8096461  -13.05723877 -14.07382443 -14.44467067]\n",
            "Test set scores for Linear Regression: {'MSE': 12.179176340373877, 'R2': 0.6056898636431678, 'MAE': 2.639137693667176}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-18.18457669 -19.60916364 -19.50221679 -17.53878177 -19.37935029]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 17.920182808049784, 'R2': 0.419820394326883, 'MAE': 2.9164365673026373}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-13.06670535 -14.4909809  -13.3149714  -12.69096625 -13.93424886]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 12.40245058433275, 'R2': 0.5984611894602887, 'MAE': 2.5499879853521454}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-13.14815406 -14.07359675 -12.47765276 -13.22198435 -13.38141548]\n",
            "Test set scores for Support Vector Machine: {'MSE': 11.413809678694768, 'R2': 0.630469193894691, 'MAE': 2.4442028936472173}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-13.77038797 -15.27810364 -13.79489958 -14.19413629 -14.35276044]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 12.21674297095264, 'R2': 0.6044736152851509, 'MAE': 2.5345826543545176}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-13.23707592 -13.73641524 -12.57485356 -12.57832681 -14.23640307]\n",
            "Test set scores for Neural Network: {'MSE': 11.371420081079174, 'R2': 0.6318415894942611, 'MAE': 2.445861992098806}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.55266215 0.56918612 0.59523893 0.57508779 0.57424644]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.6229404309252218, 'F1-Score': 0.5848762742816279}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': None}\n",
            "Cross-validation scores for Decision Tree: [0.59252662 0.5898788  0.62386004 0.60436083 0.61638819]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.5817490494296578, 'F1-Score': 0.5759564704486814}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.61773062 0.61084211 0.64566389 0.63231437 0.64767675]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.6140684410646388, 'F1-Score': 0.6090020548759759}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.54296835 0.54442316 0.552781   0.54049721 0.54220267]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6134347275031685, 'F1-Score': 0.5385850515657332}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.60639777 0.60822365 0.61626047 0.59760745 0.64408347]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.5937896070975919, 'F1-Score': 0.5931256880063529}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.632271   0.60884774 0.59428373 0.59555573 0.60690626]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6267427122940431, 'F1-Score': 0.606304759148641}\n",
            "Best model saved to best_models/predict_total_pp_Lasso_more_restricted_regression_Random Forest.pkl\n",
            "Processing complete for 'predict_total_pp_Lasso_more_restricted_regression'. Total time: 418.14 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.61773062, 0.61084211, 0.64566389, 0.63231437, 0.64767675]), 'dataset': 'predict_total_pp_Lasso_more_restricted_regression', 'test_scores': {'Accuracy': 0.6140684410646388, 'F1-Score': 0.6090020548759759}}\n"
          ]
        }
      ],
      "source": [
        "lasso_more_restricted_total_pp_results, lasso_more_restricted_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=lasso_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='Lasso',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-uTcs-iDLDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff2c5c5-154a-4447-dd91-8f97f35270d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_FR_more_restricted_regression\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-14.08428735 -14.80491935 -13.08837325 -14.06953453 -14.47990177]\n",
            "Test set scores for Linear Regression: {'MSE': 12.222191979281321, 'R2': 0.6042971994786097, 'MAE': 2.6466078394776225}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-17.65371301 -16.90705732 -17.71388714 -16.0989159  -17.38888106]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 16.04973849003883, 'R2': 0.4803774577497877, 'MAE': 2.734412268773845}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-12.81178706 -14.04308067 -12.6823436  -12.66841083 -13.96457843]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 11.799534912831461, 'R2': 0.6179810448263081, 'MAE': 2.4996719095493045}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-13.19734328 -13.97631667 -12.38730341 -13.10532492 -13.51336476]\n",
            "Test set scores for Support Vector Machine: {'MSE': 11.4249807036176, 'R2': 0.6301075234304934, 'MAE': 2.439881702632104}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-13.59833244 -15.38571913 -13.08006404 -13.11819593 -14.0407193 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 11.984609813507152, 'R2': 0.6119891035585114, 'MAE': 2.5229042187217092}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-13.43397386 -13.89629905 -12.27632006 -12.8542496  -13.76245399]\n",
            "Test set scores for Neural Network: {'MSE': 11.357666977447227, 'R2': 0.632286856728833, 'MAE': 2.479505457938126}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.5487227  0.56582652 0.5740477  0.55414439 0.55816126]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.6096324461343473, 'F1-Score': 0.5534971312158932}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.61397642 0.61184261 0.60852309 0.62104278 0.6146353 ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.5931558935361216, 'F1-Score': 0.5951293908558833}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.61307138 0.61990969 0.62768652 0.62612893 0.62705017]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.6121673003802282, 'F1-Score': 0.608923336423812}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.47267299 0.47441078 0.46973529 0.52100352 0.46728204]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.596958174904943, 'F1-Score': 0.49494413143326244}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.61855538 0.60062152 0.60797025 0.6085948  0.62481912]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.5899873257287706, 'F1-Score': 0.5851085082886318}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.62899825 0.60075687 0.62126969 0.61113381 0.63336912]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6191381495564005, 'F1-Score': 0.6091988303030682}\n",
            "Best model saved to best_models/predict_total_pp_FR_more_restricted_regression_Neural Network.pkl\n",
            "Processing complete for 'predict_total_pp_FR_more_restricted_regression'. Total time: 394.19 seconds\n",
            "Best model: {'model_name': 'Neural Network', 'params': array([0.62899825, 0.60075687, 0.62126969, 0.61113381, 0.63336912]), 'dataset': 'predict_total_pp_FR_more_restricted_regression', 'test_scores': {'Accuracy': 0.6191381495564005, 'F1-Score': 0.6091988303030682}}\n"
          ]
        }
      ],
      "source": [
        "fr_more_restricted_total_pp_results, fr_more_restricted_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=fr_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='FR',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTOqLuhm3PA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de1017e-6673-44f8-b831-f8c71b123b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column to Predict/Classify: Total_PP. Feature Set: more_restricted\n",
            "Best Model Info using MI Feature Selection method: {'model_name': 'Neural Network', 'params': array([0.61880804, 0.62025956, 0.60358579, 0.65678492, 0.64384519]), 'dataset': 'predict_total_pp_MI_more_restricted', 'test_scores': {'Accuracy': 0.6375158428390367, 'F1-Score': 0.6362248650354846}}\n",
            "Best Model Info using RF Feature Selection method: {'model_name': 'Neural Network', 'params': array([0.61880804, 0.62025956, 0.60358579, 0.65678492, 0.64384519]), 'dataset': 'predict_total_pp_RF_more_restricted', 'test_scores': {'Accuracy': 0.6375158428390367, 'F1-Score': 0.6362248650354846}}\n",
            "Best Model Info using DT Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.62387325, 0.63931746, 0.64905391, 0.65082863, 0.66396568]), 'dataset': 'predict_total_pp_DT_more_restricted', 'test_scores': {'Accuracy': 0.6273764258555133, 'F1-Score': 0.6266884689965861}}\n",
            "Best Model Info using Lasso Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.61773062, 0.61084211, 0.64566389, 0.63231437, 0.64767675]), 'dataset': 'predict_total_pp_Lasso_more_restricted_regression', 'test_scores': {'Accuracy': 0.6140684410646388, 'F1-Score': 0.6090020548759759}}\n",
            "Best Model Info using FR Feature Selection method: {'model_name': 'Neural Network', 'params': array([0.62899825, 0.60075687, 0.62126969, 0.61113381, 0.63336912]), 'dataset': 'predict_total_pp_FR_more_restricted_regression', 'test_scores': {'Accuracy': 0.6191381495564005, 'F1-Score': 0.6091988303030682}}\n"
          ]
        }
      ],
      "source": [
        "print(\"Column to Predict/Classify: Total_PP. Feature Set: more_restricted\")\n",
        "print(\"Best Model Info using MI Feature Selection method:\", mi_more_restricted_total_pp_best_model)\n",
        "print(\"Best Model Info using RF Feature Selection method:\", rf_more_restricted_total_pp_best_model)\n",
        "print(\"Best Model Info using DT Feature Selection method:\", dt_more_restricted_total_pp_best_model)\n",
        "print(\"Best Model Info using Lasso Feature Selection method:\", lasso_more_restricted_total_pp_best_model)\n",
        "print(\"Best Model Info using FR Feature Selection method:\", fr_more_restricted_total_pp_best_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ySIpw40JrRi"
      },
      "source": [
        "#### Restricted - L4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN4hUTk3DLDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9a0486-576c-4033-faee-b2aa618a3d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_MI_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-13.53566097 -14.57720999 -12.356423   -13.35859839 -13.9410284 ]\n",
            "Test set scores for Linear Regression: {'MSE': 11.891709662789353, 'R2': 0.6149968168942379, 'MAE': 2.608585749647105}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-17.28301356 -17.83313985 -15.92170243 -15.2221133  -15.89422277]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 14.490328360894642, 'R2': 0.5308645517432223, 'MAE': 2.65768515481242}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-12.14900579 -13.71506491 -11.53682476 -11.42656    -12.22864578]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 10.529636623873845, 'R2': 0.6590949718673559, 'MAE': 2.36594367820937}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-12.50441682 -13.54709681 -11.50866152 -12.17807023 -12.55858122]\n",
            "Test set scores for Support Vector Machine: {'MSE': 10.84107494986631, 'R2': 0.6490119181897651, 'MAE': 2.388213676165599}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-13.03469226 -14.65480634 -12.35644749 -11.96097869 -12.74083897]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 11.487041204314426, 'R2': 0.6280982673192228, 'MAE': 2.453648379503893}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-12.15334953 -13.12756358 -10.96506599 -11.67343533 -12.591208  ]\n",
            "Test set scores for Neural Network: {'MSE': 10.824238918621838, 'R2': 0.649556997560503, 'MAE': 2.3860587052696456}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.5967371  0.59623289 0.59321457 0.59329929 0.6062736 ]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.6121673003802282, 'F1-Score': 0.5811198347953734}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': None}\n",
            "Cross-validation scores for Decision Tree: [0.62016522 0.62434586 0.62167876 0.63079451 0.64526471]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6128010139416984, 'F1-Score': 0.6119273603368364}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.63182214 0.64694799 0.65606413 0.65661862 0.66339998]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.6394169835234474, 'F1-Score': 0.6395088963549419}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.5704284  0.59884088 0.591071   0.57692797 0.58764091]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6216730038022814, 'F1-Score': 0.5850521915244138}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.63191558 0.62236665 0.63130559 0.64124398 0.64002455]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.6216730038022814, 'F1-Score': 0.621995234822579}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.63311568 0.62516769 0.60722075 0.61930836 0.65062648]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6305449936628644, 'F1-Score': 0.6321496139388657}\n",
            "Best model saved to best_models/predict_total_pp_MI_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_pp_MI_restricted'. Total time: 420.11 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.63182214, 0.64694799, 0.65606413, 0.65661862, 0.66339998]), 'dataset': 'predict_total_pp_MI_restricted', 'test_scores': {'Accuracy': 0.6394169835234474, 'F1-Score': 0.6395088963549419}}\n"
          ]
        }
      ],
      "source": [
        "mi_restricted_total_pp_results, mi_restricted_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='MI',\n",
        "    exclusion_type='restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1U_Tcq_DLDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a681fc-f115-4124-d4fc-3c96c0533b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_RF_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-13.49719437 -14.57016573 -12.27867205 -13.34393698 -13.85022322]\n",
            "Test set scores for Linear Regression: {'MSE': 11.807144585189867, 'R2': 0.6177346758715105, 'MAE': 2.5989238323213595}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-16.67763404 -20.66361549 -17.13055089 -17.07886706 -16.92200486]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 16.013769864217775, 'R2': 0.48154196948323336, 'MAE': 2.790051912968126}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-12.32444554 -14.33698947 -11.9362552  -11.75980029 -12.33037598]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 10.747012221655941, 'R2': 0.6520572708597789, 'MAE': 2.394289448824901}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-12.54930618 -13.65084341 -11.7312359  -12.37640116 -12.49263174]\n",
            "Test set scores for Support Vector Machine: {'MSE': 10.88175711666978, 'R2': 0.6476948019668569, 'MAE': 2.3955363405338326}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-13.19569538 -14.78350865 -13.53328051 -13.39967334 -13.25092985]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 11.647642326892734, 'R2': 0.6228986833101642, 'MAE': 2.455096867644396}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-12.06664769 -13.32709434 -11.24920636 -11.85917561 -12.27161148]\n",
            "Test set scores for Neural Network: {'MSE': 10.928854092768782, 'R2': 0.6461700013934363, 'MAE': 2.403752032581806}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.59198492 0.60589456 0.59234586 0.58953416 0.61034203]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.6191381495564005, 'F1-Score': 0.5882413969989239}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': None}\n",
            "Cross-validation scores for Decision Tree: [0.59715646 0.59861233 0.60890068 0.61067218 0.63351591]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6102661596958175, 'F1-Score': 0.6081082410842075}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.62444188 0.64922035 0.64362218 0.65296301 0.66215869]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.6311787072243346, 'F1-Score': 0.630602653896984}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.56835734 0.56255101 0.55364121 0.55896655 0.57908518]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6204055766793409, 'F1-Score': 0.5511773540128687}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 3}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.60959213 0.61492514 0.61476119 0.62666161 0.6401498 ]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.6083650190114068, 'F1-Score': 0.606447154766233}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.63011208 0.63542668 0.62894314 0.63508828 0.64854931]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6261089987325729, 'F1-Score': 0.6125594895189953}\n",
            "Best model saved to best_models/predict_total_pp_RF_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_pp_RF_restricted'. Total time: 435.55 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.62444188, 0.64922035, 0.64362218, 0.65296301, 0.66215869]), 'dataset': 'predict_total_pp_RF_restricted', 'test_scores': {'Accuracy': 0.6311787072243346, 'F1-Score': 0.630602653896984}}\n"
          ]
        }
      ],
      "source": [
        "rf_restricted_total_pp_results, rf_restricted_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c4daUp9DLDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b178e536-4941-41a5-9dc8-6c4650a77575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_DT_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-13.49719437 -14.57016573 -12.27867205 -13.34393698 -13.85022322]\n",
            "Test set scores for Linear Regression: {'MSE': 11.807144585189867, 'R2': 0.6177346758715105, 'MAE': 2.5989238323213595}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-16.67763404 -20.66361549 -17.13055089 -17.07886706 -16.92200486]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 16.013769864217775, 'R2': 0.48154196948323336, 'MAE': 2.790051912968126}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-12.32444554 -14.33698947 -11.9362552  -11.75980029 -12.33037598]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 10.747012221655941, 'R2': 0.6520572708597789, 'MAE': 2.394289448824901}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-12.54930618 -13.65084341 -11.7312359  -12.37640116 -12.49263174]\n",
            "Test set scores for Support Vector Machine: {'MSE': 10.88175711666978, 'R2': 0.6476948019668569, 'MAE': 2.3955363405338326}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-13.19569538 -14.78350865 -13.53328051 -13.39967334 -13.25092985]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 11.647642326892734, 'R2': 0.6228986833101642, 'MAE': 2.455096867644396}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-12.06664769 -13.32709434 -11.24920636 -11.85917561 -12.27161148]\n",
            "Test set scores for Neural Network: {'MSE': 10.928854092768782, 'R2': 0.6461700013934363, 'MAE': 2.403752032581806}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.59198492 0.60589456 0.59234586 0.58953416 0.61034203]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.6191381495564005, 'F1-Score': 0.5882413969989239}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': None}\n",
            "Cross-validation scores for Decision Tree: [0.59715646 0.59861233 0.60890068 0.61067218 0.63351591]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.6102661596958175, 'F1-Score': 0.6081082410842075}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.62444188 0.64922035 0.64362218 0.65296301 0.66215869]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.6311787072243346, 'F1-Score': 0.630602653896984}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.56835734 0.56255101 0.55364121 0.55896655 0.57908518]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.6204055766793409, 'F1-Score': 0.5511773540128687}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 3}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.60959213 0.61492514 0.61476119 0.62666161 0.6401498 ]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.6083650190114068, 'F1-Score': 0.606447154766233}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.63011208 0.63542668 0.62894314 0.63508828 0.64854931]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6261089987325729, 'F1-Score': 0.6125594895189953}\n",
            "Best model saved to best_models/predict_total_pp_DT_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_pp_DT_restricted'. Total time: 432.58 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.62444188, 0.64922035, 0.64362218, 0.65296301, 0.66215869]), 'dataset': 'predict_total_pp_DT_restricted', 'test_scores': {'Accuracy': 0.6311787072243346, 'F1-Score': 0.630602653896984}}\n"
          ]
        }
      ],
      "source": [
        "dt_restricted_total_pp_results, dt_restricted_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=dt_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='DT',\n",
        "    exclusion_type='restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--4GUugH3XxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1829196c-babd-4083-979f-5e7056274b6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column to Predict/Classify: Total_PP. Feature Set: restricted\n",
            "Best Model Info using MI Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.63182214, 0.64694799, 0.65606413, 0.65661862, 0.66339998]), 'dataset': 'predict_total_pp_MI_restricted', 'test_scores': {'Accuracy': 0.6394169835234474, 'F1-Score': 0.6395088963549419}}\n",
            "Best Model Info using RF Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.62444188, 0.64922035, 0.64362218, 0.65296301, 0.66215869]), 'dataset': 'predict_total_pp_RF_restricted', 'test_scores': {'Accuracy': 0.6311787072243346, 'F1-Score': 0.630602653896984}}\n",
            "Best Model Info using DT Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.62444188, 0.64922035, 0.64362218, 0.65296301, 0.66215869]), 'dataset': 'predict_total_pp_DT_restricted', 'test_scores': {'Accuracy': 0.6311787072243346, 'F1-Score': 0.630602653896984}}\n"
          ]
        }
      ],
      "source": [
        "print(\"Column to Predict/Classify: Total_PP. Feature Set: restricted\")\n",
        "print(\"Best Model Info using MI Feature Selection method:\", mi_restricted_total_pp_best_model)\n",
        "print(\"Best Model Info using RF Feature Selection method:\", rf_restricted_total_pp_best_model)\n",
        "print(\"Best Model Info using DT Feature Selection method:\", dt_restricted_total_pp_best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgcH-_f9JvxT"
      },
      "source": [
        "#### Less Restricted - L4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGaC1C1nDLDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5aaa314-87c6-46e9-cdd5-3534f9d0a736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: less_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_MI_less_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-1.3726167  -1.50460784 -1.00714629 -1.37721692 -1.05327227]\n",
            "Test set scores for Linear Regression: {'MSE': 0.9996773450151357, 'R2': 0.9676346824112365, 'MAE': 0.5586129644386143}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-4.26444972 -4.09738717 -4.01347068 -4.47147385 -3.62282092]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 5.229404309252218, 'R2': 0.8306940413194683, 'MAE': 1.162230671736375}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-2.12777981 -2.03238377 -1.55015578 -2.04893716 -1.54362005]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.4439044993662864, 'R2': 0.9532524889927092, 'MAE': 0.7166096324461343}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-4.89792148 -5.21895169 -4.19835853 -4.77549368 -4.77441152]\n",
            "Test set scores for Support Vector Machine: {'MSE': 3.505107732028657, 'R2': 0.8865194600081487, 'MAE': 1.296820862947023}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-5.07723755 -5.13902758 -4.34472331 -4.96156085 -4.80824089]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 4.011626703913505, 'R2': 0.8701205214190795, 'MAE': 1.404218721709216}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-1.30415276 -1.38572016 -0.91006059 -1.62729086 -1.39131859]\n",
            "Test set scores for Neural Network: {'MSE': 1.174482774807686, 'R2': 0.9619752231070042, 'MAE': 0.5415343969577938}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.89000031 0.90743661 0.89996023 0.89229437 0.91266879]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8910012674271229, 'F1-Score': 0.8910102915011406}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.86279024 0.85224656 0.86549797 0.86672016 0.87792649]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8555133079847909, 'F1-Score': 0.8552063899010606}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.90718654 0.90646434 0.90209358 0.89144211 0.90862407]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9062103929024081, 'F1-Score': 0.9062467373667265}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.82050553 0.79224732 0.82126287 0.79464599 0.81240185]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8168567807351077, 'F1-Score': 0.8174961073218289}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.80378737 0.80113712 0.80545919 0.78790194 0.80758712]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.785171102661597, 'F1-Score': 0.7823461818826049}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.90608964 0.91243631 0.90208612 0.9109772  0.92525305]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.899873257287706, 'F1-Score': 0.899065317682924}\n",
            "Best model saved to best_models/predict_total_pp_MI_less_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_pp_MI_less_restricted'. Total time: 520.98 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.90718654, 0.90646434, 0.90209358, 0.89144211, 0.90862407]), 'dataset': 'predict_total_pp_MI_less_restricted', 'test_scores': {'Accuracy': 0.9062103929024081, 'F1-Score': 0.9062467373667265}}\n"
          ]
        }
      ],
      "source": [
        "mi_less_restricted_total_pp_results, mi_less_restricted_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='MI',\n",
        "    exclusion_type='less_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "531fUnH6DLDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea19901-623b-4a91-ad71-368be1c2ef1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: less_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_RF_less_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-1.37276883 -1.5029492  -1.00776497 -1.36858087 -1.05519267]\n",
            "Test set scores for Linear Regression: {'MSE': 1.0002341922667404, 'R2': 0.9676166540561513, 'MAE': 0.559615825255855}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.63895487 -4.34996041 -3.93977813 -3.63391442 -3.54675119]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 5.56020278833967, 'R2': 0.8199841878983295, 'MAE': 1.1749049429657794}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-2.0666361  -2.00300222 -1.58969525 -2.07784208 -1.54387187]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.458448732572877, 'R2': 0.952781608333901, 'MAE': 0.721362484157161}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-4.91124245 -5.23190565 -4.201988   -4.78115564 -4.78162522]\n",
            "Test set scores for Support Vector Machine: {'MSE': 3.507520151885553, 'R2': 0.8864413560726987, 'MAE': 1.2975446681039102}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-5.0349327  -5.31420428 -4.40694136 -5.12304279 -5.02386688]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 4.206818757921419, 'R2': 0.863801029584768, 'MAE': 1.4444866920152089}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-1.32910511 -1.32569474 -0.90878258 -1.62063673 -1.22733962]\n",
            "Test set scores for Neural Network: {'MSE': 1.0438724498315755, 'R2': 0.9662038321370102, 'MAE': 0.5467097884410766}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.89000031 0.90819052 0.89837484 0.89154975 0.91188931]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8929024081115335, 'F1-Score': 0.8929580587996512}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.8629224  0.85545871 0.86549797 0.86678922 0.8761513 ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8574144486692015, 'F1-Score': 0.8571927510485464}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.90804161 0.90488385 0.90360846 0.89605874 0.90940219]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9024081115335868, 'F1-Score': 0.9024006018517943}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.82208441 0.79377697 0.82114973 0.79468258 0.81002248]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8174904942965779, 'F1-Score': 0.8181425616702258}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.79841794 0.79319552 0.80083578 0.79592429 0.80039587]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7801013941698353, 'F1-Score': 0.7781408769729218}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.8994241  0.911402   0.91887274 0.89914844 0.91568955]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8504435994930292, 'F1-Score': 0.8512427177436624}\n",
            "Best model saved to best_models/predict_total_pp_RF_less_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_pp_RF_less_restricted'. Total time: 529.31 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.90804161, 0.90488385, 0.90360846, 0.89605874, 0.90940219]), 'dataset': 'predict_total_pp_RF_less_restricted', 'test_scores': {'Accuracy': 0.9024081115335868, 'F1-Score': 0.9024006018517943}}\n"
          ]
        }
      ],
      "source": [
        "rf_less_restricted_total_pp_results, rf_less_restricted_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='less_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWhjsc49DLDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4ced44-3f1a-42db-f324-db6df4dc8805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_PP with exclusion type: less_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_pp_DT_less_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-1.37256398 -1.50388331 -1.00671781 -1.37660052 -1.0549386 ]\n",
            "Test set scores for Linear Regression: {'MSE': 0.9989182694324558, 'R2': 0.9676592580630005, 'MAE': 0.5590792611170265}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-4.2557403  -4.36262866 -4.16719493 -4.40332805 -3.56339144]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 5.414448669201521, 'R2': 0.8247030888310152, 'MAE': 1.1660329531051965}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-2.10744656 -2.02354339 -1.57953067 -2.04242758 -1.54041141]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.4762446768060835, 'R2': 0.9522054510469806, 'MAE': 0.719512040557668}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-4.90918662 -5.23150498 -4.20072735 -4.78027634 -4.78066794]\n",
            "Test set scores for Support Vector Machine: {'MSE': 3.5070588595273153, 'R2': 0.8864562907651273, 'MAE': 1.2972659233034844}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-5.03505938 -5.32025337 -4.41359746 -5.11144216 -5.02427892]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 4.213586818757921, 'R2': 0.8635819084458068, 'MAE': 1.447148288973384}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-1.49603767 -1.31705045 -0.91941137 -1.40699303 -1.11036853]\n",
            "Test set scores for Neural Network: {'MSE': 1.1273270152061816, 'R2': 0.9635019268412166, 'MAE': 0.5839437033842875}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.89000031 0.90979552 0.89840573 0.89229437 0.91188916]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8916349809885932, 'F1-Score': 0.8916319531918425}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.86679443 0.85313342 0.86460712 0.86449198 0.87696685]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8542458808618505, 'F1-Score': 0.8539527042544599}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.91347653 0.90720658 0.90603414 0.89220227 0.9085131 ]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9062103929024081, 'F1-Score': 0.9062806759103527}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.82132313 0.79377697 0.82196862 0.79544926 0.81002248]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8174904942965779, 'F1-Score': 0.8181425616702258}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.79423203 0.79394498 0.79910233 0.79452304 0.79941871]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.7807351077313055, 'F1-Score': 0.7787405019055117}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.90539846 0.91867614 0.91194189 0.91423441 0.91504433]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9188846641318125, 'F1-Score': 0.9184544024331468}\n",
            "Best model saved to best_models/predict_total_pp_DT_less_restricted_Neural Network.pkl\n",
            "Processing complete for 'predict_total_pp_DT_less_restricted'. Total time: 522.71 seconds\n",
            "Best model: {'model_name': 'Neural Network', 'params': array([0.90539846, 0.91867614, 0.91194189, 0.91423441, 0.91504433]), 'dataset': 'predict_total_pp_DT_less_restricted', 'test_scores': {'Accuracy': 0.9188846641318125, 'F1-Score': 0.9184544024331468}}\n"
          ]
        }
      ],
      "source": [
        "dt_less_restricted_total_pp_results, dt_less_restricted_total_pp_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=dt_total_pp_datasets,\n",
        "    target_column='Total_PP',\n",
        "    feature_set_name='DT',\n",
        "    exclusion_type='less_restricted'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bSLEmWE3ceV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce7de13-16c6-44ed-ee03-82a20619fc50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column to Predict/Classify: Total_PP. Feature Set: less_restricted\n",
            "Best Model Info using MI Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.90718654, 0.90646434, 0.90209358, 0.89144211, 0.90862407]), 'dataset': 'predict_total_pp_MI_less_restricted', 'test_scores': {'Accuracy': 0.9062103929024081, 'F1-Score': 0.9062467373667265}}\n",
            "Best Model Info using RF Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.90804161, 0.90488385, 0.90360846, 0.89605874, 0.90940219]), 'dataset': 'predict_total_pp_RF_less_restricted', 'test_scores': {'Accuracy': 0.9024081115335868, 'F1-Score': 0.9024006018517943}}\n",
            "Best Model Info using DT Feature Selection method: {'model_name': 'Neural Network', 'params': array([0.90539846, 0.91867614, 0.91194189, 0.91423441, 0.91504433]), 'dataset': 'predict_total_pp_DT_less_restricted', 'test_scores': {'Accuracy': 0.9188846641318125, 'F1-Score': 0.9184544024331468}}\n"
          ]
        }
      ],
      "source": [
        "print(\"Column to Predict/Classify: Total_PP. Feature Set: less_restricted\")\n",
        "print(\"Best Model Info using MI Feature Selection method:\", mi_less_restricted_total_pp_best_model)\n",
        "print(\"Best Model Info using RF Feature Selection method:\", rf_less_restricted_total_pp_best_model)\n",
        "print(\"Best Model Info using DT Feature Selection method:\", dt_less_restricted_total_pp_best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F37RPSkfDLDT"
      },
      "source": [
        "### 4.3.3 Predicting Total_SPA feature"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CPU Tests"
      ],
      "metadata": {
        "id": "8HINfUDfl30w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_SPA' using the original dataset without feature selection\n",
        "original_total_spa_results, original_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    group_col='Year',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488684a6-65e8-4566-8582-7fcda79917e0",
        "id": "GFERNCjQl8-G"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_SPA\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-41.08611485 -41.67571183 -36.9647161  -43.15098941 -37.40749107]\n",
            "Test set scores for Linear Regression: {'MSE': 34.177352829641045, 'R2': 0.7599340965367161, 'MAE': 4.224361155398544}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-39.5946318  -39.34141741 -40.16540616 -40.50519759 -35.59599497]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 37.9212650625926, 'R2': 0.7336363994280253, 'MAE': 2.768247261583435}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-28.74651896 -29.11211397 -27.96492396 -28.80380947 -24.25517222]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 25.188374261915428, 'R2': 0.8230737806377509, 'MAE': 2.4438262057828464}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-213.75737847 -196.57137309 -194.36148904 -191.16869912 -186.77731646]\n",
            "Test set scores for Support Vector Machine: {'MSE': 195.48961308120127, 'R2': -0.37314293520488806, 'MAE': 11.475205020915542}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-26.93646837 -28.89940677 -25.12190774 -28.42075046 -24.32380869]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 22.586019821008254, 'R2': 0.8413530362928657, 'MAE': 2.269835234474018}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-82.3020639  -86.00694043 -78.14597951 -85.01301316 -84.6056657 ]\n",
            "Test set scores for Neural Network: {'MSE': 66.99328643801529, 'R2': 0.529430968077535, 'MAE': 6.221418998203947}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.81129165 0.80053417 0.82097967 0.79150984 0.81532018]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8409378960709759, 'F1-Score': 0.8400787229838055}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.97036795 0.96719893 0.97072268 0.96992612 0.97230703]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9740177439797212, 'F1-Score': 0.9730938817399066}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.97036795 0.96719893 0.97151443 0.96992612 0.97309888]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.1}\n",
            "Cross-validation scores for Support Vector Machine: [0.35331333 0.35331333 0.35377785 0.35377785 0.35377785]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.5069708491761724, 'F1-Score': 0.3411073838023027}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.97036743 0.96640788 0.9715135  0.968341   0.97230626]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9790874524714829, 'F1-Score': 0.978160857400545}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.75626923 0.70908487 0.82362945 0.7146673  0.73641734]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8307984790874525, 'F1-Score': 0.8295253419801998}\n",
            "Best regression model saved to best_models/combined_df_Total_SPA_regression_k-Nearest Neighbors.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_SPA_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 413.72 seconds\n",
            "Best regression model: {'model_name': 'k-Nearest Neighbors', 'params': array([-26.93646837, -28.89940677, -25.12190774, -28.42075046,\n",
            "       -24.32380869]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'MSE': 22.586019821008254, 'R2': 0.8413530362928657, 'MAE': 2.269835234474018}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.97036795, 0.96719893, 0.97151443, 0.96992612, 0.97309888]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_SPA' using the original dataset without feature selection\n",
        "original_total_spa_results_zero, original_total_spa_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7USqmtOmDbD",
        "outputId": "f0579a30-2c2b-44ab-cc01-ddc001d3cd14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_SPA\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-23.77778247 -25.96166354 -22.84757956 -27.05530389 -22.61638882]\n",
            "Test set scores for Linear Regression: {'MSE': 3.7105348177551872e+22, 'R2': -2.6063250064932726e+20, 'MAE': 4849140373.876287}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-52.4107541  -51.59531669 -50.85697219 -51.06913342 -50.34671743]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 48.39383574753212, 'R2': 0.66007578296967, 'MAE': 3.056277134005271}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-26.59775648 -27.84029868 -22.85979511 -28.94598416 -25.68754438]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 23.021639703317284, 'R2': 0.8382931889976595, 'MAE': 2.3835224358333114}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-24.72281296 -26.44427244 -21.60131558 -26.93211233 -21.88001888]\n",
            "Test set scores for Support Vector Machine: {'MSE': 18.39552211091043, 'R2': 0.8707876043751247, 'MAE': 1.8099829075860407}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-26.18969692 -28.93459232 -23.37588687 -28.63775934 -24.35988502]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 21.089457873567678, 'R2': 0.851865070322883, 'MAE': 2.1643880137606373}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-24.67312281 -28.62289748 -26.99529155 -29.44898468 -23.6860717 ]\n",
            "Test set scores for Neural Network: {'MSE': 21.958751565345594, 'R2': 0.8457590451859505, 'MAE': 2.3099840917466006}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.96641134 0.96640918 0.97309948 0.96992612 0.97070883]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9790874524714829, 'F1-Score': 0.9781608856574516}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.96560769 0.96482739 0.96439196 0.9659426  0.96552275]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9784537389100126, 'F1-Score': 0.9775274697649109}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.97115918 0.96719893 0.97230626 0.96913274 0.97230703]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9790874524714829, 'F1-Score': 0.978160857400545}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.3}\n",
            "Cross-validation scores for Support Vector Machine: [0.96957554 0.96719893 0.97230537 0.9683392  0.97309817]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.96878437 0.9656169  0.9718907  0.96913274 0.97230703]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9771863117870723, 'F1-Score': 0.9765755289025614}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.96600044 0.96363779 0.96122811 0.95645394 0.96991782]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9727503168567807, 'F1-Score': 0.9721380883301016}\n",
            "Best regression model saved to best_models/combined_df_Total_SPA_regression_Support Vector Machine.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_SPA_classification_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 773.33 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-24.72281296, -26.44427244, -21.60131558, -26.93211233,\n",
            "       -21.88001888]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'MSE': 18.39552211091043, 'R2': 0.8707876043751247, 'MAE': 1.8099829075860407}}\n",
            "Best classification model: {'model_name': 'Support Vector Machine', 'params': array([0.96957554, 0.96719893, 0.97230537, 0.9683392 , 0.97309817]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_SPA' using the original dataset without feature selection\n",
        "original_total_spa_results_knn, original_total_spa_best_model_knn = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='knn',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvYEFaNqmK_2",
        "outputId": "ddb99b22-4ef8-4766-ad1d-819f522d2484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_SPA\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-38.22684684 -39.21160438 -34.09947624 -40.99316432 -35.53946991]\n",
            "Test set scores for Linear Regression: {'MSE': 1.5892364841299968e+23, 'R2': -1.116299130249192e+21, 'MAE': 10035540409.853456}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-45.4634289  -44.25665867 -54.64277808 -58.09473738 -51.50672337]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 44.75245207366756, 'R2': 0.6856533069481978, 'MAE': 3.0363424668557752}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-24.96588457 -26.41139283 -23.62259921 -30.15763343 -24.19007843]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 22.078871457180824, 'R2': 0.8449153083845349, 'MAE': 2.291534144904619}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-33.13464708 -34.23041755 -29.09623401 -35.91963301 -29.69968626]\n",
            "Test set scores for Support Vector Machine: {'MSE': 25.423442497319016, 'R2': 0.8214226326220175, 'MAE': 3.1218971355353378}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-27.31155485 -34.17723013 -25.73509847 -33.68207226 -26.18134033]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 22.12849302009777, 'R2': 0.8445667604618126, 'MAE': 2.3042667028788704}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-25.51704665 -27.69248728 -26.48865609 -29.31692877 -23.04199853]\n",
            "Test set scores for Neural Network: {'MSE': 24.44330744454475, 'R2': 0.8283072210257227, 'MAE': 2.9884776653927663}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.9672031  0.96324589 0.97151689 0.96755104 0.96676492]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9784537389100126, 'F1-Score': 0.9775274831490649}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.97115952 0.96482739 0.96597111 0.96596804 0.97309948]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9778200253485425, 'F1-Score': 0.9768940730092718}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.97036795 0.96640788 0.9715135  0.968341   0.97309817]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943468676498}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.3}\n",
            "Cross-validation scores for Support Vector Machine: [0.96957623 0.96640788 0.9715135  0.9675475  0.97230626]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9803548795944234, 'F1-Score': 0.9794278143300347}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.9648306  0.95848811 0.96279671 0.95962716 0.96636271]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9784537389100126, 'F1-Score': 0.9778431418127835}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.9679727  0.96124811 0.95844346 0.95726391 0.97072353]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.967680608365019, 'F1-Score': 0.9667607297214186}\n",
            "Best regression model saved to best_models/combined_df_Total_SPA_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_SPA_classification_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 730.60 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-24.96588457, -26.41139283, -23.62259921, -30.15763343,\n",
            "       -24.19007843]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'MSE': 22.078871457180824, 'R2': 0.8449153083845349, 'MAE': 2.291534144904619}}\n",
            "Best classification model: {'model_name': 'Support Vector Machine', 'params': array([0.96957623, 0.96640788, 0.9715135 , 0.9675475 , 0.97230626]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'Accuracy': 0.9803548795944234, 'F1-Score': 0.9794278143300347}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_more_restricted_total_spa_results_zero, dt_more_restricted_total_spa_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=dt_total_spa_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    feature_set_name='DT',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID1kukgamRlH",
        "outputId": "8d463b72-4564-4b21-e1f6-1cf6a93959c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_spa_DT_more_restricted\n",
            "Column 'Year' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-24.27397323 -26.09608914 -22.32110002 -27.25087396 -22.80329723]\n",
            "Test set scores for Linear Regression: {'MSE': 19.429588859078947, 'R2': 0.8635241931513883, 'MAE': 2.2766407800611366}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-54.07974606 -51.19315722 -40.54849456 -48.94769335 -53.84198898]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 43.37672615914027, 'R2': 0.6953165739143397, 'MAE': 2.962100774173018}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-25.34184357 -28.3440171  -23.61153897 -29.7481243  -24.54579391]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 21.81065242102241, 'R2': 0.8467993116764919, 'MAE': 2.321772318679502}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-24.82032211 -26.47946061 -21.39733588 -26.91067042 -21.58276627]\n",
            "Test set scores for Support Vector Machine: {'MSE': 18.170550474815826, 'R2': 0.8723678326432961, 'MAE': 1.7951476986884787}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-25.63029921 -28.80399072 -23.44181371 -27.93867274 -23.83366589]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 20.35304249760741, 'R2': 0.8570377419290008, 'MAE': 2.142998370450842}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-25.4974115  -27.38337699 -27.89725815 -31.07571122 -25.03383419]\n",
            "Test set scores for Neural Network: {'MSE': 23.069344048402883, 'R2': 0.8379581078473088, 'MAE': 2.3256039424274384}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.96797653 0.96799236 0.97151443 0.97071936 0.97149989]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9784537389100126, 'F1-Score': 0.9775274697649109}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.96956077 0.96561825 0.96397732 0.96002488 0.96949609]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9778200253485425, 'F1-Score': 0.9768940730092718}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.97115918 0.96719893 0.9715135  0.968341   0.97389082]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943468676498}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.96957554 0.96719893 0.97230537 0.96913274 0.97309817]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.96878437 0.96601144 0.97072171 0.968341   0.97309888]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.96757129 0.96007112 0.9540769  0.95923562 0.9683502 ]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9746514575411914, 'F1-Score': 0.9743385416793932}\n",
            "Best regression model saved to best_models/predict_total_spa_DT_more_restricted_regression_Support Vector Machine.pkl\n",
            "Best classification model saved to best_models/predict_total_spa_DT_more_restricted_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 869.99 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-24.82032211, -26.47946061, -21.39733588, -26.91067042,\n",
            "       -21.58276627]), 'dataset': 'predict_total_spa_DT_more_restricted', 'test_scores': {'MSE': 18.170550474815826, 'R2': 0.8723678326432961, 'MAE': 1.7951476986884787}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.97115918, 0.96719893, 0.9715135 , 0.968341  , 0.97389082]), 'dataset': 'predict_total_spa_DT_more_restricted', 'test_scores': {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943468676498}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Several Imputations and Test - T4 GPU"
      ],
      "metadata": {
        "id": "FMdIrrTmzRp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_SPA' using the original dataset without feature selection\n",
        "original_total_spa_results, original_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILbsloAUQxya",
        "outputId": "bfbbf825-6529-494b-fd5f-fb657e504190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_SPA\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-41.08611485 -41.67571183 -36.9647161  -43.15098941 -37.40749107]\n",
            "Test set scores for Linear Regression: {'MSE': 34.177352829641045, 'R2': 0.7599340965367161, 'MAE': 4.224361155398544}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-39.5946318  -39.34141741 -40.16540616 -40.50519759 -35.59599497]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 37.9212650625926, 'R2': 0.7336363994280253, 'MAE': 2.768247261583435}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-28.74651896 -29.11211397 -27.96492396 -28.80380947 -24.25517222]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 25.188374261915428, 'R2': 0.8230737806377509, 'MAE': 2.4438262057828464}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-213.75737847 -196.57137309 -194.36148904 -191.16869912 -186.77731646]\n",
            "Test set scores for Support Vector Machine: {'MSE': 195.48961308120127, 'R2': -0.37314293520488806, 'MAE': 11.475205020915542}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-26.93646837 -28.89940677 -25.12190774 -28.42075046 -24.32380869]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 22.586019821008254, 'R2': 0.8413530362928657, 'MAE': 2.269835234474018}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-82.3020639  -86.00694043 -78.14597951 -85.01301316 -84.6056657 ]\n",
            "Test set scores for Neural Network: {'MSE': 66.99328643801529, 'R2': 0.529430968077535, 'MAE': 6.221418998203947}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.8207612  0.80476117 0.79225324 0.82216167 0.84456148 0.80537617\n",
            " 0.79743278 0.78853611 0.80504814 0.82726506]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8409378960709759, 'F1-Score': 0.8400787229838055}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.96289469 0.97554262 0.96675164 0.96992612 0.97151443 0.96676492\n",
            " 0.97151244 0.96675164 0.97785155 0.96517288]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9740177439797212, 'F1-Score': 0.9730938817399066}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.96289469 0.97554262 0.96675164 0.96992612 0.97151443 0.97151443\n",
            " 0.97151244 0.96675588 0.97785208 0.96834266]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.1}\n",
            "Cross-validation scores for Support Vector Machine: [0.35284975 0.35284975 0.35377785 0.35377785 0.35377785 0.35377785\n",
            " 0.35377785 0.35377785 0.35377785 0.35377785]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.5069708491761724, 'F1-Score': 0.3411073838023027}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.9628976  0.97554538 0.96675164 0.96834266 0.97309817 0.96992883\n",
            " 0.97309817 0.96358042 0.97626801 0.9683392 ]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9790874524714829, 'F1-Score': 0.978160857400545}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.83258125 0.78928414 0.71440939 0.84650487 0.79505863 0.72931409\n",
            " 0.81495839 0.80098177 0.84155142 0.56792108]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6520912547528517, 'F1-Score': 0.6089055099315587}\n",
            "Best regression model saved to best_models/combined_df_Total_SPA_regression_k-Nearest Neighbors.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_SPA_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 658.21 seconds\n",
            "Best regression model: {'model_name': 'k-Nearest Neighbors', 'params': array([-26.93646837, -28.89940677, -25.12190774, -28.42075046,\n",
            "       -24.32380869]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'MSE': 22.586019821008254, 'R2': 0.8413530362928657, 'MAE': 2.269835234474018}}\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.96289469, 0.97554262, 0.96675164, 0.96992612, 0.97151443,\n",
            "       0.97151443, 0.97151244, 0.96675588, 0.97785208, 0.96834266]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_SPA' using the original dataset without feature selection\n",
        "original_total_spa_results_mean, original_total_spa_best_model_mean = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    imputation_strategy='mean',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac4319AHzRqE",
        "outputId": "5fad5bcc-f174-417b-d916-aa564879611b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_SPA\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-33.19368593 -34.65878617 -30.44891976 -36.98840841 -30.67205946]\n",
            "Test set scores for Linear Regression: {'MSE': 2.275338377761459e+22, 'R2': -1.5982254859371577e+20, 'MAE': 3797252677.374121}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-54.65277635 -53.76001448 -38.70836598 -52.83177034 -50.58113865]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 40.15441479516395, 'R2': 0.7179504827688046, 'MAE': 2.833893365119601}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-25.69775434 -27.32801441 -22.56326827 -29.03369063 -25.94917141]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 22.331571831298447, 'R2': 0.8431403100714576, 'MAE': 2.343356383278938}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-27.42207502 -28.98488347 -24.4879071  -30.60511563 -24.71242633]\n",
            "Test set scores for Support Vector Machine: {'MSE': 21.053435342803215, 'R2': 0.8521180969816798, 'MAE': 2.5134776000711048}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-25.70492919 -29.34319366 -23.9148397  -29.3281383  -23.71007468]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 20.959237196399474, 'R2': 0.8527797562750049, 'MAE': 2.1621953648379506}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-24.68830216 -26.21772728 -24.00975472 -27.71894821 -22.13344614]\n",
            "Test set scores for Neural Network: {'MSE': 20.2329057519193, 'R2': 0.8578815971139391, 'MAE': 2.3117501652125316}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.96403896 0.96087548 0.96993402 0.9667595  0.9659657 ]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9784537389100126, 'F1-Score': 0.9775274831490649}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.96876872 0.96403661 0.96913939 0.96675777 0.96751072]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9790874524714829, 'F1-Score': 0.9781608034323934}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.97115918 0.96719893 0.97309817 0.96992612 0.97309888]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.97036743 0.96719893 0.97230537 0.96913274 0.97230626]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9803548795944234, 'F1-Score': 0.9794278143300347}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.97036795 0.9656169  0.97230626 0.96913274 0.97389082]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9790874524714829, 'F1-Score': 0.978160857400545}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.96969396 0.96204372 0.96557227 0.9643853  0.97151524]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9778200253485425, 'F1-Score': 0.977208977477901}\n",
            "Best regression model saved to best_models/combined_df_Total_SPA_regression_Neural Network.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_SPA_classification_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 532.84 seconds\n",
            "Best regression model: {'model_name': 'Neural Network', 'params': array([-24.68830216, -26.21772728, -24.00975472, -27.71894821,\n",
            "       -22.13344614]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'MSE': 20.2329057519193, 'R2': 0.8578815971139391, 'MAE': 2.3117501652125316}}\n",
            "Best classification model: {'model_name': 'Support Vector Machine', 'params': array([0.97036743, 0.96719893, 0.97230537, 0.96913274, 0.97230626]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'Accuracy': 0.9803548795944234, 'F1-Score': 0.9794278143300347}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_SPA' using the original dataset without feature selection\n",
        "original_total_spa_results_median, original_total_spa_best_model_median = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    imputation_strategy='median',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn5h8wiDzRqE",
        "outputId": "90378eb8-314c-48ba-a558-865a3b37fa77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_SPA\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-29.0716771  -31.28328598 -26.89764604 -33.48949842 -27.88004546]\n",
            "Test set scores for Linear Regression: {'MSE': 2.583650379924717e+23, 'R2': -1.814787604474627e+21, 'MAE': 12795680070.339197}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-58.46391576 -50.38579052 -37.8712312  -56.8469405  -52.0754671 ]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 36.76391596115539, 'R2': 0.7417657609688051, 'MAE': 2.7030865371074495}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-25.73534395 -27.65642793 -22.81891537 -28.85548684 -26.32222066]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 22.692999356024654, 'R2': 0.8406015989637722, 'MAE': 2.375796697555174}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-26.84769263 -29.61688432 -24.15971115 -30.24473831 -24.70984871]\n",
            "Test set scores for Support Vector Machine: {'MSE': 19.89056262435971, 'R2': 0.8602862570834134, 'MAE': 2.3427096724262677}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-25.70787844 -29.71142711 -24.03417122 -29.3898661  -23.54607334]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 20.79742637153721, 'R2': 0.8539163352855071, 'MAE': 2.164824370812964}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-23.69373486 -25.75096794 -23.78111678 -27.88005421 -23.04822348]\n",
            "Test set scores for Neural Network: {'MSE': 19.98920859522706, 'R2': 0.8595933557274388, 'MAE': 2.3183536775610945}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.96562137 0.96561825 0.96991575 0.96913435 0.96914029]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943468676498}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.96483195 0.96600907 0.969931   0.96992612 0.968336  ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9771863117870723, 'F1-Score': 0.9762605136288363}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.97036743 0.96719893 0.97230537 0.96913274 0.97309817]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.97036743 0.96719893 0.97230537 0.96913274 0.97230626]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9803548795944234, 'F1-Score': 0.9794278143300347}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.97036795 0.9656169  0.97230626 0.96992612 0.97389082]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9784537389100126, 'F1-Score': 0.9775274302482431}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.9675951  0.96403661 0.96478035 0.96834266 0.97072353]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9727503168567807, 'F1-Score': 0.9718274678087618}\n",
            "Best regression model saved to best_models/combined_df_Total_SPA_regression_Support Vector Machine.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_SPA_classification_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 535.69 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-26.84769263, -29.61688432, -24.15971115, -30.24473831,\n",
            "       -24.70984871]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'MSE': 19.89056262435971, 'R2': 0.8602862570834134, 'MAE': 2.3427096724262677}}\n",
            "Best classification model: {'model_name': 'Support Vector Machine', 'params': array([0.97036743, 0.96719893, 0.97230537, 0.96913274, 0.97230626]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'Accuracy': 0.9803548795944234, 'F1-Score': 0.9794278143300347}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_SPA' using the original dataset without feature selection\n",
        "original_total_spa_results_knn, original_total_spa_best_model_knn = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    imputation_strategy='knn',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqfuSOAizRqE",
        "outputId": "1cee28a8-02c5-42d2-81bd-59041582a12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_SPA\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-38.22684684 -39.21160438 -34.09947624 -40.99316432 -35.53946991]\n",
            "Test set scores for Linear Regression: {'MSE': 1.5892364841299968e+23, 'R2': -1.116299130249192e+21, 'MAE': 10035540409.853456}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-45.4634289  -44.25665867 -54.64277808 -58.09473738 -51.50672337]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 44.75245207366756, 'R2': 0.6856533069481978, 'MAE': 3.0363424668557752}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-24.96588457 -26.41139283 -23.62259921 -30.15763343 -24.19007843]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 22.078871457180824, 'R2': 0.8449153083845349, 'MAE': 2.291534144904619}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-33.13464708 -34.23041755 -29.09623401 -35.91963301 -29.69968626]\n",
            "Test set scores for Support Vector Machine: {'MSE': 25.423442497319016, 'R2': 0.8214226326220175, 'MAE': 3.1218971355353378}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-27.31155485 -34.17723013 -25.73509847 -33.68207226 -26.18134033]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 22.12849302009777, 'R2': 0.8445667604618126, 'MAE': 2.3042667028788704}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-25.51704665 -27.69248728 -26.48865609 -29.31692877 -23.04199853]\n",
            "Test set scores for Neural Network: {'MSE': 24.44330744454475, 'R2': 0.8283072210257227, 'MAE': 2.9884776653927663}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.9672031  0.96324589 0.97151689 0.96755104 0.96676492]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9784537389100126, 'F1-Score': 0.9775274831490649}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.97115952 0.96482739 0.96597111 0.96596804 0.97309948]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9778200253485425, 'F1-Score': 0.9768940730092718}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.97036795 0.96640788 0.9715135  0.968341   0.97309817]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943468676498}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.3}\n",
            "Cross-validation scores for Support Vector Machine: [0.96957623 0.96640788 0.9715135  0.9675475  0.97230626]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9803548795944234, 'F1-Score': 0.9794278143300347}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.9648306  0.95848811 0.96279671 0.95962716 0.96636271]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9784537389100126, 'F1-Score': 0.9778431418127835}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.9679727  0.96124811 0.95844346 0.95726391 0.97072353]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.967680608365019, 'F1-Score': 0.9667607297214186}\n",
            "Best regression model saved to best_models/combined_df_Total_SPA_regression_Random Forest Regressor.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_SPA_classification_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 574.96 seconds\n",
            "Best regression model: {'model_name': 'Random Forest Regressor', 'params': array([-24.96588457, -26.41139283, -23.62259921, -30.15763343,\n",
            "       -24.19007843]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'MSE': 22.078871457180824, 'R2': 0.8449153083845349, 'MAE': 2.291534144904619}}\n",
            "Best classification model: {'model_name': 'Support Vector Machine', 'params': array([0.96957623, 0.96640788, 0.9715135 , 0.9675475 , 0.97230626]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'Accuracy': 0.9803548795944234, 'F1-Score': 0.9794278143300347}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_SPA' using the original dataset without feature selection\n",
        "original_total_spa_results_zero, original_total_spa_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lykM2yGzRqE",
        "outputId": "10d0b9c7-356b-4d28-abb3-535941264e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_SPA\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-23.77778247 -25.96166354 -22.84757956 -27.05530389 -22.61638882]\n",
            "Test set scores for Linear Regression: {'MSE': 3.7105348177551872e+22, 'R2': -2.6063250064932726e+20, 'MAE': 4849140373.876287}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-52.4107541  -51.59531669 -50.85697219 -51.06913342 -50.34671743]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 48.39383574753212, 'R2': 0.66007578296967, 'MAE': 3.056277134005271}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-26.59775648 -27.84029868 -22.85979511 -28.94598416 -25.68754438]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 23.021639703317284, 'R2': 0.8382931889976595, 'MAE': 2.3835224358333114}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-24.72281296 -26.44427244 -21.60131558 -26.93211233 -21.88001888]\n",
            "Test set scores for Support Vector Machine: {'MSE': 18.39552211091043, 'R2': 0.8707876043751247, 'MAE': 1.8099829075860407}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-26.18969692 -28.93459232 -23.37588687 -28.63775934 -24.35988502]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 21.089457873567678, 'R2': 0.851865070322883, 'MAE': 2.1643880137606373}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-24.67312281 -28.62289748 -26.99529155 -29.44898468 -23.6860717 ]\n",
            "Test set scores for Neural Network: {'MSE': 21.958751565345594, 'R2': 0.8457590451859505, 'MAE': 2.3099840917466006}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.96641134 0.96640918 0.97309948 0.96992612 0.97070883]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9790874524714829, 'F1-Score': 0.9781608856574516}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.96560769 0.96482739 0.96439196 0.9659426  0.96552275]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9784537389100126, 'F1-Score': 0.9775274697649109}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.97115918 0.96719893 0.97230626 0.96913274 0.97230703]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9790874524714829, 'F1-Score': 0.978160857400545}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.3}\n",
            "Cross-validation scores for Support Vector Machine: [0.96957554 0.96719893 0.97230537 0.9683392  0.97309817]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.96878437 0.9656169  0.9718907  0.96913274 0.97230703]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9771863117870723, 'F1-Score': 0.9765755289025614}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.96600044 0.96363779 0.96122811 0.95645394 0.96991782]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9727503168567807, 'F1-Score': 0.9721380883301016}\n",
            "Best regression model saved to best_models/combined_df_Total_SPA_regression_Support Vector Machine.pkl\n",
            "Best classification model saved to best_models/combined_df_Total_SPA_classification_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 602.11 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-24.72281296, -26.44427244, -21.60131558, -26.93211233,\n",
            "       -21.88001888]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'MSE': 18.39552211091043, 'R2': 0.8707876043751247, 'MAE': 1.8099829075860407}}\n",
            "Best classification model: {'model_name': 'Support Vector Machine', 'params': array([0.96957554, 0.96719893, 0.97230537, 0.9683392 , 0.97309817]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AI1x86KJdmb"
      },
      "source": [
        "#### More Restricted - L4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Total_SPA' using the original dataset without feature selection\n",
        "original_total_spa_results, original_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,  # Use the main dataset dictionary\n",
        "    target_column='Total_SPA',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNmuk2wSSqNO",
        "outputId": "96c14a7b-33d0-4640-f8b7-fa2ac8f76809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Total_SPA\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-41.08611485 -41.67571183 -36.9647161  -43.15098941 -37.40749107]\n",
            "Test set scores for Linear Regression: {'MSE': 34.17735282964122, 'R2': 0.7599340965367148, 'MAE': 4.224361155398571}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-39.5946318  -39.34141741 -40.16540616 -40.50519759 -35.59599497]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 37.9212650625926, 'R2': 0.7336363994280253, 'MAE': 2.768247261583435}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-28.74651896 -29.11211397 -27.96492396 -28.80380947 -24.25517222]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 25.188374261915428, 'R2': 0.8230737806377509, 'MAE': 2.4438262057828464}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-213.75737847 -196.57137309 -194.36148904 -191.16869912 -186.77731646]\n",
            "Test set scores for Support Vector Machine: {'MSE': 195.48961308120127, 'R2': -0.37314293520488806, 'MAE': 11.475205020915542}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-26.93646837 -28.89940677 -25.12190774 -28.42075046 -24.32380869]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 22.586019821008254, 'R2': 0.8413530362928657, 'MAE': 2.269835234474018}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-82.3020639  -86.00694043 -78.14597951 -85.01301316 -84.6056657 ]\n",
            "Test set scores for Neural Network: {'MSE': 66.99328643801535, 'R2': 0.5294309680775345, 'MAE': 6.221418998203948}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.81129165 0.80053417 0.82097967 0.79150984 0.81532018]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8409378960709759, 'F1-Score': 0.8400787229838055}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.97036795 0.96719893 0.97072268 0.96992612 0.97230703]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9740177439797212, 'F1-Score': 0.9730938817399066}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.97036795 0.96719893 0.97151443 0.96992612 0.97309888]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.1}\n",
            "Cross-validation scores for Support Vector Machine: [0.35331333 0.35331333 0.35377785 0.35377785 0.35377785]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.5069708491761724, 'F1-Score': 0.3411073838023027}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.97036743 0.96640788 0.9715135  0.968341   0.97230626]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9790874524714829, 'F1-Score': 0.978160857400545}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.78222638 0.80284737 0.73653151 0.77436409 0.44177104]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.6520912547528517, 'F1-Score': 0.6089055099315587}\n",
            "Best model saved to best_models/combined_df_Total_SPA_Random Forest.pkl\n",
            "Processing complete for 'combined_df_Total_SPA'. Total time: 318.86 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.97036795, 0.96719893, 0.97151443, 0.96992612, 0.97309888]), 'dataset': 'combined_df_Total_SPA', 'test_scores': {'Accuracy': 0.9797211660329531, 'F1-Score': 0.9787943093094502}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n-WW4gtDLDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a980c1ba-2d34-41d8-e767-c69b3b0897f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_spa_MI_more_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-56.59756112 -55.74291067 -53.33120426 -61.53141922 -56.95883629]\n",
            "Test set scores for Linear Regression: {'MSE': 47.30797397516009, 'R2': 0.667703008773848, 'MAE': 4.817974382758264}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-42.18349978 -46.46342    -44.54367989 -44.4359919  -39.25385382]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 36.90605904563757, 'R2': 0.7407673305705421, 'MAE': 2.8564817144607346}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-31.51557319 -32.90186547 -29.98566509 -33.20265987 -27.55961578]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 26.15782857908376, 'R2': 0.8162642150263513, 'MAE': 2.587995407607544}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-40.46082367 -41.20267962 -38.63609608 -44.16021712 -40.38021612]\n",
            "Test set scores for Support Vector Machine: {'MSE': 32.42387869816494, 'R2': 0.7722507131474452, 'MAE': 3.1424735602522262}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-33.87202454 -33.54205868 -27.44518473 -35.13980608 -30.10028885]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 26.22202731176121, 'R2': 0.815813275281596, 'MAE': 2.6229657794676813}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-28.96123895 -29.78770048 -27.97668241 -30.33103233 -24.99748774]\n",
            "Test set scores for Neural Network: {'MSE': 24.565893041553238, 'R2': 0.8274461648098093, 'MAE': 2.8216812176613124}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.91110778 0.91504905 0.90666013 0.89872055 0.90030321]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9277566539923955, 'F1-Score': 0.9268409379725897}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.95849439 0.96245525 0.95806114 0.96279089 0.96043702]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.967680608365019, 'F1-Score': 0.9667572974113136}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.9600862  0.96245377 0.96360225 0.96200979 0.95964594]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9714828897338403, 'F1-Score': 0.9705604208936631}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.94270228 0.94902166 0.93907511 0.94540699 0.94065832]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9581749049429658, 'F1-Score': 0.9572636197915327}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.94823404 0.95613348 0.95806224 0.95489336 0.95252697]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9651457541191382, 'F1-Score': 0.9642270113177007}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.95771397 0.95532971 0.96201616 0.95804793 0.96438979]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9689480354879595, 'F1-Score': 0.9680273521430783}\n",
            "Best model saved to best_models/predict_total_spa_MI_more_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_spa_MI_more_restricted'. Total time: 369.24 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96245377, 0.96360225, 0.96200979, 0.95964594]), 'dataset': 'predict_total_spa_MI_more_restricted', 'test_scores': {'Accuracy': 0.9714828897338403, 'F1-Score': 0.9705604208936631}}\n"
          ]
        }
      ],
      "source": [
        "mi_more_restricted_total_spa_results, mi_more_restricted_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_spa_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    feature_set_name='MI',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmrjyzd_DLDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0bba9cd-3f8b-4baf-c937-b89e565b1245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_spa_RF_more_restricted_regression\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-78.61993623 -77.37373247 -76.00180209 -82.81804374 -76.86288781]\n",
            "Test set scores for Linear Regression: {'MSE': 70.14243107983546, 'R2': 0.5073109912219997, 'MAE': 6.528072441576014}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-76.39686447 -69.09319427 -72.62431406 -76.19739952 -66.6327697 ]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 59.52850465565976, 'R2': 0.58186450766938, 'MAE': 4.293839737891116}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-55.37039213 -55.12268797 -55.95231631 -58.66483712 -51.37243039]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 48.98330677904622, 'R2': 0.6559352663986167, 'MAE': 4.225455512449944}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-66.88525812 -64.38361322 -62.09180149 -71.04954044 -62.91264344]\n",
            "Test set scores for Support Vector Machine: {'MSE': 57.35462753094939, 'R2': 0.597134086286632, 'MAE': 5.27333069443393}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-61.1754089  -59.72075355 -55.24719985 -63.84130066 -57.01695115]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 51.06735103463439, 'R2': 0.641296684833536, 'MAE': 4.498465507876154}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-58.76470058 -57.14255241 -57.487462   -61.12433601 -55.53992388]\n",
            "Test set scores for Neural Network: {'MSE': 51.73928321780943, 'R2': 0.6365769510547374, 'MAE': 4.767597812587864}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.84003153 0.83352391 0.82589566 0.82915483 0.83302466]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8599493029150824, 'F1-Score': 0.8591312281581679}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': None}\n",
            "Cross-validation scores for Decision Tree: [0.87725593 0.8689769  0.87006898 0.8716754  0.88722412]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.885297845373891, 'F1-Score': 0.8861427080578739}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.89678937 0.88812537 0.88883019 0.88609965 0.8935414 ]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.903041825095057, 'F1-Score': 0.9021863885942843}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.86740594 0.88422056 0.87492909 0.86230757 0.87490568]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9024081115335868, 'F1-Score': 0.90141983928408}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.88031525 0.88575789 0.88053379 0.87582007 0.88009071]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8910012674271229, 'F1-Score': 0.890150555988589}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.89296201 0.88816421 0.88925338 0.87021234 0.88208351]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8910012674271229, 'F1-Score': 0.8901339335261}\n",
            "Best model saved to best_models/predict_total_spa_RF_more_restricted_regression_Random Forest.pkl\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_spa_RF_more_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-56.59756112 -55.74291067 -53.33120426 -61.53141922 -56.95883629]\n",
            "Test set scores for Linear Regression: {'MSE': 47.30797397516009, 'R2': 0.667703008773848, 'MAE': 4.817974382758264}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-42.18349978 -46.46342    -44.54367989 -44.4359919  -39.25385382]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 36.90605904563757, 'R2': 0.7407673305705421, 'MAE': 2.8564817144607346}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-31.51557319 -32.90186547 -29.98566509 -33.20265987 -27.55961578]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 26.15782857908376, 'R2': 0.8162642150263513, 'MAE': 2.587995407607544}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-40.46082367 -41.20267962 -38.63609608 -44.16021712 -40.38021612]\n",
            "Test set scores for Support Vector Machine: {'MSE': 32.42387869816494, 'R2': 0.7722507131474452, 'MAE': 3.1424735602522262}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-33.87202454 -33.54205868 -27.44518473 -35.13980608 -30.10028885]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 26.22202731176121, 'R2': 0.815813275281596, 'MAE': 2.6229657794676813}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-28.96123895 -29.78770048 -27.97668241 -30.33103233 -24.99748774]\n",
            "Test set scores for Neural Network: {'MSE': 24.565893041553238, 'R2': 0.8274461648098093, 'MAE': 2.8216812176613124}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.91110778 0.91504905 0.90666013 0.89872055 0.90030321]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9277566539923955, 'F1-Score': 0.9268409379725897}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.95849439 0.96245525 0.95806114 0.96279089 0.96043702]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.967680608365019, 'F1-Score': 0.9667572974113136}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.9600862  0.96245377 0.96360225 0.96200979 0.95964594]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9714828897338403, 'F1-Score': 0.9705604208936631}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.94270228 0.94902166 0.93907511 0.94540699 0.94065832]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9581749049429658, 'F1-Score': 0.9572636197915327}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.94823404 0.95613348 0.95806224 0.95489336 0.95252697]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9651457541191382, 'F1-Score': 0.9642270113177007}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.95771397 0.95532971 0.96201616 0.95804793 0.96438979]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9689480354879595, 'F1-Score': 0.9680273521430783}\n",
            "Best model saved to best_models/predict_total_spa_RF_more_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_spa_RF_more_restricted'. Total time: 753.39 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96245377, 0.96360225, 0.96200979, 0.95964594]), 'dataset': 'predict_total_spa_RF_more_restricted', 'test_scores': {'Accuracy': 0.9714828897338403, 'F1-Score': 0.9705604208936631}}\n"
          ]
        }
      ],
      "source": [
        "rf_more_restricted_total_spa_results, rf_more_restricted_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_spa_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbN0SzFMDLDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481335e9-099d-467c-a7f7-31e6639b3556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_spa_DT_more_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-56.82940228 -55.843724   -53.13801577 -61.45601849 -57.04445572]\n",
            "Test set scores for Linear Regression: {'MSE': 47.2979657979218, 'R2': 0.6677733074339784, 'MAE': 4.815260844611008}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-41.30818841 -47.7466404  -42.08813624 -40.4501978  -42.0981799 ]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 37.78231507078805, 'R2': 0.7346124011530578, 'MAE': 2.917927608628941}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-31.16059657 -33.34957404 -30.38637924 -32.36880139 -28.20297734]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 26.088870718463657, 'R2': 0.8167485834674408, 'MAE': 2.605227204267164}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-40.34388897 -41.06716394 -38.45121213 -43.9583644  -40.16211307]\n",
            "Test set scores for Support Vector Machine: {'MSE': 32.18358543958422, 'R2': 0.773938562364582, 'MAE': 3.1290076555018453}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-33.27759857 -33.87308037 -27.80090438 -34.42098043 -29.90916236]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 26.432162692377332, 'R2': 0.8143372586852065, 'MAE': 2.661159695817491}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-28.0515662  -30.77017446 -28.22488062 -30.43200244 -24.9131715 ]\n",
            "Test set scores for Neural Network: {'MSE': 23.450385692998278, 'R2': 0.8352816247644061, 'MAE': 2.5040938777167554}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.9118982  0.91504905 0.9050782  0.8963503  0.90267334]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9271229404309252, 'F1-Score': 0.9262095818824136}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.95929463 0.96245525 0.95964422 0.96199846 0.96122731]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.967680608365019, 'F1-Score': 0.9667572974113136}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.95929463 0.96245377 0.96360225 0.96121437 0.95885415]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.973384030418251, 'F1-Score': 0.9724607202774468}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.94270228 0.94981162 0.9438241  0.94857014 0.94144984]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9575411913814955, 'F1-Score': 0.9566306658512903}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.94902415 0.95613348 0.95885415 0.95489336 0.95173583]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9645120405576679, 'F1-Score': 0.9635936540310527}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.96245525 0.95532694 0.96122516 0.96121869 0.95964422]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9689480354879595, 'F1-Score': 0.9680276473161744}\n",
            "Best model saved to best_models/predict_total_spa_DT_more_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_spa_DT_more_restricted'. Total time: 368.87 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.95929463, 0.96245377, 0.96360225, 0.96121437, 0.95885415]), 'dataset': 'predict_total_spa_DT_more_restricted', 'test_scores': {'Accuracy': 0.973384030418251, 'F1-Score': 0.9724607202774468}}\n"
          ]
        }
      ],
      "source": [
        "dt_more_restricted_total_spa_results, dt_more_restricted_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=dt_total_spa_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    feature_set_name='DT',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZtOVYLNDLDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402c25b5-9ab4-464d-e077-f2b14df3ff57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_spa_Lasso_more_restricted_regression\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-78.61993623 -77.37373247 -76.00180209 -82.81804374 -76.86288781]\n",
            "Test set scores for Linear Regression: {'MSE': 70.14243107983546, 'R2': 0.5073109912219997, 'MAE': 6.528072441576014}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-76.39686447 -69.09319427 -72.62431406 -76.19739952 -66.6327697 ]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 59.52850465565976, 'R2': 0.58186450766938, 'MAE': 4.293839737891116}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-55.37039213 -55.12268797 -55.95231631 -58.66483712 -51.37243039]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 48.98330677904622, 'R2': 0.6559352663986167, 'MAE': 4.225455512449944}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-66.88525812 -64.38361322 -62.09180149 -71.04954044 -62.91264344]\n",
            "Test set scores for Support Vector Machine: {'MSE': 57.35462753094939, 'R2': 0.597134086286632, 'MAE': 5.27333069443393}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-61.1754089  -59.72075355 -55.24719985 -63.84130066 -57.01695115]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 51.06735103463439, 'R2': 0.641296684833536, 'MAE': 4.498465507876154}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-58.76470058 -57.14255241 -57.487462   -61.12433601 -55.53992388]\n",
            "Test set scores for Neural Network: {'MSE': 51.73928321780943, 'R2': 0.6365769510547374, 'MAE': 4.767597812587864}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.84003153 0.83352391 0.82589566 0.82915483 0.83302466]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8599493029150824, 'F1-Score': 0.8591312281581679}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': None}\n",
            "Cross-validation scores for Decision Tree: [0.87725593 0.8689769  0.87006898 0.8716754  0.88722412]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.885297845373891, 'F1-Score': 0.8861427080578739}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.89678937 0.88812537 0.88883019 0.88609965 0.8935414 ]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.903041825095057, 'F1-Score': 0.9021863885942843}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.86740594 0.88422056 0.87492909 0.86230757 0.87490568]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9024081115335868, 'F1-Score': 0.90141983928408}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.88031525 0.88575789 0.88053379 0.87582007 0.88009071]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8910012674271229, 'F1-Score': 0.890150555988589}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.89296201 0.88816421 0.88925338 0.87021234 0.88208351]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.8910012674271229, 'F1-Score': 0.8901339335261}\n",
            "Best model saved to best_models/predict_total_spa_Lasso_more_restricted_regression_Random Forest.pkl\n",
            "Processing complete for 'predict_total_spa_Lasso_more_restricted_regression'. Total time: 385.67 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.89678937, 0.88812537, 0.88883019, 0.88609965, 0.8935414 ]), 'dataset': 'predict_total_spa_Lasso_more_restricted_regression', 'test_scores': {'Accuracy': 0.903041825095057, 'F1-Score': 0.9021863885942843}}\n"
          ]
        }
      ],
      "source": [
        "lasso_more_restricted_total_spa_results, lasso_more_restricted_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=lasso_total_spa_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    feature_set_name='Lasso',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR0NQVAdDLDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8922293-f601-49e2-b5ab-aac4e1c11fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_spa_FR_more_restricted_regression\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-80.1391861  -79.60665495 -77.94896346 -83.61738543 -79.36937864]\n",
            "Test set scores for Linear Regression: {'MSE': 71.7687325766494, 'R2': 0.4958876507402955, 'MAE': 6.657310456381313}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-60.51692391 -64.09356087 -68.74509534 -65.31592644 -62.66279476]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 56.644459542034014, 'R2': 0.6021223930381665, 'MAE': 4.28448592773232}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-53.06443433 -56.06865259 -53.80132658 -56.79321783 -50.92219448]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 47.25421448731048, 'R2': 0.6680806219870383, 'MAE': 4.19550965609624}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-66.38712649 -66.11830799 -62.44355688 -70.01311954 -63.06030546]\n",
            "Test set scores for Support Vector Machine: {'MSE': 57.638048218525945, 'R2': 0.5951433047371539, 'MAE': 5.249704910283858}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-58.07999476 -58.10000484 -55.76492245 -62.32041395 -55.52793229]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 48.83560337032151, 'R2': 0.6569727531939471, 'MAE': 4.402643490856419}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-57.59662827 -58.05437992 -58.90087927 -61.3544563  -57.8511134 ]\n",
            "Test set scores for Neural Network: {'MSE': 52.44475571116313, 'R2': 0.631621626037916, 'MAE': 4.995399606460941}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.81790933 0.81233901 0.81396625 0.81256693 0.82045845]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.8485424588086184, 'F1-Score': 0.8476986041097598}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 10}\n",
            "Cross-validation scores for Decision Tree: [0.87477162 0.87704488 0.86939544 0.88283498 0.8767349 ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.8897338403041825, 'F1-Score': 0.8894025666094999}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.89003925 0.88972364 0.88487105 0.88845516 0.89794004]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9074778200253485, 'F1-Score': 0.9066170749969701}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.85523898 0.87294554 0.87389035 0.85727429 0.86258891]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.8891001267427123, 'F1-Score': 0.8877672955498916}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.88663    0.88184806 0.88688892 0.86946541 0.88135575]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.8954372623574145, 'F1-Score': 0.8945710142127108}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.88583746 0.88813423 0.881353   0.87501454 0.86823662]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9005069708491762, 'F1-Score': 0.8996008645387165}\n",
            "Best model saved to best_models/predict_total_spa_FR_more_restricted_regression_Random Forest.pkl\n",
            "Processing complete for 'predict_total_spa_FR_more_restricted_regression'. Total time: 354.25 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.89003925, 0.88972364, 0.88487105, 0.88845516, 0.89794004]), 'dataset': 'predict_total_spa_FR_more_restricted_regression', 'test_scores': {'Accuracy': 0.9074778200253485, 'F1-Score': 0.9066170749969701}}\n"
          ]
        }
      ],
      "source": [
        "fr_more_restricted_total_spa_results, fr_more_restricted_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=fr_total_spa_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    feature_set_name='FR',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdPEYVaI3w9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50293037-6bc1-4100-f720-132d5c54295c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column to Predict/Classify: Total_SPA. Feature Set: more_restricted\n",
            "Best Model Info using MI Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96245377, 0.96360225, 0.96200979, 0.95964594]), 'dataset': 'predict_total_spa_MI_more_restricted', 'test_scores': {'Accuracy': 0.9714828897338403, 'F1-Score': 0.9705604208936631}}\n",
            "Best Model Info using RF Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96245377, 0.96360225, 0.96200979, 0.95964594]), 'dataset': 'predict_total_spa_RF_more_restricted', 'test_scores': {'Accuracy': 0.9714828897338403, 'F1-Score': 0.9705604208936631}}\n",
            "Best Model Info using DT Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.95929463, 0.96245377, 0.96360225, 0.96121437, 0.95885415]), 'dataset': 'predict_total_spa_DT_more_restricted', 'test_scores': {'Accuracy': 0.973384030418251, 'F1-Score': 0.9724607202774468}}\n",
            "Best Model Info using Lasso Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.89678937, 0.88812537, 0.88883019, 0.88609965, 0.8935414 ]), 'dataset': 'predict_total_spa_Lasso_more_restricted_regression', 'test_scores': {'Accuracy': 0.903041825095057, 'F1-Score': 0.9021863885942843}}\n",
            "Best Model Info using FR Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.89003925, 0.88972364, 0.88487105, 0.88845516, 0.89794004]), 'dataset': 'predict_total_spa_FR_more_restricted_regression', 'test_scores': {'Accuracy': 0.9074778200253485, 'F1-Score': 0.9066170749969701}}\n"
          ]
        }
      ],
      "source": [
        "print(\"Column to Predict/Classify: Total_SPA. Feature Set: more_restricted\")\n",
        "print(\"Best Model Info using MI Feature Selection method:\", mi_more_restricted_total_spa_best_model)\n",
        "print(\"Best Model Info using RF Feature Selection method:\", rf_more_restricted_total_spa_best_model)\n",
        "print(\"Best Model Info using DT Feature Selection method:\", dt_more_restricted_total_spa_best_model)\n",
        "print(\"Best Model Info using Lasso Feature Selection method:\", lasso_more_restricted_total_spa_best_model)\n",
        "print(\"Best Model Info using FR Feature Selection method:\", fr_more_restricted_total_spa_best_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27Vilrj9JtfN"
      },
      "source": [
        "#### Restricted - L4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryIki0aVDLDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe3acf27-b925-47d2-8b73-fb096b594a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_spa_MI_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-56.59756112 -55.74291067 -53.33120426 -61.53141922 -56.95883629]\n",
            "Test set scores for Linear Regression: {'MSE': 47.30797397516009, 'R2': 0.667703008773848, 'MAE': 4.817974382758266}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-41.36129019 -46.32327748 -42.39391761 -44.37885156 -39.23820405]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 36.972598969591935, 'R2': 0.7402999460121151, 'MAE': 2.8659874178827875}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-31.50582472 -32.78780175 -30.02956185 -33.1305952  -27.5811895 ]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 26.190089984990266, 'R2': 0.8160376069682449, 'MAE': 2.5871671647495007}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-40.46082367 -41.20267962 -38.63609608 -44.16021712 -40.38021612]\n",
            "Test set scores for Support Vector Machine: {'MSE': 32.42387869816494, 'R2': 0.7722507131474452, 'MAE': 3.1424735602522262}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-33.17917753 -32.83794355 -27.42215245 -34.94405227 -29.58724593]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 27.290428176974213, 'R2': 0.8083086970233914, 'MAE': 2.6594559116422234}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-28.42379825 -29.75772048 -28.03612899 -31.18322495 -25.04460965]\n",
            "Test set scores for Neural Network: {'MSE': 23.87694596891447, 'R2': 0.8322854132517767, 'MAE': 2.605735199899108}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.91110778 0.91504905 0.90666013 0.89872055 0.90030321]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9277566539923955, 'F1-Score': 0.9268409379725897}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.95770238 0.96245525 0.95885322 0.96279089 0.95964518]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9702154626108999, 'F1-Score': 0.9692886039174753}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.9600862  0.96087094 0.96281039 0.96200979 0.96122811]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9702154626108999, 'F1-Score': 0.969294386244122}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.94270228 0.94902166 0.93907511 0.94540699 0.94065832]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9581749049429658, 'F1-Score': 0.9572636197915327}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.95218519 0.95455314 0.96201625 0.95568414 0.9541092 ]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9626108998732573, 'F1-Score': 0.9616956429663701}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.95928125 0.9545396  0.96122381 0.95805041 0.96281105]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9664131812420785, 'F1-Score': 0.9654921303196758}\n",
            "Best model saved to best_models/predict_total_spa_MI_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_spa_MI_restricted'. Total time: 344.36 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96087094, 0.96281039, 0.96200979, 0.96122811]), 'dataset': 'predict_total_spa_MI_restricted', 'test_scores': {'Accuracy': 0.9702154626108999, 'F1-Score': 0.969294386244122}}\n"
          ]
        }
      ],
      "source": [
        "mi_restricted_total_spa_results, mi_restricted_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_spa_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    feature_set_name='MI',\n",
        "    exclusion_type='restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqYXkv3KDLDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7af19bb-ca6f-40a2-b671-32e608389d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_spa_RF_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-56.59756112 -55.74291067 -53.33120426 -61.53141922 -56.95883629]\n",
            "Test set scores for Linear Regression: {'MSE': 47.30797397516009, 'R2': 0.667703008773848, 'MAE': 4.817974382758266}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-41.36129019 -46.32327748 -42.39391761 -44.37885156 -39.23820405]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 36.972598969591935, 'R2': 0.7402999460121151, 'MAE': 2.8659874178827875}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-31.50582472 -32.78780175 -30.02956185 -33.1305952  -27.5811895 ]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 26.190089984990266, 'R2': 0.8160376069682449, 'MAE': 2.5871671647495007}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-40.46082367 -41.20267962 -38.63609608 -44.16021712 -40.38021612]\n",
            "Test set scores for Support Vector Machine: {'MSE': 32.42387869816494, 'R2': 0.7722507131474452, 'MAE': 3.1424735602522262}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-33.17917753 -32.83794355 -27.42215245 -34.94405227 -29.58724593]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 27.290428176974213, 'R2': 0.8083086970233914, 'MAE': 2.6594559116422234}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-28.42379825 -29.75772048 -28.03612899 -31.18322495 -25.04460965]\n",
            "Test set scores for Neural Network: {'MSE': 23.87694596891447, 'R2': 0.8322854132517767, 'MAE': 2.605735199899108}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.91110778 0.91504905 0.90666013 0.89872055 0.90030321]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9277566539923955, 'F1-Score': 0.9268409379725897}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.95770238 0.96245525 0.95885322 0.96279089 0.95964518]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9702154626108999, 'F1-Score': 0.9692886039174753}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.9600862  0.96087094 0.96281039 0.96200979 0.96122811]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9702154626108999, 'F1-Score': 0.969294386244122}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.94270228 0.94902166 0.93907511 0.94540699 0.94065832]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9581749049429658, 'F1-Score': 0.9572636197915327}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.95218519 0.95455314 0.96201625 0.95568414 0.9541092 ]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9626108998732573, 'F1-Score': 0.9616956429663701}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.95928125 0.9545396  0.96122381 0.95805041 0.96281105]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9664131812420785, 'F1-Score': 0.9654921303196758}\n",
            "Best model saved to best_models/predict_total_spa_RF_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_spa_RF_restricted'. Total time: 344.95 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96087094, 0.96281039, 0.96200979, 0.96122811]), 'dataset': 'predict_total_spa_RF_restricted', 'test_scores': {'Accuracy': 0.9702154626108999, 'F1-Score': 0.969294386244122}}\n"
          ]
        }
      ],
      "source": [
        "rf_restricted_total_spa_results, rf_restricted_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_spa_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='restricted'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buzt7UcnDLDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ebcaa68-3ca8-4d8a-e8d7-be7749021983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: restricted\n",
            "No datasets found for the given criteria: restricted, DT, Total_SPA\n"
          ]
        }
      ],
      "source": [
        "dt_regression_restricted_total_spa_results, dt_regression_restricted_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_spa_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    feature_set_name='DT',\n",
        "    exclusion_type='restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgffjqFQ3pMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87f69732-a912-4cec-cb8e-b5612af643e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column to Predict/Classify: Total_SPA. Feature Set: restricted\n",
            "Best Model Info using MI Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96087094, 0.96281039, 0.96200979, 0.96122811]), 'dataset': 'predict_total_spa_MI_restricted', 'test_scores': {'Accuracy': 0.9702154626108999, 'F1-Score': 0.969294386244122}}\n",
            "Best Model Info using RF Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96087094, 0.96281039, 0.96200979, 0.96122811]), 'dataset': 'predict_total_spa_RF_restricted', 'test_scores': {'Accuracy': 0.9702154626108999, 'F1-Score': 0.969294386244122}}\n"
          ]
        }
      ],
      "source": [
        "print(\"Column to Predict/Classify: Total_SPA. Feature Set: restricted\")\n",
        "print(\"Best Model Info using MI Feature Selection method:\", mi_restricted_total_spa_best_model)\n",
        "print(\"Best Model Info using RF Feature Selection method:\", rf_restricted_total_spa_best_model)\n",
        "#print(\"Best Model Info using DT Feature Selection method:\", dt_restricted_total_spa_best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGz0u3eHJxY3"
      },
      "source": [
        "#### Less Restricted - L4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcIvQl5GDLDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec980bcc-0995-477c-bc0a-a1bf89f1f91a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: less_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_spa_MI_less_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-56.59756112 -55.74291067 -53.33120426 -61.53141922 -56.95883629]\n",
            "Test set scores for Linear Regression: {'MSE': 47.30797397516009, 'R2': 0.667703008773848, 'MAE': 4.817974382758266}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-41.36129019 -46.32327748 -42.39391761 -44.37885156 -39.23820405]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 36.972598969591935, 'R2': 0.7402999460121151, 'MAE': 2.8659874178827875}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-31.50582472 -32.78780175 -30.02956185 -33.1305952  -27.5811895 ]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 26.190089984990266, 'R2': 0.8160376069682449, 'MAE': 2.5871671647495007}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-40.46082367 -41.20267962 -38.63609608 -44.16021712 -40.38021612]\n",
            "Test set scores for Support Vector Machine: {'MSE': 32.42387869816494, 'R2': 0.7722507131474452, 'MAE': 3.1424735602522262}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-33.17917753 -32.83794355 -27.42215245 -34.94405227 -29.58724593]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 27.290428176974213, 'R2': 0.8083086970233914, 'MAE': 2.6594559116422234}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-28.42379825 -29.75772048 -28.03612899 -31.18322495 -25.04460965]\n",
            "Test set scores for Neural Network: {'MSE': 23.87694596891447, 'R2': 0.8322854132517767, 'MAE': 2.605735199899108}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.91110778 0.91504905 0.90666013 0.89872055 0.90030321]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9277566539923955, 'F1-Score': 0.9268409379725897}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.95770238 0.96245525 0.95885322 0.96279089 0.95964518]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9702154626108999, 'F1-Score': 0.9692886039174753}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.9600862  0.96087094 0.96281039 0.96200979 0.96122811]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9702154626108999, 'F1-Score': 0.969294386244122}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.94270228 0.94902166 0.93907511 0.94540699 0.94065832]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9581749049429658, 'F1-Score': 0.9572636197915327}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.95218519 0.95455314 0.96201625 0.95568414 0.9541092 ]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9626108998732573, 'F1-Score': 0.9616956429663701}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.95928125 0.9545396  0.96122381 0.95805041 0.96281105]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9664131812420785, 'F1-Score': 0.9654921303196758}\n",
            "Best model saved to best_models/predict_total_spa_MI_less_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_spa_MI_less_restricted'. Total time: 346.89 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96087094, 0.96281039, 0.96200979, 0.96122811]), 'dataset': 'predict_total_spa_MI_less_restricted', 'test_scores': {'Accuracy': 0.9702154626108999, 'F1-Score': 0.969294386244122}}\n"
          ]
        }
      ],
      "source": [
        "mi_less_restricted_total_spa_results, mi_less_restricted_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_total_spa_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    feature_set_name='MI',\n",
        "    exclusion_type='less_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKTkvZENDLDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a9f111-c174-43b7-a1d6-eed798b0d47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: less_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_spa_RF_less_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-56.59756112 -55.74291067 -53.33120426 -61.53141922 -56.95883629]\n",
            "Test set scores for Linear Regression: {'MSE': 47.30797397516009, 'R2': 0.667703008773848, 'MAE': 4.817974382758266}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-41.36129019 -46.32327748 -42.39391761 -44.37885156 -39.23820405]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 36.972598969591935, 'R2': 0.7402999460121151, 'MAE': 2.8659874178827875}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-31.50582472 -32.78780175 -30.02956185 -33.1305952  -27.5811895 ]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 26.190089984990266, 'R2': 0.8160376069682449, 'MAE': 2.5871671647495007}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-40.46082367 -41.20267962 -38.63609608 -44.16021712 -40.38021612]\n",
            "Test set scores for Support Vector Machine: {'MSE': 32.42387869816494, 'R2': 0.7722507131474452, 'MAE': 3.1424735602522262}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-33.17917753 -32.83794355 -27.42215245 -34.94405227 -29.58724593]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 27.290428176974213, 'R2': 0.8083086970233914, 'MAE': 2.6594559116422234}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-28.42379825 -29.75772048 -28.03612899 -31.18322495 -25.04460965]\n",
            "Test set scores for Neural Network: {'MSE': 23.87694596891447, 'R2': 0.8322854132517767, 'MAE': 2.605735199899108}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.91110778 0.91504905 0.90666013 0.89872055 0.90030321]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9277566539923955, 'F1-Score': 0.9268409379725897}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.95770238 0.96245525 0.95885322 0.96279089 0.95964518]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9702154626108999, 'F1-Score': 0.9692886039174753}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.9600862  0.96087094 0.96281039 0.96200979 0.96122811]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9702154626108999, 'F1-Score': 0.969294386244122}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.94270228 0.94902166 0.93907511 0.94540699 0.94065832]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9581749049429658, 'F1-Score': 0.9572636197915327}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.95218519 0.95455314 0.96201625 0.95568414 0.9541092 ]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9626108998732573, 'F1-Score': 0.9616956429663701}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.95928125 0.9545396  0.96122381 0.95805041 0.96281105]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9664131812420785, 'F1-Score': 0.9654921303196758}\n",
            "Best model saved to best_models/predict_total_spa_RF_less_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_spa_RF_less_restricted'. Total time: 361.32 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96087094, 0.96281039, 0.96200979, 0.96122811]), 'dataset': 'predict_total_spa_RF_less_restricted', 'test_scores': {'Accuracy': 0.9702154626108999, 'F1-Score': 0.969294386244122}}\n"
          ]
        }
      ],
      "source": [
        "rf_less_restricted_total_spa_results, rf_less_restricted_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_total_spa_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='less_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVETz-hGDLDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d0adbd-374a-478b-fdd3-7f07d711a44a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Total_SPA with exclusion type: less_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_total_spa_DT_less_restricted\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-56.82940228 -55.843724   -53.13801577 -61.45601849 -57.04445572]\n",
            "Test set scores for Linear Regression: {'MSE': 47.297965797921805, 'R2': 0.6677733074339784, 'MAE': 4.815260844611011}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-40.97465713 -47.74743217 -42.08813624 -40.4501978  -42.0981799 ]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 37.78231507078805, 'R2': 0.7346124011530578, 'MAE': 2.917927608628941}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-31.18854615 -33.39516573 -30.44958327 -32.34025715 -28.18541582]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 26.127171018330817, 'R2': 0.8164795574800756, 'MAE': 2.60568532803394}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-40.34388897 -41.06716394 -38.45121213 -43.9583644  -40.16211307]\n",
            "Test set scores for Support Vector Machine: {'MSE': 32.18358543958422, 'R2': 0.773938562364582, 'MAE': 3.1290076555018453}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-32.76354969 -33.53748    -27.95250124 -34.97613637 -29.19088881]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 27.149423690540857, 'R2': 0.8092991297697989, 'MAE': 2.67166938258193}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-27.82790062 -30.15185851 -28.14594306 -29.76455378 -25.3264917 ]\n",
            "Test set scores for Neural Network: {'MSE': 23.262414410402283, 'R2': 0.8366019580273856, 'MAE': 2.5856250493334207}\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.9118982  0.91504905 0.9050782  0.8963503  0.90267334]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9271229404309252, 'F1-Score': 0.9262095818824136}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.95929463 0.96245525 0.9604341  0.96199846 0.96122731]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.967680608365019, 'F1-Score': 0.9667572974113136}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.9600862  0.96166317 0.96201924 0.95884131 0.96043804]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9727503168567807, 'F1-Score': 0.971827151316785}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.94270228 0.94981162 0.9438241  0.94857014 0.94144984]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9575411913814955, 'F1-Score': 0.9566306658512903}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.95376557 0.95455314 0.96201625 0.95489154 0.95290398]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9626108998732573, 'F1-Score': 0.9616956429663701}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.95692371 0.95533462 0.96359856 0.96042084 0.96280452]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9683143219264893, 'F1-Score': 0.9673920972929887}\n",
            "Best model saved to best_models/predict_total_spa_DT_less_restricted_Random Forest.pkl\n",
            "Processing complete for 'predict_total_spa_DT_less_restricted'. Total time: 371.80 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96166317, 0.96201924, 0.95884131, 0.96043804]), 'dataset': 'predict_total_spa_DT_less_restricted', 'test_scores': {'Accuracy': 0.9727503168567807, 'F1-Score': 0.971827151316785}}\n"
          ]
        }
      ],
      "source": [
        "dt_less_restricted_total_spa_results, dt_less_restricted_total_spa_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=dt_total_spa_datasets,\n",
        "    target_column='Total_SPA',\n",
        "    feature_set_name='DT',\n",
        "    exclusion_type='less_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocr1WZA03gcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71dc1fa-83fd-4d7f-c2a6-47af3bf72002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column to Predict/Classify: Total_SPA. Feature Set: less_restricted\n",
            "Best Model Info using MI Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96087094, 0.96281039, 0.96200979, 0.96122811]), 'dataset': 'predict_total_spa_MI_less_restricted', 'test_scores': {'Accuracy': 0.9702154626108999, 'F1-Score': 0.969294386244122}}\n",
            "Best Model Info using RF Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96087094, 0.96281039, 0.96200979, 0.96122811]), 'dataset': 'predict_total_spa_RF_less_restricted', 'test_scores': {'Accuracy': 0.9702154626108999, 'F1-Score': 0.969294386244122}}\n",
            "Best Model Info using DT Feature Selection method: {'model_name': 'Random Forest', 'params': array([0.9600862 , 0.96166317, 0.96201924, 0.95884131, 0.96043804]), 'dataset': 'predict_total_spa_DT_less_restricted', 'test_scores': {'Accuracy': 0.9727503168567807, 'F1-Score': 0.971827151316785}}\n"
          ]
        }
      ],
      "source": [
        "print(\"Column to Predict/Classify: Total_SPA. Feature Set: less_restricted\")\n",
        "print(\"Best Model Info using MI Feature Selection method:\", mi_less_restricted_total_spa_best_model)\n",
        "print(\"Best Model Info using RF Feature Selection method:\", rf_less_restricted_total_spa_best_model)\n",
        "print(\"Best Model Info using DT Feature Selection method:\", dt_less_restricted_total_spa_best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJD1F0k9JDrj"
      },
      "source": [
        "### 4.3.4 Predicting Passed feature"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CPU Tests"
      ],
      "metadata": {
        "id": "D97VnPaCmnLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Passed' using the original dataset without feature selection\n",
        "original_passed_results_mean, original_passed_best_model_mean = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,  # Use the main dataset dictionary\n",
        "    target_column='Passed',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='mean',\n",
        "    cv=10,  # Use 10-fold cross-validation\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heKD-1rKmsxv",
        "outputId": "6f62f64e-be0e-489d-851b-37fad43ef6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Passed with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Passed\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.9895015  0.99026909 0.99266178 0.99034072 0.98777583]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9955248976246943}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.98778549 0.98798369 0.98787248 0.98637074 0.98690559]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9936628643852978, 'F1-Score': 0.9936258628759812}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.99018695 0.99267129 0.99103106 0.98931505 0.98922101]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9961977186311787, 'F1-Score': 0.996129328233229}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.99010179 0.99182246 0.9900865  0.99017919 0.98922101]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9954702905160852}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.99104269 0.99182246 0.99095284 0.99188449 0.98931505]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9954702905160852}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.98778549 0.99284662 0.98741867 0.98863829 0.98712095]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.991128010139417, 'F1-Score': 0.9912276906527675}\n",
            "Best classification model saved to best_models/combined_df_Passed_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 272.32 seconds\n",
            "Best regression model: None\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.99018695, 0.99267129, 0.99103106, 0.98931505, 0.98922101]), 'dataset': 'combined_df_Passed', 'test_scores': {'Accuracy': 0.9961977186311787, 'F1-Score': 0.996129328233229}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mi_more_restricted_passed_results, mi_more_restricted_passed_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_passed_datasets,\n",
        "    target_column='Passed',\n",
        "    feature_set_name='MI',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='mean',\n",
        "    cv=10,  # Use 10-fold cross-validation\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLVCewkqm7vF",
        "outputId": "4c9ce1cc-5dc4-41b8-cf4f-192d661a9673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Passed with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_passed_MI_more_restricted\n",
            "Column 'Year' not found. Scaling without grouping.\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.15}\n",
            "Cross-validation scores for Logistic Regression: [0.99273181 0.99273181 0.99266178 0.99272605 0.99103561]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9974651457541192, 'F1-Score': 0.9974503451503924}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.98958587 0.98659102 0.9889065  0.99272605 0.99049128]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9942965779467681, 'F1-Score': 0.9942462969460354}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.99351273 0.99345797 0.99266178 0.9935076  0.99095889]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9961977186311787, 'F1-Score': 0.996129328233229}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.99267129 0.99351273 0.99266178 0.99103561 0.99095889]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9968314321926489, 'F1-Score': 0.9967842550603949}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.99267129 0.99351273 0.99188101 0.99110962 0.99017919]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9968314321926489, 'F1-Score': 0.9967842550603949}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.99279022 0.99042489 0.99188101 0.99356048 0.98949319]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9942965779467681, 'F1-Score': 0.994312907613265}\n",
            "Best classification model saved to best_models/predict_passed_MI_more_restricted_classification_Logistic Regression.pkl\n",
            "Processing complete for datasets. Total time: 193.41 seconds\n",
            "Best regression model: None\n",
            "Best classification model: {'model_name': 'Logistic Regression', 'params': array([0.99273181, 0.99273181, 0.99266178, 0.99272605, 0.99103561]), 'dataset': 'predict_passed_MI_more_restricted', 'test_scores': {'Accuracy': 0.9974651457541192, 'F1-Score': 0.9974503451503924}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### T4 GPU Results"
      ],
      "metadata": {
        "id": "gwYG07vsGRyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Passed' using the original dataset without feature selection\n",
        "original_passed_results, original_passed_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,  # Use the main dataset dictionary\n",
        "    target_column='Passed',\n",
        "    cv=10,  # Use 10-fold cross-validation\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auFDr1hE5moW",
        "outputId": "19d90166-0416-434d-9987-6bde7d00e9af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Passed with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Passed\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.98746001 0.99088652 0.98731786 0.98487063 0.98655168]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9917617237008872, 'F1-Score': 0.9914249158119939}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.98788634 0.98788634 0.98722133 0.98797418 0.98614594]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9923954372623575, 'F1-Score': 0.9922105878911961}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.99026909 0.99267129 0.98787248 0.98701519 0.98767121]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9942965779467681, 'F1-Score': 0.9941760878063955}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.1}\n",
            "Cross-validation scores for Support Vector Machine: [0.9269834  0.9269834  0.9280946  0.92692604 0.92692604]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9480354879594424, 'F1-Score': 0.9227463174933804}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 3}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.99104269 0.99189091 0.98940034 0.98701519 0.98854239]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9936628643852978, 'F1-Score': 0.9935088232426637}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.98958587 0.98482355 0.9900865  0.98584418 0.98873084]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9936628643852978, 'F1-Score': 0.9934676680850968}\n",
            "Best classification model saved to best_models/combined_df_Passed_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 81.30 seconds\n",
            "Best regression model: None\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.99026909, 0.99267129, 0.98787248, 0.98701519, 0.98767121]), 'dataset': 'combined_df_Passed', 'test_scores': {'Accuracy': 0.9942965779467681, 'F1-Score': 0.9941760878063955}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Passed' using the original dataset without feature selection\n",
        "original_passed_results_mean, original_passed_best_model_mean = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,  # Use the main dataset dictionary\n",
        "    target_column='Passed',\n",
        "    cv=10,  # Use 10-fold cross-validation,\n",
        "    imputation_strategy='mean',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNakyHVW5p4d",
        "outputId": "18dee498-38e3-4da5-8db1-f62aa90329f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Passed with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Passed\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.9895015  0.99026909 0.99266178 0.99034072 0.98777583]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9955248976246943}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.98778549 0.98798369 0.98787248 0.98637074 0.98690559]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9936628643852978, 'F1-Score': 0.9936258628759812}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.99018695 0.99267129 0.99103106 0.98931505 0.98922101]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9961977186311787, 'F1-Score': 0.996129328233229}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.99010179 0.99182246 0.9900865  0.99017919 0.98922101]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9954702905160852}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.99104269 0.99182246 0.99095284 0.99188449 0.98931505]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9954702905160852}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.98778549 0.99284662 0.98741867 0.98863829 0.98712095]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.991128010139417, 'F1-Score': 0.9912276906527675}\n",
            "Best classification model saved to best_models/combined_df_Passed_classification_Random Forest.pkl\n",
            "Processing complete for datasets. Total time: 207.48 seconds\n",
            "Best regression model: None\n",
            "Best classification model: {'model_name': 'Random Forest', 'params': array([0.99018695, 0.99267129, 0.99103106, 0.98931505, 0.98922101]), 'dataset': 'combined_df_Passed', 'test_scores': {'Accuracy': 0.9961977186311787, 'F1-Score': 0.996129328233229}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Passed' using the original dataset without feature selection\n",
        "original_passed_results_zero, original_passed_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,  # Use the main dataset dictionary\n",
        "    target_column='Passed',\n",
        "    cv=10,  # Use 10-fold cross-validation,\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDOYFVmP52oo",
        "outputId": "5574abc6-b241-42e1-bfd2-b3a4ca916d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Passed with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Passed\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.99026909 0.99111665 0.99181119 0.99188449 0.98844296]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9954979570845527}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.98798369 0.99111665 0.98881867 0.98787676 0.9875627 ]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9930291508238276, 'F1-Score': 0.9929676962673767}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.99104269 0.99267129 0.99103106 0.98931505 0.98922101]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9954702905160852}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.99018695 0.99267129 0.99095284 0.98931505 0.98922101]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9954702905160852}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.99104269 0.99267129 0.99188101 0.99188449 0.98931505]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9949302915082383, 'F1-Score': 0.9948391043109722}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.99042489 0.9913236  0.98948948 0.98973795 0.98637074]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9917617237008872, 'F1-Score': 0.9917853109969385}\n",
            "Best classification model saved to best_models/combined_df_Passed_classification_Logistic Regression.pkl\n",
            "Processing complete for datasets. Total time: 149.87 seconds\n",
            "Best regression model: None\n",
            "Best classification model: {'model_name': 'Logistic Regression', 'params': array([0.99026909, 0.99111665, 0.99181119, 0.99188449, 0.98844296]), 'dataset': 'combined_df_Passed', 'test_scores': {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9954979570845527}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Passed' using the original dataset without feature selection\n",
        "original_passed_results_median, original_passed_best_model_median = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,  # Use the main dataset dictionary\n",
        "    target_column='Passed',\n",
        "    cv=10,  # Use 10-fold cross-validation,\n",
        "    imputation_strategy='median',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHZVeHTM56ra",
        "outputId": "46a3f2f4-3a44-4e7a-c5e5-32cfe4824d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Passed with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Passed\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.98864728 0.99026909 0.99266178 0.99034072 0.98854239]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9955248976246943}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.98778549 0.98882903 0.98550703 0.98562595 0.98614594]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9917617237008872, 'F1-Score': 0.9916390631570262}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.99018695 0.99267129 0.99017343 0.98854239 0.98679195]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9949302915082383, 'F1-Score': 0.9948070585941308}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.99010179 0.99182246 0.9900865  0.99017919 0.9883398 ]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9954702905160852}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.99104269 0.99104269 0.99095284 0.99188449 0.98931505]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9949302915082383, 'F1-Score': 0.9948070585941308}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [0.98958587 0.99125699 0.98636792 0.98573693 0.98626032]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9923954372623575, 'F1-Score': 0.9924387045492397}\n",
            "Best classification model saved to best_models/combined_df_Passed_classification_Logistic Regression.pkl\n",
            "Processing complete for datasets. Total time: 205.67 seconds\n",
            "Best regression model: None\n",
            "Best classification model: {'model_name': 'Logistic Regression', 'params': array([0.98864728, 0.99026909, 0.99266178, 0.99034072, 0.98854239]), 'dataset': 'combined_df_Passed', 'test_scores': {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9955248976246943}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Passed' using the original dataset without feature selection\n",
        "original_passed_results_knn, original_passed_best_model_knn = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,  # Use the main dataset dictionary\n",
        "    target_column='Passed',\n",
        "    cv=10,  # Use 10-fold cross-validation,\n",
        "    imputation_strategy='knn',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_zOP-Tm5-ry",
        "outputId": "4a5785ec-9e48-4b17-f629-5c727b05239a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Passed with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Passed\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.99026909 0.99111665 0.99103106 0.99034072 0.98679195]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9955248976246943}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.98778549 0.99118804 0.98776963 0.98626032 0.99103561]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9904942965779467, 'F1-Score': 0.9903527651811843}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.98941409 0.99345797 0.99017343 0.98940572 0.9883398 ]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9942965779467681, 'F1-Score': 0.9941760878063955}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.98941409 0.99267129 0.9900865  0.98767121 0.9883398 ]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9954702905160852}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.98941409 0.99267129 0.98940034 0.98940572 0.99009396]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9954702905160852}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.98873975 0.99125699 0.98647646 0.98701519 0.98701519]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9885931558935361, 'F1-Score': 0.9887213165535581}\n",
            "Best classification model saved to best_models/combined_df_Passed_classification_Logistic Regression.pkl\n",
            "Processing complete for datasets. Total time: 183.10 seconds\n",
            "Best regression model: None\n",
            "Best classification model: {'model_name': 'Logistic Regression', 'params': array([0.99026909, 0.99111665, 0.99103106, 0.99034072, 0.98679195]), 'dataset': 'combined_df_Passed', 'test_scores': {'Accuracy': 0.9955640050697085, 'F1-Score': 0.9955248976246943}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### L4 GPU Results"
      ],
      "metadata": {
        "id": "xeTpxN3D5KeJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPPIM59NB1SO"
      },
      "source": [
        "##### More Restricted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'Passed' using the original dataset without feature selection\n",
        "original_passed_results, original_passed_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,  # Use the main dataset dictionary\n",
        "    target_column='Passed',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx6f6GEbSxRj",
        "outputId": "f087e3e5-bd0b-4be1-cd51-d95f8f77c6a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Passed with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: combined_df_Passed\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.1}\n",
            "Cross-validation scores for Logistic Regression: [0.98746001 0.99088652 0.98731786 0.98487063 0.98655168]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9917617237008872, 'F1-Score': 0.9914249158119939}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.98788634 0.98788634 0.98722133 0.98797418 0.98614594]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9923954372623575, 'F1-Score': 0.9922105878911961}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.99026909 0.99267129 0.98787248 0.98701519 0.98767121]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9942965779467681, 'F1-Score': 0.9941760878063955}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 0.1}\n",
            "Cross-validation scores for Support Vector Machine: [0.9269834  0.9269834  0.9280946  0.92692604 0.92692604]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9480354879594424, 'F1-Score': 0.9227463174933804}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 3}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.99104269 0.99189091 0.98940034 0.98701519 0.98854239]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9936628643852978, 'F1-Score': 0.9935088232426637}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.98958587 0.98482355 0.9900865  0.98584418 0.98873084]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9936628643852978, 'F1-Score': 0.9934676680850968}\n",
            "Best model saved to best_models/combined_df_Passed_Random Forest.pkl\n",
            "Processing complete for 'combined_df_Passed'. Total time: 72.92 seconds\n",
            "Best model: {'model_name': 'Random Forest', 'params': array([0.99026909, 0.99267129, 0.98787248, 0.98701519, 0.98767121]), 'dataset': 'combined_df_Passed', 'test_scores': {'Accuracy': 0.9942965779467681, 'F1-Score': 0.9941760878063955}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDDKQjuRtxT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137bfceb-c0a2-4f97-e4f3-d4b36d57c407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Passed with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_passed_MI_more_restricted\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.99195696 0.99273181 0.99181119 0.9926655  0.99181599]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9961977186311787, 'F1-Score': 0.9961755177255888}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.99267129 0.98891528 0.98806746 0.99188449 0.99118106]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9942965779467681, 'F1-Score': 0.9942462969460354}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.99351273 0.99345797 0.99188101 0.9926655  0.99181599]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9949302915082383, 'F1-Score': 0.9948391043109722}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.99189091 0.99345797 0.99095284 0.99181599 0.99009396]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9949302915082383, 'F1-Score': 0.9948391043109722}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.99273181 0.9942999  0.99095284 0.99026139 0.99095889]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9949302915082383, 'F1-Score': 0.9948391043109722}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.99195696 0.9913236  0.99117924 0.99118106 0.98882019]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9917617237008872, 'F1-Score': 0.9917853109969385}\n",
            "Best model saved to best_models/predict_passed_MI_more_restricted_Logistic Regression.pkl\n",
            "Processing complete for 'predict_passed_MI_more_restricted'. Total time: 123.05 seconds\n",
            "Best model: {'model_name': 'Logistic Regression', 'params': array([0.99195696, 0.99273181, 0.99181119, 0.9926655 , 0.99181599]), 'dataset': 'predict_passed_MI_more_restricted', 'test_scores': {'Accuracy': 0.9961977186311787, 'F1-Score': 0.9961755177255888}}\n"
          ]
        }
      ],
      "source": [
        "mi_more_restricted_passed_results, mi_more_restricted_passed_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=mi_passed_datasets,\n",
        "    target_column='Passed',\n",
        "    feature_set_name='MI',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRAxLl2ycBdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e52df72-2eef-4549-c46a-84b50fdc9d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Passed with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_passed_RF_more_restricted\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.99195696 0.99273181 0.99181119 0.9926655  0.99181599]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9961977186311787, 'F1-Score': 0.9961755177255888}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.99267129 0.98891528 0.98806746 0.99188449 0.99118106]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9942965779467681, 'F1-Score': 0.9942462969460354}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Cross-validation scores for Random Forest: [0.99351273 0.99345797 0.99188101 0.9926655  0.99181599]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9949302915082383, 'F1-Score': 0.9948391043109722}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.99189091 0.99345797 0.99095284 0.99181599 0.99009396]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9949302915082383, 'F1-Score': 0.9948391043109722}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.99273181 0.9942999  0.99095284 0.99026139 0.99095889]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9949302915082383, 'F1-Score': 0.9948391043109722}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.99195696 0.9913236  0.99117924 0.99118106 0.98882019]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.9917617237008872, 'F1-Score': 0.9917853109969385}\n",
            "Best model saved to best_models/predict_passed_RF_more_restricted_Logistic Regression.pkl\n",
            "Processing complete for 'predict_passed_RF_more_restricted'. Total time: 126.38 seconds\n",
            "Best model: {'model_name': 'Logistic Regression', 'params': array([0.99195696, 0.99273181, 0.99181119, 0.9926655 , 0.99181599]), 'dataset': 'predict_passed_RF_more_restricted', 'test_scores': {'Accuracy': 0.9961977186311787, 'F1-Score': 0.9961755177255888}}\n"
          ]
        }
      ],
      "source": [
        "rf_more_restricted_passed_results, rf_more_restricted_passed_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_passed_datasets,\n",
        "    target_column='Passed',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQNaMsARcK9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2abfc05-5cc7-48ce-f5b3-ceac20190f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: Passed with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_passed_DT_more_restricted\n",
            "Performing classification analysis...\n",
            "Evaluating model: Logistic Regression\n",
            "Best parameters for Logistic Regression: {'C': 0.3}\n",
            "Cross-validation scores for Logistic Regression: [0.99195696 0.99273181 0.99181119 0.9926655  0.99181599]\n",
            "Test set scores for Logistic Regression: {'Accuracy': 0.9961977186311787, 'F1-Score': 0.9961755177255888}\n",
            "Evaluating model: Decision Tree\n",
            "Best parameters for Decision Tree: {'max_depth': 5}\n",
            "Cross-validation scores for Decision Tree: [0.99273181 0.98891528 0.98732171 0.99188449 0.99118106]\n",
            "Test set scores for Decision Tree: {'Accuracy': 0.9942965779467681, 'F1-Score': 0.9942462969460354}\n",
            "Evaluating model: Random Forest\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "Cross-validation scores for Random Forest: [0.99351273 0.99267129 0.99188101 0.99188449 0.99095889]\n",
            "Test set scores for Random Forest: {'Accuracy': 0.9949302915082383, 'F1-Score': 0.9948391043109722}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [0.99189091 0.99345797 0.99095284 0.99181599 0.99009396]\n",
            "Test set scores for Support Vector Machine: {'Accuracy': 0.9949302915082383, 'F1-Score': 0.9948391043109722}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 5}\n",
            "Cross-validation scores for k-Nearest Neighbors: [0.99273181 0.9942999  0.99095284 0.99026139 0.99095889]\n",
            "Test set scores for k-Nearest Neighbors: {'Accuracy': 0.9949302915082383, 'F1-Score': 0.9948391043109722}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [0.99351273 0.9913236  0.98948948 0.99110962 0.98949319]\n",
            "Test set scores for Neural Network: {'Accuracy': 0.991128010139417, 'F1-Score': 0.9912276906527675}\n",
            "Best model saved to best_models/predict_passed_DT_more_restricted_Logistic Regression.pkl\n",
            "Processing complete for 'predict_passed_DT_more_restricted'. Total time: 124.78 seconds\n",
            "Best model: {'model_name': 'Logistic Regression', 'params': array([0.99195696, 0.99273181, 0.99181119, 0.9926655 , 0.99181599]), 'dataset': 'predict_passed_DT_more_restricted', 'test_scores': {'Accuracy': 0.9961977186311787, 'F1-Score': 0.9961755177255888}}\n"
          ]
        }
      ],
      "source": [
        "dt_more_restricted_passed_results, dt_more_restricted_passed_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=dt_passed_datasets,\n",
        "    target_column='Passed',\n",
        "    feature_set_name='DT',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neZ7vor749nO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4743c6-b743-4de1-fcbf-79aad26896a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column to Classify: Passed. Feature Set: more_restricted\n",
            "Best Model Info using MI Feature Selection method: {'model_name': 'Logistic Regression', 'params': array([0.99195696, 0.99273181, 0.99181119, 0.9926655 , 0.99181599]), 'dataset': 'predict_passed_MI_more_restricted', 'test_scores': {'Accuracy': 0.9961977186311787, 'F1-Score': 0.9961755177255888}}\n",
            "Best Model Info using RF Feature Selection method: {'model_name': 'Logistic Regression', 'params': array([0.99195696, 0.99273181, 0.99181119, 0.9926655 , 0.99181599]), 'dataset': 'predict_passed_RF_more_restricted', 'test_scores': {'Accuracy': 0.9961977186311787, 'F1-Score': 0.9961755177255888}}\n",
            "Best Model Info using DT Feature Selection method: {'model_name': 'Logistic Regression', 'params': array([0.99195696, 0.99273181, 0.99181119, 0.9926655 , 0.99181599]), 'dataset': 'predict_passed_DT_more_restricted', 'test_scores': {'Accuracy': 0.9961977186311787, 'F1-Score': 0.9961755177255888}}\n"
          ]
        }
      ],
      "source": [
        "print(\"Column to Classify: Passed. Feature Set: more_restricted\")\n",
        "print(\"Best Model Info using MI Feature Selection method:\", mi_more_restricted_passed_best_model)\n",
        "print(\"Best Model Info using RF Feature Selection method:\", rf_more_restricted_passed_best_model)\n",
        "print(\"Best Model Info using DT Feature Selection method:\", dt_more_restricted_passed_best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ckZPGhV3tH"
      },
      "source": [
        "### 4.3.5 Predicting FinalGrade feature from students that passed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CPU"
      ],
      "metadata": {
        "id": "o-kPy6Bk2VOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'FinalGrade' using the original dataset without feature selection\n",
        "original_finalgrade_results_zero, original_finalgrade_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZiPCa6QpGB-",
        "outputId": "83eda092-39a1-468b-a532-148fc5278f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: filtered_combined_df_FinalGrade\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-1.66094957e+00 -3.81845096e+19 -1.53845885e+00 -1.64729813e+00\n",
            " -1.43368663e+00]\n",
            "Test set scores for Linear Regression: {'MSE': 3.081687220586811e+21, 'R2': -1.0226671550599767e+21, 'MAE': 1433338114.2409737}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.05370756 -3.05258986 -3.28384251 -3.17936663 -2.85559656]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 3.025924757467669, 'R2': -0.004162214280707399, 'MAE': 1.3010038229034229}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-1.81021229 -1.71477544 -1.69491935 -1.86542179 -1.59244656]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.6853482139918319, 'R2': 0.44071213594472347, 'MAE': 0.9926705266320509}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-1.62359021 -1.49774528 -1.50743505 -1.60634891 -1.39637694]\n",
            "Test set scores for Support Vector Machine: {'MSE': 1.4651801525085522, 'R2': 0.5137755680698375, 'MAE': 0.9126361979024984}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-1.95369484 -1.86682164 -1.84190309 -1.98622529 -1.7823213 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 1.9027428526852455, 'R2': 0.3685690724979567, 'MAE': 1.0320869340280308}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-1.75540046 -1.63370506 -1.77121593 -1.80901383 -1.50770425]\n",
            "Test set scores for Neural Network: {'MSE': 1.7790125258204532, 'R2': 0.4096293528937599, 'MAE': 1.0138651492387107}\n",
            "Best regression model saved to best_models/filtered_combined_df_FinalGrade_regression_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 403.06 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-1.62359021, -1.49774528, -1.50743505, -1.60634891, -1.39637694]), 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4651801525085522, 'R2': 0.5137755680698375, 'MAE': 0.9126361979024984}}\n",
            "Best classification model: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'FinalGrade' using the original dataset without feature selection\n",
        "original_finalgrade_results_zero, original_finalgrade_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9KxW2_BpOxS",
        "outputId": "317b962a-9b51-4e5f-b313-b0797360921e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: filtered_combined_df_FinalGrade\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-1.66094957e+00 -3.81845096e+19 -1.53845885e+00 -1.64729813e+00\n",
            " -1.43368663e+00]\n",
            "Test set scores for Linear Regression: {'MSE': 3.081687220586811e+21, 'R2': -1.0226671550599767e+21, 'MAE': 1433338114.2409737}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.05370756 -3.05258986 -3.28384251 -3.17936663 -2.85559656]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 3.025924757467669, 'R2': -0.004162214280707399, 'MAE': 1.3010038229034229}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-1.81021229 -1.71477544 -1.69491935 -1.86542179 -1.59244656]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.6853482139918319, 'R2': 0.44071213594472347, 'MAE': 0.9926705266320509}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-1.62359021 -1.49774528 -1.50743505 -1.60634891 -1.39637694]\n",
            "Test set scores for Support Vector Machine: {'MSE': 1.4651801525085522, 'R2': 0.5137755680698375, 'MAE': 0.9126361979024984}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-1.95369484 -1.86682164 -1.84190309 -1.98622529 -1.7823213 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 1.9027428526852455, 'R2': 0.3685690724979567, 'MAE': 1.0320869340280308}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-1.75540046 -1.63370506 -1.77121593 -1.80901383 -1.50770425]\n",
            "Test set scores for Neural Network: {'MSE': 1.7790125258204532, 'R2': 0.4096293528937599, 'MAE': 1.0138651492387107}\n",
            "Best regression model saved to best_models/filtered_combined_df_FinalGrade_regression_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 405.23 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-1.62359021, -1.49774528, -1.50743505, -1.60634891, -1.39637694]), 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4651801525085522, 'R2': 0.5137755680698375, 'MAE': 0.9126361979024984}}\n",
            "Best classification model: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'FinalGrade' using the original dataset without feature selection\n",
        "original_finalgrade_results_remove_miss, original_finalgrade_best_model_remove_miss = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szKUPalFTM6J",
        "outputId": "58079f3e-71a4-4559-e120-3878479591cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: filtered_combined_df_FinalGrade\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-2.0006911  -1.84215961 -1.9472068  -1.9190993  -1.81520376]\n",
            "Test set scores for Linear Regression: {'MSE': 1.836522415826088, 'R2': 0.39054452325661215, 'MAE': 1.0485898819811703}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.00810554 -2.77334315 -2.75190128 -3.13746596 -2.64945154]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 2.82063259960325, 'R2': 0.06396460457255426, 'MAE': 1.2647467369070156}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-2.08209838 -1.95933419 -1.99900606 -2.16883448 -1.81411228]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.9691464099388438, 'R2': 0.3465328526866973, 'MAE': 1.081043824672544}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-3.11729511 -3.21353652 -2.96323344 -3.26038868 -2.99984903]\n",
            "Test set scores for Support Vector Machine: {'MSE': 3.1102487886923047, 'R2': -0.032145397174675905, 'MAE': 1.3171925130750395}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-2.17653646 -2.00110654 -2.13178194 -2.32088729 -1.98731459]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 2.0534884582514774, 'R2': 0.31854369076799116, 'MAE': 1.088256507078507}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-2.71836115 -2.0665563  -2.06525796 -2.05131413 -2.00356466]\n",
            "Test set scores for Neural Network: {'MSE': 1.9094820552940255, 'R2': 0.3663326478818395, 'MAE': 1.0716176593030775}\n",
            "Best regression model saved to best_models/filtered_combined_df_FinalGrade_regression_Linear Regression.pkl\n",
            "Processing complete for datasets. Total time: 122.44 seconds\n",
            "Best regression model: {'model_name': 'Linear Regression', 'params': array([-2.0006911 , -1.84215961, -1.9472068 , -1.9190993 , -1.81520376]), 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.836522415826088, 'R2': 0.39054452325661215, 'MAE': 1.0485898819811703}}\n",
            "Best classification model: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'FinalGrade' using the original dataset without feature selection\n",
        "original_finalgrade_results_median, original_finalgrade_best_model_median = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    imputation_strategy='median',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gou28qgWTGnh",
        "outputId": "595667c7-d466-4b07-858b-3f6a197d8b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: filtered_combined_df_FinalGrade\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-1.72749264e+00 -8.59933402e+23 -1.64299697e+00 -1.73481778e+00\n",
            " -1.50386018e+00]\n",
            "Test set scores for Linear Regression: {'MSE': 4.586946360181301e+21, 'R2': -1.522191919167607e+21, 'MAE': 1748703589.3578944}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.22682877 -2.98965786 -3.13078757 -3.41534906 -2.82590664]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 3.159277334985475, 'R2': -0.04841566744072057, 'MAE': 1.3519265600366601}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-1.7791738  -1.71704767 -1.71642493 -1.86033476 -1.56377979]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.667222282211761, 'R2': 0.44672728081812385, 'MAE': 0.9857527902177436}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-1.70147773 -1.59867741 -1.62430877 -1.64713089 -1.48088078]\n",
            "Test set scores for Support Vector Machine: {'MSE': 1.5021144880020851, 'R2': 0.5015187979632303, 'MAE': 0.9154564347236348}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-1.9963174  -1.94139629 -1.9507974  -2.00720472 -1.82351884]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 1.8628728784769701, 'R2': 0.38180004312453686, 'MAE': 1.022187075263269}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-1.64275778 -1.58072755 -1.55589006 -1.72452078 -1.52279108]\n",
            "Test set scores for Neural Network: {'MSE': 1.5403794808375897, 'R2': 0.4888204452231931, 'MAE': 0.932099422012753}\n",
            "Best regression model saved to best_models/filtered_combined_df_FinalGrade_regression_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 266.92 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-1.70147773, -1.59867741, -1.62430877, -1.64713089, -1.48088078]), 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.5021144880020851, 'R2': 0.5015187979632303, 'MAE': 0.9154564347236348}}\n",
            "Best classification model: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'FinalGrade' using the original dataset without feature selection\n",
        "original_finalgrade_results_knn, original_finalgrade_best_model_knn = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    imputation_strategy='knn',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1CaVyWtTC-_",
        "outputId": "13623d56-43ae-42fa-ceca-8aec7ccda20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: filtered_combined_df_FinalGrade\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-1.86063283 -1.82374364 -1.74222725 -1.74090774 -1.60484651]\n",
            "Test set scores for Linear Regression: {'MSE': 8.808117552972195e+22, 'R2': -2.923000251017256e+22, 'MAE': 7662948760.182307}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.11804294 -3.05998993 -3.57111891 -3.29472256 -2.79123457]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 3.0303878458770197, 'R2': -0.005643303567082869, 'MAE': 1.3067294013065014}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-1.82834421 -1.68097199 -1.72181797 -1.8439088  -1.5349184 ]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.6421333316818036, 'R2': 0.4550531243659406, 'MAE': 0.9843726372483307}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-1.78734467 -1.62962387 -1.6708334  -1.67208425 -1.52129362]\n",
            "Test set scores for Support Vector Machine: {'MSE': 1.5183780702446634, 'R2': 0.4961216793744285, 'MAE': 0.9215048732131552}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-2.03546377 -1.94113508 -1.91607145 -2.00506212 -1.8588829 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 1.863496402976182, 'R2': 0.3815931246477169, 'MAE': 1.0227298395146975}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-1.76384632 -1.54062871 -1.62090244 -1.87404687 -1.52564725]\n",
            "Test set scores for Neural Network: {'MSE': 1.6199595691353759, 'R2': 0.4624115540303284, 'MAE': 0.9652014190435111}\n",
            "Best regression model saved to best_models/filtered_combined_df_FinalGrade_regression_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 396.24 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-1.78734467, -1.62962387, -1.6708334 , -1.67208425, -1.52129362]), 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.5183780702446634, 'R2': 0.4961216793744285, 'MAE': 0.9215048732131552}}\n",
            "Best classification model: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'FinalGrade' using the original dataset without feature selection\n",
        "original_finalgrade_results_zero, original_finalgrade_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMcpSWIMS-V1",
        "outputId": "713a01ae-143c-4533-b658-d16ca83817aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: filtered_combined_df_FinalGrade\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-1.66094957e+00 -3.81845096e+19 -1.53845885e+00 -1.64729813e+00\n",
            " -1.43368663e+00]\n",
            "Test set scores for Linear Regression: {'MSE': 3.081687220586811e+21, 'R2': -1.0226671550599767e+21, 'MAE': 1433338114.2409737}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.05370756 -3.05258986 -3.28384251 -3.17936663 -2.85559656]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 3.025924757467669, 'R2': -0.004162214280707399, 'MAE': 1.3010038229034229}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-1.81021229 -1.71477544 -1.69491935 -1.86542179 -1.59244656]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.6853482139918319, 'R2': 0.44071213594472347, 'MAE': 0.9926705266320509}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-1.62359021 -1.49774528 -1.50743505 -1.60634891 -1.39637694]\n",
            "Test set scores for Support Vector Machine: {'MSE': 1.4651801525085522, 'R2': 0.5137755680698375, 'MAE': 0.9126361979024984}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-1.95369484 -1.86682164 -1.84190309 -1.98622529 -1.7823213 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 1.9027428526852455, 'R2': 0.3685690724979567, 'MAE': 1.0320869340280308}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-1.75540046 -1.63370506 -1.77121593 -1.80901383 -1.50770425]\n",
            "Test set scores for Neural Network: {'MSE': 1.7790125258204532, 'R2': 0.4096293528937599, 'MAE': 1.0138651492387107}\n",
            "Best regression model saved to best_models/filtered_combined_df_FinalGrade_regression_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 446.78 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-1.62359021, -1.49774528, -1.50743505, -1.60634891, -1.39637694]), 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4651801525085522, 'R2': 0.5137755680698375, 'MAE': 0.9126361979024984}}\n",
            "Best classification model: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'FinalGrade' using the original dataset without feature selection\n",
        "original_finalgrade_results_mean, original_finalgrade_best_model_mean = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    imputation_strategy='mean',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs-seFRp2XlA",
        "outputId": "4c20eb44-2d9a-42fd-a4c8-00573dc5a4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: filtered_combined_df_FinalGrade\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-1.77900037e+00 -1.64048846e+04 -1.66278585e+00 -1.73825033e+00\n",
            " -1.50843758e+00]\n",
            "Test set scores for Linear Regression: {'MSE': 7.943618886770027e+20, 'R2': -2.6361137735019397e+20, 'MAE': 727718760.1275631}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-2.94311812 -3.10380944 -3.11572559 -3.17414727 -2.93639197]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 3.220220415900072, 'R2': -0.06863981178709366, 'MAE': 1.3464011747552749}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-1.78768716 -1.71424064 -1.67436671 -1.87516332 -1.56399597]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.6869803341623986, 'R2': 0.4401705119666639, 'MAE': 0.9940530188321299}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-1.71441406 -1.59332168 -1.61007699 -1.64172195 -1.46290998]\n",
            "Test set scores for Support Vector Machine: {'MSE': 1.5077651639740197, 'R2': 0.4996436041791953, 'MAE': 0.9196091520005785}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-1.9632653  -1.9070586  -1.91295301 -1.97409148 -1.78638467]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 1.858314265279691, 'R2': 0.3833128325985421, 'MAE': 1.0257461228823166}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-1.63177925 -1.56538228 -1.52745859 -1.79239775 -1.39317221]\n",
            "Test set scores for Neural Network: {'MSE': 1.5210453133540411, 'R2': 0.49523654674179196, 'MAE': 0.9384359026206327}\n",
            "Best regression model saved to best_models/filtered_combined_df_FinalGrade_regression_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 289.79 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-1.71441406, -1.59332168, -1.61007699, -1.64172195, -1.46290998]), 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.5077651639740197, 'R2': 0.4996436041791953, 'MAE': 0.9196091520005785}}\n",
            "Best classification model: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RF - More Restricted\n",
        "rf_more_restricted_finalgrade_results_mean, rf_more_restricted_finalgrade_best_model_mean = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    imputation_strategy='mean',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAQFCiIH2ggb",
        "outputId": "e0c5fea9-255b-4c63-b257-501693801771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_RF_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-1.85498108 -1.94002198 -2.13435664 -1.82414137 -1.99267997]\n",
            "Test set scores for Linear Regression: {'MSE': 1.7495952905999652, 'R2': 0.7765690944771519, 'MAE': 0.9961799624231823}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.5434959  -3.1667224  -3.45359465 -3.5237446  -3.11567099]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 3.160843254694458, 'R2': 0.5963466097522526, 'MAE': 1.310413963617961}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-1.79307551 -1.72742485 -2.01501744 -1.75634253 -1.84408763]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.6494664351046446, 'R2': 0.7893559835208543, 'MAE': 0.950514301304277}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-1.60865153 -1.73947344 -1.82531559 -1.7075855  -1.77345312]\n",
            "Test set scores for Support Vector Machine: {'MSE': 1.532755326868256, 'R2': 0.8042604981465705, 'MAE': 0.9057687942593992}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-1.90825844 -2.01369446 -2.20522441 -2.14329288 -2.05741636]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 1.867238835276724, 'R2': 0.7615455036746, 'MAE': 0.9963687688812198}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-1.63974093 -1.75089177 -1.83544849 -1.68546102 -1.67014823]\n",
            "Test set scores for Neural Network: {'MSE': 1.5759490673856127, 'R2': 0.7987444701779525, 'MAE': 0.9203326197876877}\n",
            "Best regression model saved to best_models/predict_finalgrade_RF_more_restricted_regression_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 350.21 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-1.60865153, -1.73947344, -1.82531559, -1.7075855 , -1.77345312]), 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.532755326868256, 'R2': 0.8042604981465705, 'MAE': 0.9057687942593992}}\n",
            "Best classification model: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RF - More Restricted\n",
        "rf_more_restricted_finalgrade_results_zero, rf_more_restricted_finalgrade_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7nxXNxPS29y",
        "outputId": "01f95fa4-8ea0-4c4a-e0c1-01d682cc98d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_RF_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-1.84427965 -1.88795566 -2.04206462 -1.81835179 -1.97139711]\n",
            "Test set scores for Linear Regression: {'MSE': 1.6917523040562292, 'R2': 0.783955894688063, 'MAE': 0.9689118640890197}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.54377761 -3.07398499 -3.63340837 -3.27324677 -3.23467901]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 3.1296027555708896, 'R2': 0.6003361569610651, 'MAE': 1.2757103012944102}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-1.88554154 -1.77287757 -2.0580205  -1.81639606 -1.87692249]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.6761628811610203, 'R2': 0.7859467315934715, 'MAE': 0.963139714418489}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-1.57065978 -1.65737255 -1.74418885 -1.68462077 -1.7723963 ]\n",
            "Test set scores for Support Vector Machine: {'MSE': 1.4669773797015526, 'R2': 0.8126606272380537, 'MAE': 0.8861163230314105}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-1.89092361 -2.02551289 -2.08076853 -2.15853832 -2.14769532]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 1.8359172850816219, 'R2': 0.7655454014567158, 'MAE': 0.9806904492022449}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (50,)}\n",
            "Cross-validation scores for Neural Network: [-1.71377008 -1.64998769 -1.761422   -1.71959558 -1.76605192]\n",
            "Test set scores for Neural Network: {'MSE': 1.5834489163071106, 'R2': 0.7977867069484685, 'MAE': 0.9222749095173125}\n",
            "Best regression model saved to best_models/predict_finalgrade_RF_more_restricted_regression_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 347.89 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-1.57065978, -1.65737255, -1.74418885, -1.68462077, -1.7723963 ]), 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.4669773797015526, 'R2': 0.8126606272380537, 'MAE': 0.8861163230314105}}\n",
            "Best classification model: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPU T4"
      ],
      "metadata": {
        "id": "y-WiDi562RET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'FinalGrade' using the original dataset without feature selection\n",
        "original_finalgrade_results_zero, original_finalgrade_best_model_zero = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    imputation_strategy='zero',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8k6-mbOr77u",
        "outputId": "59b6e3a3-ca62-408d-f214-81eca80fc3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: filtered_combined_df_FinalGrade\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-1.66094957e+00 -3.81845096e+19 -1.53845885e+00 -1.64729813e+00\n",
            " -1.43368663e+00]\n",
            "Test set scores for Linear Regression: {'MSE': 3.081687220586811e+21, 'R2': -1.0226671550599767e+21, 'MAE': 1433338114.2409737}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.05370756 -3.05258986 -3.28384251 -3.17936663 -2.85559656]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 3.025924757467669, 'R2': -0.004162214280707399, 'MAE': 1.3010038229034229}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-1.81021229 -1.71477544 -1.69491935 -1.86542179 -1.59244656]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.6853482139918319, 'R2': 0.44071213594472347, 'MAE': 0.9926705266320509}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-1.62359021 -1.49774528 -1.50743505 -1.60634891 -1.39637694]\n",
            "Test set scores for Support Vector Machine: {'MSE': 1.4651801525085522, 'R2': 0.5137755680698375, 'MAE': 0.9126361979024984}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-1.95369484 -1.86682164 -1.84190309 -1.98622529 -1.7823213 ]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 1.9027428526852455, 'R2': 0.3685690724979567, 'MAE': 1.0320869340280308}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-1.75540046 -1.63370506 -1.77121593 -1.80901383 -1.50770425]\n",
            "Test set scores for Neural Network: {'MSE': 1.7790125258204532, 'R2': 0.4096293528937599, 'MAE': 1.0138651492387107}\n",
            "Best regression model saved to best_models/filtered_combined_df_FinalGrade_regression_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 315.62 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-1.62359021, -1.49774528, -1.50743505, -1.60634891, -1.39637694]), 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4651801525085522, 'R2': 0.5137755680698375, 'MAE': 0.9126361979024984}}\n",
            "Best classification model: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze 'FinalGrade' using the original dataset without feature selection\n",
        "original_finalgrade_results, original_finalgrade_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    exclusion_type='more_restricted'  # When using original dataframe only more_restricted available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fHrXuyaS8Dd",
        "outputId": "04546223-9d7b-4506-c3f1-ac2345bccf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: filtered_combined_df_FinalGrade\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-2.0006911  -1.84215961 -1.9472068  -1.9190993  -1.81520376]\n",
            "Test set scores for Linear Regression: {'MSE': 1.836522415826088, 'R2': 0.39054452325661215, 'MAE': 1.0485898819811703}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.00810554 -2.77334315 -2.75190128 -3.13746596 -2.64945154]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 2.82063259960325, 'R2': 0.06396460457255426, 'MAE': 1.2647467369070156}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-2.08209838 -1.95933419 -1.99900606 -2.16883448 -1.81411228]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.9691464099388438, 'R2': 0.3465328526866973, 'MAE': 1.081043824672544}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-3.11729511 -3.21353652 -2.96323344 -3.26038868 -2.99984903]\n",
            "Test set scores for Support Vector Machine: {'MSE': 3.1102487886923047, 'R2': -0.032145397174675905, 'MAE': 1.3171925130750395}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-2.17653646 -2.00110654 -2.13178194 -2.32088729 -1.98731459]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 2.0534884582514774, 'R2': 0.31854369076799116, 'MAE': 1.088256507078507}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-2.71836115 -2.0665563  -2.06525796 -2.05131413 -2.00356466]\n",
            "Test set scores for Neural Network: {'MSE': 1.9094820552940255, 'R2': 0.3663326478818395, 'MAE': 1.0716176593030775}\n",
            "Best regression model saved to best_models/filtered_combined_df_FinalGrade_regression_Linear Regression.pkl\n",
            "Processing complete for datasets. Total time: 89.53 seconds\n",
            "Best regression model: {'model_name': 'Linear Regression', 'params': array([-2.0006911 , -1.84215961, -1.9472068 , -1.9190993 , -1.81520376]), 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.836522415826088, 'R2': 0.39054452325661215, 'MAE': 1.0485898819811703}}\n",
            "Best classification model: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyO2Vf-vQ4AU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a881b04e-43e5-46dd-8deb-d03f4b3496d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_RF_more_restricted\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-2.35568686 -2.31094363 -2.615568   -2.25695608 -2.42648909]\n",
            "Test set scores for Linear Regression: {'MSE': 2.150599460534049, 'R2': 0.7253591230693722, 'MAE': 1.0997911923342618}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.17594436 -3.21441579 -3.25460283 -3.32660802 -3.29355657]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 2.6030025911558505, 'R2': 0.6675852814962516, 'MAE': 1.1931070864932396}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-2.3572851  -2.37910687 -2.47996912 -2.4154267  -2.47336548]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 2.0252959880150803, 'R2': 0.7413609198737463, 'MAE': 1.076282905511662}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-2.07195602 -2.1618591  -2.30498734 -2.24357226 -2.25610641]\n",
            "Test set scores for Support Vector Machine: {'MSE': 1.8039760393172481, 'R2': 0.7696244370502643, 'MAE': 1.0035453895476634}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-2.47791493 -2.60745195 -2.77387677 -2.80643817 -2.63560624]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 2.1158527157033196, 'R2': 0.7297964330594094, 'MAE': 1.0955577607189666}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-2.07582027 -2.07520754 -2.27461157 -2.0839988  -2.33552425]\n",
            "Test set scores for Neural Network: {'MSE': 1.8134272661553075, 'R2': 0.7684174743989156, 'MAE': 1.012253337076256}\n",
            "Best regression model saved to best_models/predict_finalgrade_RF_more_restricted_regression_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 134.72 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-2.07195602, -2.1618591 , -2.30498734, -2.24357226, -2.25610641]), 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.8039760393172481, 'R2': 0.7696244370502643, 'MAE': 1.0035453895476634}}\n",
            "Best classification model: None\n"
          ]
        }
      ],
      "source": [
        "# RF - More Restricted\n",
        "rf_more_restricted_finalgrade_results, rf_more_restricted_finalgrade_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO6D_g3jQ-7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45a2a17-9da0-468e-d5ee-f50c64cde0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_Lasso_more_restricted\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-2.35568686 -2.31094363 -2.615568   -2.25695608 -2.42648909]\n",
            "Test set scores for Linear Regression: {'MSE': 2.150599460534049, 'R2': 0.7253591230693722, 'MAE': 1.0997911923342618}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.17594436 -3.21441579 -3.25460283 -3.32660802 -3.29355657]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 2.6030025911558505, 'R2': 0.6675852814962516, 'MAE': 1.1931070864932396}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-2.3572851  -2.37910687 -2.47996912 -2.4154267  -2.47336548]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 2.0252959880150803, 'R2': 0.7413609198737463, 'MAE': 1.076282905511662}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-2.07195602 -2.1618591  -2.30498734 -2.24357226 -2.25610641]\n",
            "Test set scores for Support Vector Machine: {'MSE': 1.8039760393172481, 'R2': 0.7696244370502643, 'MAE': 1.0035453895476634}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-2.47791493 -2.60745195 -2.77387677 -2.80643817 -2.63560624]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 2.1158527157033196, 'R2': 0.7297964330594094, 'MAE': 1.0955577607189666}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-2.07582027 -2.07520754 -2.27461157 -2.0839988  -2.33552425]\n",
            "Test set scores for Neural Network: {'MSE': 1.8134272661553075, 'R2': 0.7684174743989156, 'MAE': 1.012253337076256}\n",
            "Best regression model saved to best_models/predict_finalgrade_Lasso_more_restricted_regression_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 131.16 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-2.07195602, -2.1618591 , -2.30498734, -2.24357226, -2.25610641]), 'dataset': 'predict_finalgrade_Lasso_more_restricted', 'test_scores': {'MSE': 1.8039760393172481, 'R2': 0.7696244370502643, 'MAE': 1.0035453895476634}}\n",
            "Best classification model: None\n"
          ]
        }
      ],
      "source": [
        "# Lasso - More Restricted\n",
        "lasso_more_restricted_finalgrade_results, lasso_more_restricted_finalgrade_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=lasso_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='Lasso',\n",
        "    exclusion_type='more_restricted'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdOC-NFIRCGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c1d7ec-b43a-4adc-816e-34794b577a6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_FR_more_restricted\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Performing regression analysis...\n",
            "Evaluating model: Linear Regression\n",
            "Cross-validation scores for Linear Regression: [-2.35773592 -2.31601582 -2.61897769 -2.25351371 -2.43895817]\n",
            "Test set scores for Linear Regression: {'MSE': 2.156141568527234, 'R2': 0.7246513718459462, 'MAE': 1.098872561129167}\n",
            "Evaluating model: Decision Tree Regressor\n",
            "Cross-validation scores for Decision Tree Regressor: [-3.02345669 -2.93598351 -3.10919102 -3.05124064 -3.1000556 ]\n",
            "Test set scores for Decision Tree Regressor: {'MSE': 2.462837109199588, 'R2': 0.6854850213531158, 'MAE': 1.149253198270344}\n",
            "Evaluating model: Random Forest Regressor\n",
            "Cross-validation scores for Random Forest Regressor: [-2.3079899  -2.27356729 -2.43943595 -2.36068153 -2.45122278]\n",
            "Test set scores for Random Forest Regressor: {'MSE': 1.9812655158345311, 'R2': 0.7469838021041422, 'MAE': 1.0602247098813018}\n",
            "Evaluating model: Support Vector Machine\n",
            "Best parameters for Support Vector Machine: {'C': 1}\n",
            "Cross-validation scores for Support Vector Machine: [-2.11557309 -2.18241063 -2.29626303 -2.20187181 -2.29664885]\n",
            "Test set scores for Support Vector Machine: {'MSE': 1.8101196505393204, 'R2': 0.7688398712561615, 'MAE': 1.0010353805832066}\n",
            "Evaluating model: k-Nearest Neighbors\n",
            "Best parameters for k-Nearest Neighbors: {'n_neighbors': 7}\n",
            "Cross-validation scores for k-Nearest Neighbors: [-2.43827791 -2.6059333  -2.68787003 -2.70629995 -2.63256907]\n",
            "Test set scores for k-Nearest Neighbors: {'MSE': 2.137652621914034, 'R2': 0.727012490503584, 'MAE': 1.093057573488111}\n",
            "Evaluating model: Neural Network\n",
            "Best parameters for Neural Network: {'hidden_layer_sizes': (100,)}\n",
            "Cross-validation scores for Neural Network: [-2.05715381 -2.08058809 -2.27320542 -2.05932236 -2.29764054]\n",
            "Test set scores for Neural Network: {'MSE': 1.8618792192450533, 'R2': 0.762229949883186, 'MAE': 1.0397488484375188}\n",
            "Best regression model saved to best_models/predict_finalgrade_FR_more_restricted_regression_Support Vector Machine.pkl\n",
            "Processing complete for datasets. Total time: 129.34 seconds\n",
            "Best regression model: {'model_name': 'Support Vector Machine', 'params': array([-2.11557309, -2.18241063, -2.29626303, -2.20187181, -2.29664885]), 'dataset': 'predict_finalgrade_FR_more_restricted', 'test_scores': {'MSE': 1.8101196505393204, 'R2': 0.7688398712561615, 'MAE': 1.0010353805832066}}\n",
            "Best classification model: None\n"
          ]
        }
      ],
      "source": [
        "# FR - More Restricted\n",
        "fr_more_restricted_finalgrade_results, fr_more_restricted_finalgrade_best_model = analyze_datasets_for_target_column(\n",
        "    datasets_dict=fr_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='FR',\n",
        "    exclusion_type='more_restricted'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc0Khb7E8Ti8"
      },
      "source": [
        "#### Improve results - Fine Tunning\n",
        "Between Bagging, Boosting and Stacking, with better results on stacking and then boosting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### A100 GPU"
      ],
      "metadata": {
        "id": "CiWPVfWMN0JB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis with imputation testing for RF with more_restricted\n",
        "original_finalgrade_best_model = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    group_col='Year',\n",
        "    exclusion_type='more_restricted',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7358872b-f2f3-47b9-b3f3-617d3fee67d6",
        "id": "G6HVu3WVN0JJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Processing complete for datasets. Total time: 172.39 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d378afa70a0>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.6792544847288797, 'R2': 0.4427343582933829, 'MAE': 1.006535434283224}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis with imputation testing for RF with more_restricted\n",
        "original_finalgrade_best_model_knn = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='knn',\n",
        "    exclusion_type='more_restricted',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9077be-b045-4394-e798-106a80e2c1c3",
        "id": "69as4Jx7N0JJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Processing complete for datasets. Total time: 421.16 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3848321060>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.417487463820378, 'R2': 0.5296025299795568, 'MAE': 0.9211656989314588}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis for RF with more_restricted\n",
        "rf_more_restricted_finalgrade_results_mean, rf_more_restricted_finalgrade_best_model_mean = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategy='mean',  # Use the best strategy found\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P02favPGOKtn",
        "outputId": "81a27f7a-564e-487c-e219-c7537d154648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 436.34 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d378ab743d0>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.3925497758755025, 'R2': 0.8221653549931494, 'MAE': 0.8832376288494231}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis for RF with more_restricted\n",
        "rf_more_restricted_finalgrade_results_knn, rf_more_restricted_finalgrade_best_model_knn = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategy='knn',  # Use the best strategy found\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_Pa_juIOM3f",
        "outputId": "cd61888e-8e37-4fe8-c87d-20a02b0eafe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 467.72 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_Stacking', 'params': StackingRegressor(estimators=[('xgb',\n",
            "                               XGBRegressor(base_score=None, booster=None,\n",
            "                                            callbacks=None,\n",
            "                                            colsample_bylevel=None,\n",
            "                                            colsample_bynode=None,\n",
            "                                            colsample_bytree=None, device=None,\n",
            "                                            early_stopping_rounds=None,\n",
            "                                            enable_categorical=False,\n",
            "                                            eval_metric=None,\n",
            "                                            feature_types=None, gamma=None,\n",
            "                                            grow_policy=None,\n",
            "                                            importance_type=None,\n",
            "                                            interaction_constraints=None,\n",
            "                                            learning_ra...\n",
            "                                            max_delta_step=None, max_depth=None,\n",
            "                                            max_leaves=None,\n",
            "                                            min_child_weight=None, missing=nan,\n",
            "                                            monotone_constraints=None,\n",
            "                                            multi_strategy=None,\n",
            "                                            n_estimators=None, n_jobs=None,\n",
            "                                            num_parallel_tree=None,\n",
            "                                            random_state=2024, ...)),\n",
            "                              ('lgbm',\n",
            "                               LGBMRegressor(random_state=2024, verbosity=-1)),\n",
            "                              ('mlp',\n",
            "                               MLPRegressor(max_iter=1000, random_state=2024))],\n",
            "                  final_estimator=LinearRegression()), 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.4280121869566278, 'R2': 0.8176366513195346, 'MAE': 0.9034274285562686}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis for RF with more_restricted\n",
        "rf_more_restricted_finalgrade_results_zero, rf_more_restricted_finalgrade_best_model_zero = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategy='zero',  # Use the best strategy found\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N87U1ElJOQ14",
        "outputId": "7d5d076e-2c59-48fb-cd2c-8d3c1b92d6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 585.47 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d378ab828f0>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.406426058269903, 'R2': 0.8203932935585255, 'MAE': 0.8844594491722937}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### L4 GPU"
      ],
      "metadata": {
        "id": "EW48kMYoPWlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis with imputation testing for RF with more_restricted\n",
        "original_finalgrade_best_model = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    exclusion_type='more_restricted',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVNaReEDBNzc",
        "outputId": "a1ead022-d01a-4b16-b4c3-5436369d4107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Processing complete for datasets. Total time: 169.93 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3346791630>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.6792544847288797, 'R2': 0.4427343582933829, 'MAE': 1.006535434283224}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis with imputation testing for RF with more_restricted\n",
        "original_finalgrade_best_model = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='mean',\n",
        "    exclusion_type='more_restricted',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScbTcuyySmnP",
        "outputId": "bad713d2-ae32-4cd1-93dc-ca14454bb4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Processing complete for datasets. Total time: 351.10 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3345f8bd90>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4264121162681838, 'R2': 0.5266408572738632, 'MAE': 0.9178229082371265}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### CPU"
      ],
      "metadata": {
        "id": "NRYx09DpmpZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis with imputation testing for RF with more_restricted\n",
        "original_finalgrade_best_model = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    group_col='Year',\n",
        "    exclusion_type='more_restricted',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb70f9d0-995e-4755-cbfe-0136e49cdba2",
        "id": "Rnaled5PZVtS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Processing complete for datasets. Total time: 219.13 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7cd6ee2d81c0>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.6792544847288797, 'R2': 0.4427343582933829, 'MAE': 1.006535434283224}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis with imputation testing for RF with more_restricted\n",
        "original_finalgrade_best_model_knn = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='knn',\n",
        "    exclusion_type='more_restricted',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a2da237-c424-42e7-843e-0c8cb6f512cf",
        "id": "MKLDveGsZVtZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Processing complete for datasets. Total time: 638.80 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7cd6ee2d8130>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.417487463820378, 'R2': 0.5296025299795568, 'MAE': 0.9211656989314588}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis for RF with more_restricted\n",
        "rf_more_restricted_finalgrade_results_mean, rf_more_restricted_finalgrade_best_model_mean = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategy='mean',  # Use the best strategy found\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a049b9-e732-4e8d-c747-4db974e41fec",
        "id": "JSKGS9KxZVta"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 654.31 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7cd6f05d8d60>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.3925497758755025, 'R2': 0.8221653549931494, 'MAE': 0.8832376288494231}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis for RF with more_restricted\n",
        "rf_more_restricted_finalgrade_results_mean, rf_more_restricted_finalgrade_best_model_mean = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted',\n",
        "    group_col='Year',\n",
        "    imputation_strategy='mean',  # Use the best strategy found\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4igxliMaZpgZ",
        "outputId": "c9654a6a-4297-439d-ed3c-eb8630b103a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'Year' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 887.28 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7cd6ed593f40>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.3925497758755025, 'R2': 0.8221653549931494, 'MAE': 0.8832376288494231}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis for RF with more_restricted\n",
        "rf_more_restricted_finalgrade_results_knn, rf_more_restricted_finalgrade_best_model_knn = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategy='knn',  # Use the best strategy found\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0f85a3-22dd-40df-b0ca-d36e2105d738",
        "id": "JlpFZGM2ZVta"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 678.13 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_Stacking', 'params': StackingRegressor(estimators=[('xgb',\n",
            "                               XGBRegressor(base_score=None, booster=None,\n",
            "                                            callbacks=None,\n",
            "                                            colsample_bylevel=None,\n",
            "                                            colsample_bynode=None,\n",
            "                                            colsample_bytree=None, device=None,\n",
            "                                            early_stopping_rounds=None,\n",
            "                                            enable_categorical=False,\n",
            "                                            eval_metric=None,\n",
            "                                            feature_types=None, gamma=None,\n",
            "                                            grow_policy=None,\n",
            "                                            importance_type=None,\n",
            "                                            interaction_constraints=None,\n",
            "                                            learning_ra...\n",
            "                                            max_delta_step=None, max_depth=None,\n",
            "                                            max_leaves=None,\n",
            "                                            min_child_weight=None, missing=nan,\n",
            "                                            monotone_constraints=None,\n",
            "                                            multi_strategy=None,\n",
            "                                            n_estimators=None, n_jobs=None,\n",
            "                                            num_parallel_tree=None,\n",
            "                                            random_state=2024, ...)),\n",
            "                              ('lgbm',\n",
            "                               LGBMRegressor(random_state=2024, verbosity=-1)),\n",
            "                              ('mlp',\n",
            "                               MLPRegressor(max_iter=1000, random_state=2024))],\n",
            "                  final_estimator=LinearRegression()), 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.4280121869566278, 'R2': 0.8176366513195346, 'MAE': 0.9034274285562686}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis for RF with more_restricted\n",
        "rf_more_restricted_finalgrade_results_zero, rf_more_restricted_finalgrade_best_model_zero = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategy='zero',  # Use the best strategy found\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61413833-e2b2-4117-8579-5b97d55410de",
        "id": "ZMQdSgKJZVta"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 819.71 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7cd6ec077d90>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.406426058269903, 'R2': 0.8203932935585255, 'MAE': 0.8844594491722937}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Old results"
      ],
      "metadata": {
        "id": "Sgn9NLUsZUFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis for RF with more_restricted\n",
        "rf_more_restricted_finalgrade_results_mean, rf_more_restricted_finalgrade_best_model_mean = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategy='mean',  # Use the best strategy found\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXQKdR4ZPe6C",
        "outputId": "aa066f5e-8112-4ded-ec04-5c2134e64f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 658.05 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x79947d02f940>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.3925497758755025, 'R2': 0.8221653549931494, 'MAE': 0.8832376288494231}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis for Lasso with more_restricted\n",
        "lasso_more_restricted_finalgrade_results_zero, lasso_more_restricted_finalgrade_best_model_zero = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=lasso_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='Lasso',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategy='zero',  # Use the best strategy found\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VeNiS4APzYf",
        "outputId": "38d8fe6b-cc32-41d3-9636-1aa4d2886af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 946.76 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_Lasso_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x79947d00e3e0>, 'dataset': 'predict_finalgrade_Lasso_more_restricted', 'test_scores': {'MSE': 1.406491126503044, 'R2': 0.8203849840629928, 'MAE': 0.8903159880405647}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### T4 GPU"
      ],
      "metadata": {
        "id": "hAF_p5i_PYcY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKvQSa-28YDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b144780a-93a6-4766-d220-7987336abbd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_RF_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
            "Cross-validation scores for XGBoost: -1.6715343238650893  0.06155015826305219\n",
            "Test set scores for XGBoost: {'MSE': 1.4070921377302954, 'R2': 0.8203082323230585, 'MAE': 0.8877477020044002}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001864 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001376 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001534 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001626 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001578 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001795 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001383 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001540 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001612 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001936 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001867 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001393 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001896 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001654 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001659 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001761 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001592 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001528 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001663 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001721 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001958 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001372 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001654 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001659 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001672 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001876 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001378 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001955 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001762 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001658 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001897 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001383 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001568 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001956 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001659 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001810 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001471 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001567 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001595 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001945 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001933 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001815 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001681 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001665 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001616 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.6962272686811768  0.049294919778485054\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001972 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.4609529418078286, 'R2': 0.8134299740813453, 'MAE': 0.9045306258309674}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 4, 'iterations': 500, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.6201718193807775  0.060668075595546475\n",
            "Test set scores for CatBoost: {'MSE': 1.406426058269903, 'R2': 0.8203932935585255, 'MAE': 0.8844594491722937}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001849 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001404 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001359 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001360 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 156\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001345 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001359 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001380 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001392 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001296 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001542 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001636 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001646 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001994 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001896 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001974 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 156\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001807 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001687 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002122 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001275 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001111 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 154\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001308 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001101 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001335 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001615 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001448 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001114 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001334 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001246 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001432 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.6469316716529838  0.04818866751525524\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002010 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001806 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001371 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001711 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001634 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001644 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.482075446892976, 'R2': 0.8107325385867247, 'MAE': 0.9147588319385438}\n",
            "Best model saved to best_models/predict_finalgrade_RF_more_restricted_predict_finalgrade_RF_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 658.43 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab50830faf0>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.406426058269903, 'R2': 0.8203932935585255, 'MAE': 0.8844594491722937}}\n"
          ]
        }
      ],
      "source": [
        "# Apply the optimized analysis for RF with more_restricted\n",
        "rf_more_restricted_finalgrade_results, rf_more_restricted_finalgrade_best_model = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategy='zero',  # Use the best strategy found\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcvyX8OG8a1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f630ed13-353a-486a-9597-b59f79f5000b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_Lasso_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
            "Cross-validation scores for XGBoost: -1.6704152613303211  0.05926084313181137\n",
            "Test set scores for XGBoost: {'MSE': 1.4821339357127656, 'R2': 0.8107250693108077, 'MAE': 0.9054457450206288}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001715 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001462 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001489 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002401 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001682 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001614 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001509 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001520 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001697 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001636 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001490 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001483 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001660 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001607 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001588 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001480 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001675 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001623 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001492 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001513 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001841 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001737 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002273 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002229 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002371 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002590 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002397 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002013 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002089 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002526 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002513 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002983 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002157 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001475 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001807 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002203 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002515 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001942 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001474 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001488 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001714 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.675530962756167  0.04507037669682824\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002160 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.4541943890762965, 'R2': 0.8142930705728272, 'MAE': 0.901711872271352}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.6292700593330398  0.04057732870985961\n",
            "Test set scores for CatBoost: {'MSE': 1.406491126503044, 'R2': 0.8203849840629928, 'MAE': 0.8903159880405647}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002375 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001291 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001706 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001446 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001478 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001372 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001595 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001531 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001349 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001246 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001202 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001183 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001586 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001180 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001361 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001286 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001269 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001516 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001375 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001190 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 163\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001250 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001246 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001242 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002416 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001315 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001151 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001382 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001207 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001248 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.6355032180600464  0.046901004061529736\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002124 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001680 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001574 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001497 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001904 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001982 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.4754715425553706, 'R2': 0.8115758858076876, 'MAE': 0.9019987247418598}\n",
            "Best model saved to best_models/predict_finalgrade_Lasso_more_restricted_predict_finalgrade_Lasso_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 686.94 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_Lasso_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab50830f9a0>, 'dataset': 'predict_finalgrade_Lasso_more_restricted', 'test_scores': {'MSE': 1.406491126503044, 'R2': 0.8203849840629928, 'MAE': 0.8903159880405647}}\n"
          ]
        }
      ],
      "source": [
        "# Apply the optimized analysis for Lasso with more_restricted\n",
        "lasso_more_restricted_finalgrade_results, lasso_more_restricted_finalgrade_best_model = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=lasso_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='Lasso',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategy='zero',  # Use the best strategy found\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdfViGHq8ZV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b71aa16-c519-4926-b8ac-b1ea23d7e514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_FR_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
            "Cross-validation scores for XGBoost: -1.6943884781841532  0.07533710014043599\n",
            "Test set scores for XGBoost: {'MSE': 1.415285743125347, 'R2': 0.8192618733835089, 'MAE': 0.8956791861572084}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001323 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001295 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001354 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001334 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001317 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001534 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001430 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001409 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001368 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001341 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001305 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001288 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001353 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001328 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001728 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001624 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001341 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001376 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001354 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001373 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001298 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001307 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001366 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001514 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001262 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001368 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001336 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002068 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001538 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001392 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001683 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001857 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001922 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001980 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001995 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001802 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001854 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001958 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002024 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001854 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002480 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001298 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001337 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001360 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001352 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.7356646373661484  0.051919757823070976\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001598 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.5012163051737895, 'R2': 0.8082881679821947, 'MAE': 0.9146652194095279}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 4, 'iterations': 500, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.6540324497667147  0.06156255431478905\n",
            "Test set scores for CatBoost: {'MSE': 1.4493845354475923, 'R2': 0.8149073097385746, 'MAE': 0.9031648396017565}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001396 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001020 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001128 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001064 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001061 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001074 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001754 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001058 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001106 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000966 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001086 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001020 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001346 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001393 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000972 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001097 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001091 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000999 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001443 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001087 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 123\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001083 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000986 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000980 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001280 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001058 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001091 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001096 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001015 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.650973475754641  0.05488153043789783\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001608 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001292 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002366 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001350 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 124\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001336 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 126\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.4934657371389524, 'R2': 0.8092779491296658, 'MAE': 0.9110306459065736}\n",
            "Best model saved to best_models/predict_finalgrade_FR_more_restricted_predict_finalgrade_FR_more_restricted_XGBoost.pkl\n",
            "Processing complete for datasets. Total time: 603.24 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_FR_more_restricted_XGBoost', 'params': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
            "             colsample_bylevel=None, colsample_bynode=None,\n",
            "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
            "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "             gamma=None, grow_policy=None, importance_type=None,\n",
            "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
            "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "             max_delta_step=None, max_depth=3, max_leaves=None,\n",
            "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "             multi_strategy=None, n_estimators=200, n_jobs=None,\n",
            "             num_parallel_tree=None, random_state=2024, ...), 'dataset': 'predict_finalgrade_FR_more_restricted', 'test_scores': {'MSE': 1.415285743125347, 'R2': 0.8192618733835089, 'MAE': 0.8956791861572084}}\n"
          ]
        }
      ],
      "source": [
        "# Apply the optimized analysis for RF with more_restricted\n",
        "fr_more_restricted_finalgrade_results, fr_more_restricted_finalgrade_best_model = analyze_final_grade_with_advanced_ensembles(\n",
        "    datasets_dict=fr_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='FR',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategy='zero',  # Use the best strategy found\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7YrkNJWjvGy"
      },
      "source": [
        "#### Improve results - Test different imputation techniques\n",
        "Zero imputation got the best results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### CPU"
      ],
      "metadata": {
        "id": "KOoi95LzRsQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis with imputation testing for RF with more_restricted\n",
        "rf_more_restricted_finalgrade_best_model = analyze_final_grade_with_imputation_strategies(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategies=['mean', 'median', 'zero', 'knn', 'most_frequent', 'constant'],  # Including all imputation strategies\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvXPdYRTRprT",
        "outputId": "3586a57b-8688-407d-93e8-71aaf46a819d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Testing imputation strategy: mean\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 702.98 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7994a92b8be0>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.3925497758755025, 'R2': 0.8221653549931494, 'MAE': 0.8832376288494231}}\n",
            "Testing imputation strategy: median\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 770.15 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7994a92ba830>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.5113925635463725, 'R2': 0.8069886156598739, 'MAE': 0.9207625401377196}}\n",
            "Testing imputation strategy: zero\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 918.70 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x79947ae0e9e0>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.406426058269903, 'R2': 0.8203932935585255, 'MAE': 0.8844594491722937}}\n",
            "Testing imputation strategy: knn\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 788.49 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_Stacking', 'params': StackingRegressor(estimators=[('xgb',\n",
            "                               XGBRegressor(base_score=None, booster=None,\n",
            "                                            callbacks=None,\n",
            "                                            colsample_bylevel=None,\n",
            "                                            colsample_bynode=None,\n",
            "                                            colsample_bytree=None, device=None,\n",
            "                                            early_stopping_rounds=None,\n",
            "                                            enable_categorical=False,\n",
            "                                            eval_metric=None,\n",
            "                                            feature_types=None, gamma=None,\n",
            "                                            grow_policy=None,\n",
            "                                            importance_type=None,\n",
            "                                            interaction_constraints=None,\n",
            "                                            learning_ra...\n",
            "                                            max_delta_step=None, max_depth=None,\n",
            "                                            max_leaves=None,\n",
            "                                            min_child_weight=None, missing=nan,\n",
            "                                            monotone_constraints=None,\n",
            "                                            multi_strategy=None,\n",
            "                                            n_estimators=None, n_jobs=None,\n",
            "                                            num_parallel_tree=None,\n",
            "                                            random_state=2024, ...)),\n",
            "                              ('lgbm',\n",
            "                               LGBMRegressor(random_state=2024, verbosity=-1)),\n",
            "                              ('mlp',\n",
            "                               MLPRegressor(max_iter=1000, random_state=2024))],\n",
            "                  final_estimator=LinearRegression()), 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.4280121869566278, 'R2': 0.8176366513195346, 'MAE': 0.9034274285562686}}\n",
            "Testing imputation strategy: most_frequent\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 763.14 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7994a00dbf10>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.4921243929910446, 'R2': 0.809449244593934, 'MAE': 0.9155177669495322}}\n",
            "Testing imputation strategy: constant\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade.\n",
            "Dropping rows with NaN values in the target variable.\n",
            "Processing complete for datasets. Total time: 949.67 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x79947aecb460>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.406426058269903, 'R2': 0.8203932935585255, 'MAE': 0.8844594491722937}}\n",
            "Best overall model saved to best_models/predict_finalgrade_RF_more_restricted_predict_finalgrade_RF_more_restricted_CatBoost_overall.pkl\n",
            "Best overall model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7994a92b8be0>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.3925497758755025, 'R2': 0.8221653549931494, 'MAE': 0.8832376288494231}, 'imputation_strategy': 'mean'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### L4 GPU"
      ],
      "metadata": {
        "id": "RDTbgZ3KQPve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis with imputation testing for RF with more_restricted\n",
        "original_finalgrade_best_model = analyze_final_grade_with_imputation_strategies(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategies=['mean', 'median', 'zero', 'knn', 'most_frequent', 'constant'],  # Including all imputation strategies\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd9_ed_T_Agv",
        "outputId": "d6b4e92b-08e7-4e80-ce1f-8f6bc5f6002a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Testing imputation strategy: mean\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Processing complete for datasets. Total time: 348.09 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3346791f90>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4264121162681838, 'R2': 0.5266408572738632, 'MAE': 0.9178229082371265}}\n",
            "Testing imputation strategy: median\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Processing complete for datasets. Total time: 429.67 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3346790310>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4217599021523226, 'R2': 0.5281847084936803, 'MAE': 0.9161535730645343}}\n",
            "Testing imputation strategy: zero\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Processing complete for datasets. Total time: 614.19 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3345f6b4f0>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4352675666524237, 'R2': 0.5237021494807015, 'MAE': 0.9229410916405137}}\n",
            "Testing imputation strategy: knn\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Processing complete for datasets. Total time: 461.51 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3345f92d70>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.417487463820378, 'R2': 0.5296025299795568, 'MAE': 0.9211656989314588}}\n",
            "Testing imputation strategy: most_frequent\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Processing complete for datasets. Total time: 389.76 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3346791870>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.421834236251559, 'R2': 0.5281600404997053, 'MAE': 0.9163452297545387}}\n",
            "Testing imputation strategy: constant\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Processing complete for datasets. Total time: 604.14 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3345f8b2b0>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4352675666524237, 'R2': 0.5237021494807015, 'MAE': 0.9229410916405137}}\n",
            "Best overall model saved to best_models/filtered_combined_df_FinalGrade_filtered_combined_df_FinalGrade_CatBoost_overall.pkl\n",
            "Best overall model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3345f92d70>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.417487463820378, 'R2': 0.5296025299795568, 'MAE': 0.9211656989314588}, 'imputation_strategy': 'knn'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the optimized analysis with imputation testing for RF with more_restricted\n",
        "original_finalgrade_best_model_group = analyze_final_grade_with_imputation_strategies(\n",
        "    datasets_dict=loaded_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    group_col='Year',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategies=['mean', 'median', 'zero', 'knn', 'most_frequent', 'constant'],  # Including all imputation strategies\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqVb6qQNTI32",
        "outputId": "b8cf3391-c166-483b-8089-4a1c0276bd98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Testing imputation strategy: mean\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Processing complete for datasets. Total time: 353.78 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3346793f70>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4264121162681838, 'R2': 0.5266408572738632, 'MAE': 0.9178229082371265}}\n",
            "Testing imputation strategy: median\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Processing complete for datasets. Total time: 414.02 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d33467910c0>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4217599021523226, 'R2': 0.5281847084936803, 'MAE': 0.9161535730645343}}\n",
            "Testing imputation strategy: zero\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Processing complete for datasets. Total time: 605.53 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3345f931f0>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4352675666524237, 'R2': 0.5237021494807015, 'MAE': 0.9229410916405137}}\n",
            "Testing imputation strategy: knn\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Processing complete for datasets. Total time: 441.34 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3345fb0730>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.417487463820378, 'R2': 0.5296025299795568, 'MAE': 0.9211656989314588}}\n",
            "Testing imputation strategy: most_frequent\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Processing complete for datasets. Total time: 389.93 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3345fb1570>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.421834236251559, 'R2': 0.5281600404997053, 'MAE': 0.9163452297545387}}\n",
            "Testing imputation strategy: constant\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "No specific columns to scale provided. Scaling all columns except the target.\n",
            "Processing complete for datasets. Total time: 610.88 seconds\n",
            "Best model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3345fb35b0>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.4352675666524237, 'R2': 0.5237021494807015, 'MAE': 0.9229410916405137}}\n",
            "Best overall model saved to best_models/filtered_combined_df_FinalGrade_filtered_combined_df_FinalGrade_CatBoost_overall.pkl\n",
            "Best overall model: {'model_name': 'filtered_combined_df_FinalGrade_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7d3345fb0730>, 'dataset': 'filtered_combined_df_FinalGrade', 'test_scores': {'MSE': 1.417487463820378, 'R2': 0.5296025299795568, 'MAE': 0.9211656989314588}, 'imputation_strategy': 'knn'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### T4 GPU"
      ],
      "metadata": {
        "id": "-LWQ28meQRB_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt6Rkjx9-JXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d291a7-4743-43ca-cbf7-50e29daca3bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Testing imputation strategy: mean\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_RF_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
            "Cross-validation scores for XGBoost: -1.6345593461138677  0.0680983568239383\n",
            "Test set scores for XGBoost: {'MSE': 1.4279573363519793, 'R2': 0.8176436559796012, 'MAE': 0.8927671727951064}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001867 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001886 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001903 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001778 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001786 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001849 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001781 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001762 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001794 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001758 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001775 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001998 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001801 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001820 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001808 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001814 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001788 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001916 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001758 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001916 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001803 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001774 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001782 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001758 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001849 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001777 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002797 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002568 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002826 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002687 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002584 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002633 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002742 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003495 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002666 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001817 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001863 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002188 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001862 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002192 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001812 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001785 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001783 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001801 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001805 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.6372245620556136  0.05777474617354442\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002218 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.430440101976985, 'R2': 0.8173265960430682, 'MAE': 0.8912979684828893}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.601262288430761  0.04484297519736299\n",
            "Test set scores for CatBoost: {'MSE': 1.3925497758755025, 'R2': 0.8221653549931494, 'MAE': 0.8832376288494231}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001771 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001464 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 200\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001581 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 200\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001484 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 199\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001472 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 200\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001420 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001802 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001439 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 200\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 200\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001469 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 200\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001463 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 200\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002187 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002238 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002195 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 200\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002157 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 200\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002060 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 199\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002313 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 200\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002098 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001829 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001466 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001437 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 197\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001422 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001510 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001413 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001847 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001435 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001440 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001521 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001425 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001597 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.5969180396942657  0.060401149188847376\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002332 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004517 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002709 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002753 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002830 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 198\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002922 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.4131073384772979, 'R2': 0.8195400651034591, 'MAE': 0.8853535110721874}\n",
            "Best model saved to best_models/predict_finalgrade_RF_more_restricted_predict_finalgrade_RF_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 504.27 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab507fc6fb0>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.3925497758755025, 'R2': 0.8221653549931494, 'MAE': 0.8832376288494231}}\n",
            "Testing imputation strategy: median\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_RF_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
            "Cross-validation scores for XGBoost: -1.7467888480238538  0.09997982261451886\n",
            "Test set scores for XGBoost: {'MSE': 1.542035988064197, 'R2': 0.8030753174673584, 'MAE': 0.9335394345899584}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002168 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002104 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002020 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001984 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002056 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002421 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002247 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002130 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002203 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001978 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002514 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003020 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002068 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002694 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002201 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002000 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002042 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001989 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002058 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002077 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002009 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001971 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002288 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002020 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001984 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002915 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002002 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002013 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001987 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002082 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002045 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002169 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001955 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001963 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002085 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002017 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002066 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003483 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003005 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002855 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003790 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003034 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002899 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002945 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003085 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003124 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.7426486903780443  0.06970443782105565\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003671 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.5869310482601804, 'R2': 0.7973420235982078, 'MAE': 0.9404583039569305}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.709062428639312  0.058207118221228815\n",
            "Test set scores for CatBoost: {'MSE': 1.5113925635463725, 'R2': 0.8069886156598739, 'MAE': 0.9207625401377196}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002071 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001630 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001624 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001556 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 156\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001639 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001652 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002014 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001649 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001652 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 56\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001671 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001684 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002263 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002036 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002340 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001742 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 56\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001714 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 156\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001639 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002028 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001586 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 154\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001755 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001708 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002089 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001637 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001621 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001687 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001599 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001609 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.696486344608891  0.09018632912814241\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002773 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001991 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002017 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002881 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002004 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002811 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.5609722557565215, 'R2': 0.8006570739681613, 'MAE': 0.925960122119607}\n",
            "Best model saved to best_models/predict_finalgrade_RF_more_restricted_predict_finalgrade_RF_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 594.08 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab507fc4c40>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.5113925635463725, 'R2': 0.8069886156598739, 'MAE': 0.9207625401377196}}\n",
            "Testing imputation strategy: zero\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_RF_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
            "Cross-validation scores for XGBoost: -1.6715343238650893  0.06155015826305219\n",
            "Test set scores for XGBoost: {'MSE': 1.4070921377302954, 'R2': 0.8203082323230585, 'MAE': 0.8877477020044002}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001917 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001420 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001534 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001629 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001789 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001416 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001700 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001709 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001792 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002014 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002303 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004147 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002397 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002457 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001922 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002300 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002492 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002344 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001786 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001435 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001574 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001662 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001609 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001769 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001433 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001555 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001628 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001601 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001791 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001415 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001524 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001633 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001630 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001775 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001414 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001632 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001630 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001626 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002025 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002379 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001369 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001559 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001648 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001615 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.6962272686811768  0.049294919778485054\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001914 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.4609529418078286, 'R2': 0.8134299740813453, 'MAE': 0.9045306258309674}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 4, 'iterations': 500, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.6201718193807775  0.060668075595546475\n",
            "Test set scores for CatBoost: {'MSE': 1.406426058269903, 'R2': 0.8203932935585255, 'MAE': 0.8844594491722937}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002158 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001410 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001365 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001352 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 156\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001367 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001280 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001406 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001957 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001917 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001686 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001666 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001123 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001613 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001400 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001294 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001392 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 156\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001808 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002381 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001752 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001143 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 154\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002842 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001138 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001273 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001610 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001392 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001271 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001284 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001301 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001386 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.6469316716529838  0.04818866751525524\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001957 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001793 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001431 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001529 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001670 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001876 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.482075446892976, 'R2': 0.8107325385867247, 'MAE': 0.9147588319385438}\n",
            "Best model saved to best_models/predict_finalgrade_RF_more_restricted_predict_finalgrade_RF_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 676.49 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab507fc5360>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.406426058269903, 'R2': 0.8203932935585255, 'MAE': 0.8844594491722937}}\n",
            "Testing imputation strategy: knn\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_RF_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
            "Cross-validation scores for XGBoost: -1.7190329345213695  0.037485777158427325\n",
            "Test set scores for XGBoost: {'MSE': 1.4387930213961173, 'R2': 0.816259892011797, 'MAE': 0.9039928539491419}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003780 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003586 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003642 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002696 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002676 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003490 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002537 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002664 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002686 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002606 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002722 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003462 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002677 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002577 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002568 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002647 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002630 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002982 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002632 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002518 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002752 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002677 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002679 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002722 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002558 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002678 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002657 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002604 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002573 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004000 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 402\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002604 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002690 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002582 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002643 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.730471822389592  0.02514769251909854\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003215 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 402\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.4321888710339585, 'R2': 0.8171032706511624, 'MAE': 0.8994696969954984}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.6736161431370147  0.06605106686588348\n",
            "Test set scores for CatBoost: {'MSE': 1.4427179360989595, 'R2': 0.8157586633843152, 'MAE': 0.9048700134305557}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002579 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002153 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002101 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 395\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002099 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 393\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002084 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 395\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002071 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 396\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002049 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001956 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002288 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 395\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002067 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 396\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002412 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 396\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002553 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002154 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 394\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001924 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 396\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002024 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 393\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002061 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 392\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002164 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 392\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002798 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002176 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 393\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001931 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 394\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002369 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 395\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002171 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 390\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002069 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 392\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003632 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002071 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 396\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002196 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002129 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002182 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 394\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002070 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 391\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.7158275906902911  0.033115791797278325\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003230 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 402\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002653 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002622 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002530 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 398\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002633 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 397\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002555 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 399\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.4280121869566278, 'R2': 0.8176366513195346, 'MAE': 0.9034274285562686}\n",
            "Best model saved to best_models/predict_finalgrade_RF_more_restricted_predict_finalgrade_RF_more_restricted_Stacking.pkl\n",
            "Processing complete for datasets. Total time: 536.85 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_Stacking', 'params': StackingRegressor(estimators=[('xgb',\n",
            "                               XGBRegressor(base_score=None, booster=None,\n",
            "                                            callbacks=None,\n",
            "                                            colsample_bylevel=None,\n",
            "                                            colsample_bynode=None,\n",
            "                                            colsample_bytree=None, device=None,\n",
            "                                            early_stopping_rounds=None,\n",
            "                                            enable_categorical=False,\n",
            "                                            eval_metric=None,\n",
            "                                            feature_types=None, gamma=None,\n",
            "                                            grow_policy=None,\n",
            "                                            importance_type=None,\n",
            "                                            interaction_constraints=None,\n",
            "                                            learning_ra...\n",
            "                                            max_cat_to_onehot=None,\n",
            "                                            max_delta_step=None, max_depth=None,\n",
            "                                            max_leaves=None,\n",
            "                                            min_child_weight=None, missing=nan,\n",
            "                                            monotone_constraints=None,\n",
            "                                            multi_strategy=None,\n",
            "                                            n_estimators=None, n_jobs=None,\n",
            "                                            num_parallel_tree=None,\n",
            "                                            random_state=2024, ...)),\n",
            "                              ('lgbm', LGBMRegressor(random_state=2024)),\n",
            "                              ('mlp',\n",
            "                               MLPRegressor(max_iter=1000, random_state=2024))],\n",
            "                  final_estimator=LinearRegression()), 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.4280121869566278, 'R2': 0.8176366513195346, 'MAE': 0.9034274285562686}}\n",
            "Testing imputation strategy: most_frequent\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_RF_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
            "Cross-validation scores for XGBoost: -1.7549579606316237  0.10363832290298802\n",
            "Test set scores for XGBoost: {'MSE': 1.5590036503343523, 'R2': 0.8009084734171971, 'MAE': 0.9331517125880969}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002115 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002142 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002101 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002101 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002099 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002076 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002087 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002097 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002129 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002113 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002074 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002119 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002575 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002157 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003012 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002114 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002093 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002137 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002096 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002236 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002202 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002070 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002188 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002150 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002168 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002068 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002140 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003089 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002997 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002965 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002991 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003228 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003073 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003272 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004908 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003080 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003083 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003082 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003218 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002123 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002602 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002113 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002924 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002123 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002089 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002079 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.7328975741315549  0.06466634416218185\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002692 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.5835541353617457, 'R2': 0.7977732700189217, 'MAE': 0.9397190386108435}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.7080049824934502  0.06139400058112228\n",
            "Test set scores for CatBoost: {'MSE': 1.4921243929910446, 'R2': 0.809449244593934, 'MAE': 0.9155177669495322}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002434 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002073 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001729 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001714 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 156\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001762 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001784 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002115 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001719 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001706 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 56\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001765 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001736 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001659 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003262 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001693 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001702 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 56\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001691 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 156\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001788 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001709 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002096 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001704 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001793 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 154\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001674 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001761 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001708 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002240 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001726 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001697 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001797 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001725 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.6866195821952492  0.08686555361744179\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003222 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002184 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002060 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002967 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002182 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002065 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.554661862643482, 'R2': 0.8014629385329705, 'MAE': 0.9317557529350307}\n",
            "Best model saved to best_models/predict_finalgrade_RF_more_restricted_predict_finalgrade_RF_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 540.32 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab507fc4250>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.4921243929910446, 'R2': 0.809449244593934, 'MAE': 0.9155177669495322}}\n",
            "Testing imputation strategy: constant\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_RF_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
            "Cross-validation scores for XGBoost: -1.6715343238650893  0.06155015826305219\n",
            "Test set scores for XGBoost: {'MSE': 1.4070921377302954, 'R2': 0.8203082323230585, 'MAE': 0.8877477020044002}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001788 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001443 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001559 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001597 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001821 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001793 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001369 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001618 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001696 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001762 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001365 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002508 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001577 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002507 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001385 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001669 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001761 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001982 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001878 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001392 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002329 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001614 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001626 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001826 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001974 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002327 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002427 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002257 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002060 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002208 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002319 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002408 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003076 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002014 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002238 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001705 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001611 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002037 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001812 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001607 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001539 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001634 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.6962272686811768  0.049294919778485054\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001910 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.4609529418078286, 'R2': 0.8134299740813453, 'MAE': 0.9045306258309674}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 4, 'iterations': 500, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.6201718193807775  0.060668075595546475\n",
            "Test set scores for CatBoost: {'MSE': 1.406426058269903, 'R2': 0.8203932935585255, 'MAE': 0.8844594491722937}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001859 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001547 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001429 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001396 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 156\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001394 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001338 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001942 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001618 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001249 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001090 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001109 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001190 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001567 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001365 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001290 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001335 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 156\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001274 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 157\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001198 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001632 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001238 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001128 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 154\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001382 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001113 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001301 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001431 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001139 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001286 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001233 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001291 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.6469316716529838  0.04818866751525524\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001986 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001797 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001376 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002259 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001723 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 155\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001632 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 158\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 57\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.482075446892976, 'R2': 0.8107325385867247, 'MAE': 0.9147588319385438}\n",
            "Best model saved to best_models/predict_finalgrade_RF_more_restricted_predict_finalgrade_RF_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 647.24 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab507fc7c10>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.406426058269903, 'R2': 0.8203932935585255, 'MAE': 0.8844594491722937}}\n",
            "Best overall model saved to best_models/predict_finalgrade_RF_more_restricted_predict_finalgrade_RF_more_restricted_CatBoost_overall.pkl\n",
            "Best overall model: {'model_name': 'predict_finalgrade_RF_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab507fc6fb0>, 'dataset': 'predict_finalgrade_RF_more_restricted', 'test_scores': {'MSE': 1.3925497758755025, 'R2': 0.8221653549931494, 'MAE': 0.8832376288494231}, 'imputation_strategy': 'mean'}\n"
          ]
        }
      ],
      "source": [
        "# Apply the optimized analysis with imputation testing for RF with more_restricted\n",
        "rf_more_restricted_finalgrade_best_model = analyze_final_grade_with_imputation_strategies(\n",
        "    datasets_dict=rf_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='RF',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategies=['mean', 'median', 'zero', 'knn', 'most_frequent', 'constant'],  # Including all imputation strategies\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7RZRHcn-NLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefa4adc-68a9-4230-904c-e5d7cede6ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "Testing imputation strategy: mean\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_Lasso_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
            "Cross-validation scores for XGBoost: -1.6344802654276926  0.07531326397915611\n",
            "Test set scores for XGBoost: {'MSE': 1.4363215153580373, 'R2': 0.8165755140502537, 'MAE': 0.8970098091916537}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001925 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001869 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002053 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001838 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001828 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001837 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001951 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001847 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001820 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002026 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001853 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001964 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001945 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001858 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001911 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001867 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001931 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001852 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001883 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001956 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001860 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001829 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001819 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001822 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001905 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001858 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001839 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001973 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002342 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002253 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002168 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001847 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001872 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001880 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002247 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002506 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002848 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002714 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002874 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002758 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003491 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002742 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002986 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002858 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002764 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002830 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.6336156919974503  0.06177107494506388\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002379 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.4360611442026543, 'R2': 0.816608764575865, 'MAE': 0.8909305236483592}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.5982395741562043  0.03718362923276939\n",
            "Test set scores for CatBoost: {'MSE': 1.3908807110241628, 'R2': 0.8223785018123679, 'MAE': 0.8819102040832262}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002849 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001488 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 215\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001508 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 216\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001474 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001489 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 216\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001472 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001924 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001520 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 216\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001540 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 216\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001614 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 216\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 216\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002108 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001899 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001573 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 216\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001487 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 215\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001509 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 215\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001509 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 216\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001810 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002411 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001497 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001511 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 212\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001524 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001484 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002854 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001582 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006270 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001535 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001487 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001516 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.5942087037007764  0.06020773388798259\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002369 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001887 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001829 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001865 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001847 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 214\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001983 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.4522088271930698, 'R2': 0.8145466354354697, 'MAE': 0.8956192549002093}\n",
            "Best model saved to best_models/predict_finalgrade_Lasso_more_restricted_predict_finalgrade_Lasso_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 463.32 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_Lasso_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab50870e9e0>, 'dataset': 'predict_finalgrade_Lasso_more_restricted', 'test_scores': {'MSE': 1.3908807110241628, 'R2': 0.8223785018123679, 'MAE': 0.8819102040832262}}\n",
            "Testing imputation strategy: median\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_Lasso_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
            "Cross-validation scores for XGBoost: -1.7528124533125091  0.09877723319712796\n",
            "Test set scores for XGBoost: {'MSE': 1.5432226045416113, 'R2': 0.8029237813975704, 'MAE': 0.9343925437273825}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003177 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003073 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003131 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003169 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003104 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002022 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002083 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002111 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002109 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002040 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002157 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002119 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002052 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002190 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002213 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002018 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002011 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002009 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002015 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002151 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002030 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002039 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002064 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002160 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002076 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002109 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002067 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002216 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002249 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002046 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002108 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002048 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002064 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001994 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002133 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002149 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002019 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002079 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003072 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002037 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002793 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002138 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002173 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002043 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003098 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002067 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.7306713698260623  0.06716492854881112\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002924 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.5728518266670914, 'R2': 0.799140001248526, 'MAE': 0.9389501513073211}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.7191907739614702  0.050401437721968256\n",
            "Test set scores for CatBoost: {'MSE': 1.5066865801249214, 'R2': 0.8075895901497342, 'MAE': 0.9217554777465243}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002172 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001690 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001655 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001715 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001720 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001670 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002012 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001639 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001635 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 59\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001680 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001656 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001667 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002200 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001687 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001653 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 164\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 59\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001747 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002106 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001738 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002630 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001636 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001826 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 163\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001669 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001716 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001754 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002036 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002358 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001631 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001614 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001643 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001776 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.686923130976383  0.08378999304931124\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002630 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002119 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002136 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002210 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002099 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002062 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.5077624256078208, 'R2': 0.8074522000162911, 'MAE': 0.9154541704003167}\n",
            "Best model saved to best_models/predict_finalgrade_Lasso_more_restricted_predict_finalgrade_Lasso_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 596.69 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_Lasso_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab508738bb0>, 'dataset': 'predict_finalgrade_Lasso_more_restricted', 'test_scores': {'MSE': 1.5066865801249214, 'R2': 0.8075895901497342, 'MAE': 0.9217554777465243}}\n",
            "Testing imputation strategy: zero\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_Lasso_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
            "Cross-validation scores for XGBoost: -1.6704152613303211  0.05926084313181137\n",
            "Test set scores for XGBoost: {'MSE': 1.4821339357127656, 'R2': 0.8107250693108077, 'MAE': 0.9054457450206288}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002017 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001634 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001504 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001590 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001679 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001795 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001615 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001496 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001581 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001766 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001707 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001527 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001578 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001720 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001681 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001619 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001488 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001507 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001788 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001707 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001646 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001496 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001575 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001720 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001789 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001547 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001803 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001476 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001677 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001676 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001551 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001521 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001654 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001634 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001477 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001544 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001669 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002841 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001852 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001984 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001500 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001767 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.675530962756167  0.04507037669682824\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002203 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.4541943890762965, 'R2': 0.8142930705728272, 'MAE': 0.901711872271352}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.6292700593330398  0.04057732870985961\n",
            "Test set scores for CatBoost: {'MSE': 1.406491126503044, 'R2': 0.8203849840629928, 'MAE': 0.8903159880405647}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001884 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001294 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002019 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001732 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001362 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001601 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001317 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001413 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001244 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001304 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001157 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001295 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001215 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001358 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001257 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001281 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001497 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001344 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001205 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 163\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001249 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001599 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001221 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001715 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001363 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001229 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001364 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001230 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.6355032180600464  0.046901004061529736\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002140 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001960 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001697 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002112 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002323 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.4754715425553706, 'R2': 0.8115758858076876, 'MAE': 0.9019987247418598}\n",
            "Best model saved to best_models/predict_finalgrade_Lasso_more_restricted_predict_finalgrade_Lasso_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 682.29 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_Lasso_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab508712ad0>, 'dataset': 'predict_finalgrade_Lasso_more_restricted', 'test_scores': {'MSE': 1.406491126503044, 'R2': 0.8203849840629928, 'MAE': 0.8903159880405647}}\n",
            "Testing imputation strategy: knn\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_Lasso_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
            "Cross-validation scores for XGBoost: -1.7186138829745274  0.02646645438478512\n",
            "Test set scores for XGBoost: {'MSE': 1.4217492939531402, 'R2': 0.8184364499143753, 'MAE': 0.9040424532766892}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002814 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 427\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002804 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002818 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002948 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 424\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002867 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002818 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 427\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002720 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002773 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002865 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 424\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003981 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003479 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 427\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003685 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003835 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003873 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 424\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003922 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003792 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 427\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003874 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002818 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002829 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 424\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002862 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002859 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 427\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002712 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002997 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002881 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 424\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002795 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002750 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 427\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002764 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002790 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003097 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 424\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002769 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002742 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 427\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003338 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002870 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003025 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 424\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002796 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002777 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 427\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002771 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003213 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002897 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 424\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002811 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003478 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 429\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002785 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 427\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003818 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002775 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002764 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 424\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002833 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.7408433233985483  0.035891984746643235\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003479 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 429\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.4312987311047876, 'R2': 0.8172169453801041, 'MAE': 0.9056383483991934}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 4, 'iterations': 500, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.6778064823543992  0.045315683047868094\n",
            "Test set scores for CatBoost: {'MSE': 1.4103874528588127, 'R2': 0.8198874062914022, 'MAE': 0.8982296063632387}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002729 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 427\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002235 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 422\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002233 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 422\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002263 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 419\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002709 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 422\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002405 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 422\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002814 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002243 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 423\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002225 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 425\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002279 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 423\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002307 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 422\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002277 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 421\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003771 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002476 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 421\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002305 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 423\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002660 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 421\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002253 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 420\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002361 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 419\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002755 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 424\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003167 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 420\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003087 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 420\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003028 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 422\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002946 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 419\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003034 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 419\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002934 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002258 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 423\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002547 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 423\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002250 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 423\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002612 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 421\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003080 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 418\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.7324025773193452  0.031289178565446954\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004141 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 429\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002801 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 427\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002919 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002771 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002872 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 424\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002792 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 426\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.4242425828822305, 'R2': 0.8181180461062776, 'MAE': 0.9082312282312}\n",
            "Best model saved to best_models/predict_finalgrade_Lasso_more_restricted_predict_finalgrade_Lasso_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 568.96 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_Lasso_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab507f17760>, 'dataset': 'predict_finalgrade_Lasso_more_restricted', 'test_scores': {'MSE': 1.4103874528588127, 'R2': 0.8198874062914022, 'MAE': 0.8982296063632387}}\n",
            "Testing imputation strategy: most_frequent\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_Lasso_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
            "Cross-validation scores for XGBoost: -1.766188222414473  0.09174792297745696\n",
            "Test set scores for XGBoost: {'MSE': 1.5364851666160229, 'R2': 0.8037841814367714, 'MAE': 0.9326353493089627}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002259 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002192 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002372 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002170 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002425 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002246 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002179 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002199 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002156 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002209 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002156 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002222 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003285 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002185 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002260 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002130 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002219 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003167 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002224 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002164 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002127 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002390 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002238 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002345 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002536 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002154 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002238 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002183 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002205 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002195 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002178 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002172 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002268 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002252 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002434 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002284 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003135 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003212 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003226 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004059 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003065 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003303 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003237 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003234 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003439 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.7292956858275517  0.06068084732660833\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004244 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.554505921361066, 'R2': 0.8014828529109559, 'MAE': 0.9304366731520094}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 4, 'iterations': 500, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.7095813151154555  0.05997513013617619\n",
            "Test set scores for CatBoost: {'MSE': 1.483114495328807, 'R2': 0.8105998475957583, 'MAE': 0.9164378592001234}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002365 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001775 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002126 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001834 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001736 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001779 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002227 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002022 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002716 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 59\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003683 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002781 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002793 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002165 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001753 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001711 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 164\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 59\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001718 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001811 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001758 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002170 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001834 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001870 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 163\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001753 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002937 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001823 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002155 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001796 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001739 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001823 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001714 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001856 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.6816318073447767  0.07614738935430876\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004381 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002217 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002183 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002202 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002227 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002429 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.5253890930390348, 'R2': 0.8052011981493646, 'MAE': 0.9262751814328171}\n",
            "Best model saved to best_models/predict_finalgrade_Lasso_more_restricted_predict_finalgrade_Lasso_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 577.01 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_Lasso_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab50830ff40>, 'dataset': 'predict_finalgrade_Lasso_more_restricted', 'test_scores': {'MSE': 1.483114495328807, 'R2': 0.8105998475957583, 'MAE': 0.9164378592001234}}\n",
            "Testing imputation strategy: constant\n",
            "Processing datasets for target column: FinalGrade with exclusion type: more_restricted\n",
            "--------------------------------------------------------------------------------\n",
            "Processing dataset: predict_finalgrade_Lasso_more_restricted\n",
            "Column 'None' not found. Scaling without grouping.\n",
            "Warning: NaN values found in the target variable FinalGrade. Dropping rows with NaN values.\n",
            "Performing regression analysis with advanced ensemble techniques...\n",
            "Evaluating model: XGBoost\n",
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
            "Cross-validation scores for XGBoost: -1.6704152613303211  0.05926084313181137\n",
            "Test set scores for XGBoost: {'MSE': 1.4821339357127656, 'R2': 0.8107250693108077, 'MAE': 0.9054457450206288}\n",
            "Evaluating model: LightGBM\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001687 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001654 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001491 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001504 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001687 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001665 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001711 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001575 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001467 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001663 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001839 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001587 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001541 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001515 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001718 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001704 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001614 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001916 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001491 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001786 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001689 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001631 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001601 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001452 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001782 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001694 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001578 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001507 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001500 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002669 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001667 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001611 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001493 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002151 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002148 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001700 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001596 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001630 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001689 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002112 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 31}\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001682 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001759 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001565 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001472 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001951 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Cross-validation scores for LightGBM: -1.675530962756167  0.04507037669682824\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002147 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "Test set scores for LightGBM: {'MSE': 1.4541943890762965, 'R2': 0.8142930705728272, 'MAE': 0.901711872271352}\n",
            "Evaluating model: CatBoost\n",
            "Best parameters for CatBoost: {'depth': 6, 'iterations': 200, 'learning_rate': 0.1}\n",
            "Cross-validation scores for CatBoost: -1.6292700593330398  0.04057732870985961\n",
            "Test set scores for CatBoost: {'MSE': 1.406491126503044, 'R2': 0.8203849840629928, 'MAE': 0.8903159880405647}\n",
            "Evaluating model: Stacking\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001742 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001788 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.018221\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001441 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.023569\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001322 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.033579\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001512 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.992146\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001842 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.983274\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001573 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001324 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.009212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001357 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.020233\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001728 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.021237\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001216 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.979803\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001284 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.970932\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001526 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001849 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.042671\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001212 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.039211\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001325 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 166\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.035716\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001246 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 167\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.013262\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001237 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004393\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001565 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001366 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.013891\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001208 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 163\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010431\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001887 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010044\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001234 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010154\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001829 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.975620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001700 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001377 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.976058\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001156 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.972598\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001363 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3918, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.972212\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001787 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.990674\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001941 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 3919, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.957278\n",
            "Cross-validation scores for Stacking: -1.6355032180600464  0.046901004061529736\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003268 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 170\n",
            "[LightGBM] [Info] Number of data points in the train set: 6122, number of used features: 61\n",
            "[LightGBM] [Info] Start training from score 16.003056\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001675 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.010157\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001736 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4897, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.000282\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001697 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.027049\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001676 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 165\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 16.004027\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002419 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 168\n",
            "[LightGBM] [Info] Number of data points in the train set: 4898, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 15.973764\n",
            "Test set scores for Stacking: {'MSE': 1.4754715425553706, 'R2': 0.8115758858076876, 'MAE': 0.9019987247418598}\n",
            "Best model saved to best_models/predict_finalgrade_Lasso_more_restricted_predict_finalgrade_Lasso_more_restricted_CatBoost.pkl\n",
            "Processing complete for datasets. Total time: 683.82 seconds\n",
            "Best model: {'model_name': 'predict_finalgrade_Lasso_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab50db957e0>, 'dataset': 'predict_finalgrade_Lasso_more_restricted', 'test_scores': {'MSE': 1.406491126503044, 'R2': 0.8203849840629928, 'MAE': 0.8903159880405647}}\n",
            "Best overall model saved to best_models/predict_finalgrade_Lasso_more_restricted_predict_finalgrade_Lasso_more_restricted_CatBoost_overall.pkl\n",
            "Best overall model: {'model_name': 'predict_finalgrade_Lasso_more_restricted_CatBoost', 'params': <catboost.core.CatBoostRegressor object at 0x7ab50870e9e0>, 'dataset': 'predict_finalgrade_Lasso_more_restricted', 'test_scores': {'MSE': 1.3908807110241628, 'R2': 0.8223785018123679, 'MAE': 0.8819102040832262}, 'imputation_strategy': 'mean'}\n"
          ]
        }
      ],
      "source": [
        "# Apply the optimized analysis with imputation testing for Lasso with more_restricted\n",
        "lasso_more_restricted_finalgrade_best_model = analyze_final_grade_with_imputation_strategies(\n",
        "    datasets_dict=lasso_finalgrade_datasets,\n",
        "    target_column='FinalGrade',\n",
        "    feature_set_name='Lasso',\n",
        "    exclusion_type='more_restricted',\n",
        "    imputation_strategies=['mean', 'median', 'zero', 'knn', 'most_frequent', 'constant'],  # Including all imputation strategies\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3gad_WNC9yX"
      },
      "source": [
        "# 5. Save and Display Results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = 'best_models'\n",
        "zip_path = 'best_models.zip'\n",
        "\n",
        "shutil.make_archive(base_name='best_models', format='zip', root_dir=folder_path)\n",
        "\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "do18CYPfNeBV",
        "outputId": "bf10696f-e878-423b-a2aa-912600f2494a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f731af48-05bf-4132-803b-e1ab1c45d3c7\", \"best_models.zip\", 719605)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = 'catboost_info'\n",
        "zip_path = 'catboost_info.zip'\n",
        "shutil.make_archive(base_name='catboost_info', format='zip', root_dir=folder_path)\n",
        "\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "09nqSDGnl-_1",
        "outputId": "4d10bafc-e99e-41dc-e817-276b6fb056f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_596416b0-adce-404a-996e-b15a36815fd3\", \"catboost_info.zip\", 24363)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI1uwlRKC2XH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb9b3e34-2fc8-418d-bc36-5c13d1ac36b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document saved as prediction_analysis_output.docx\n"
          ]
        }
      ],
      "source": [
        "# Save the document at the end\n",
        "save_document('prediction_analysis_output.docx')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyObYplFxGbCebF4D8lA9xIv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}